    INTERNAL DOCUMENT â€“ STRICTLY CONFIDENTIAL    

â—†
SPARKDATA ANALYTICS, LLC

  STANDARD OPERATING PROCEDURE  

AI-Assisted Development &
Agentic Coding Operations Guide
Production Edition with Regulatory Compliance

Document Number
SDA-SOP-DEV-001
Version
5.2.1 (Gold Standard Compliance Edition)
Effective Date
November 30, 2025
Classification
INTERNAL â€“ CONFIDENTIAL
Department
Engineering / Data Operations
Compliance
AICPA QM | EU AI Act | PCAOB | Agentic AI
Review Cycle
Quarterly
Next Review
Q1 2026

DOCUMENT APPROVAL SIGNATURES
Role
Name
Signature
Date
Author
Engineering Team
 
2025-11-30
Technical Reviewer
 
 
 
Quality Assurance
 
 
 
Final Approver
 
 
 

EXECUTIVE SUMMARY
This one-page summary provides executives, clients, and auditors with a high-level overview of SparkData's AI-assisted development quality controls.
STANDARDS ALIGNMENT
AICPA QM (SQMS No. 1)
Risk-based quality management with multi-model independent verification, monitoring, and remediation processes
EU AI Act
Technical documentation, structured logging with JSON schemas, human oversight hooks, and transparency requirements
PCAOB Guidance
Adversarial review protocols, professional skepticism enforcement, complete audit trails, and decision logging
Agentic AI Patterns
Multi-agent architecture (Planner/Builder/Reviewer/Auditor), tool verification loops, guardrails, and feedback systems
AI QUALITY CONTROLS APPLIED
LOW RISK (T0/T1)
HIGH RISK (T2/T3)
â€¢ 1-2 AI agents
â€¢ Quick adversarial review
â€¢ Minimal evidence (test/lint/types)
â€¢ Human merges PR
â€¢ Full multi-agent pipeline
â€¢ 4 specialized review lenses
â€¢ Complete logging + decision JSON
â€¢ Senior + Compliance sign-off
HOW WE PREVENT "LLM ECHO CHAMBER" FAILURES
âœ“ Fresh chat sessions for each review role (Reviewer never sees Builder's reasoning)
âœ“ Different model families required for T3 (Claude / GPT / Gemini)
âœ“ Devil's Advocate review triggered when agreement exceeds 80%
âœ“ Only actual tool outputs (tests, lints) are sharedâ€”never prose explanations
AUDIT ARTIFACTS AVAILABLE
Decision Logs
Review Reports
Model Scorecards
DECISION_*.json with model attribution, risk tier, metrics, human override
VERIFICATION_*.md, PERSPECTIVE_*.md, COMPARISON_*.md
90-day rolling metrics: defects, incidents, rollbacks per model
  COMPANION DOCUMENTS: Risk Tiering Matrix (v2.1) â€¢ IDE Quick Prompts (v2.1) â€¢ Model Scorecard Template

REVISION HISTORY
Version
Date
Author
Description
5.2.0
2025-11-30
Engineering
Gold Standard Final - Fresh chat separation (Â§3.2.1), Inline AI policy (Â§3.2.2), T1 worked example, AI orchestrator script, Data-Quality Lens, agreement measurement clarification
5.1.0
2025-11-15
Engineering
AICPA QM alignment, EU AI Act compliance, risk tiering, IDE prompts
5.0.0
2025-10-01
Engineering
Multi-agent governance, adversarial review, proof of work
4.0.0
2025-08-01
Engineering
Initial Gold Standard Edition with Five Pillars

  PURPOSE: Establish mandatory procedures for AI-assisted development ensuring compliance with AICPA QM, EU AI Act, and PCAOB standards.
  SCOPE: All engineering personnel, contractors, and AI systems involved in software development, data pipelines, and code review.
  CROSS-REFERENCES: See Risk Tiering & Required Controls (v2.1) for canonical tier matrix (SOP Â§1.2). All IDE usage for T1-T3 must follow SparkData_IDE_Quick_Prompts_v2.1.

CRITICAL RULES & QUICK REFERENCE
âš ï¸  GOLDEN RULE: WHEN IN DOUBT, CHOOSE THE HIGHER TIER  âš ï¸
If you are unsure whether a change is T1 or T2, treat it as T2. If unsure between T2 and T3, treat it as T3. Erring on the side of caution protects code quality and ensures compliance.

IDE SHORTCUTS BY RISK TIER
Tier
Required IDE Prompts
T0
"Build this" only (review optional)
T1
"Build this" â†’ [NEW CHAT] â†’ "Review this"
T2
"Build this" â†’ [NEW CHAT] â†’ "Verify T2/T3" + at least 1 Lens (Security OR Performance)
T3
All T2 + ALL 4 Lenses + "Compare models" + "Challenge this" (if >80% agreement)

WHAT TO PASTE (AND NOT PASTE) IN REVIEW SESSIONS
âŒ  DO NOT PASTE
âœ“  DO PASTE
â€¢ Prior chat logs
â€¢ "Here's what I did" prose from Builder
â€¢ Screenshots of reasoning
â€¢ Builder's explanations or summaries
â€¢ Original requirement/spec/ticket
â€¢ Actual diff or file contents
â€¢ Raw test output (pytest, jest, etc.)
â€¢ Lint/type checker output (actual text)

AGREEMENT MEASUREMENT (>80% TRIGGER)
  How to measure: Approximate issue-list similarity. Operationally: "If all reviewers raise essentially the same 2-3 issues and nothing else, treat this as >80% agreement and trigger Devil's Advocate review."
  Why this matters: High agreement may indicate echo-chamber behavior where models reinforce each other rather than providing independent analysis.

PIPELINE/WORKFLOW TIER EXAMPLES
Tier
Pipeline Example
Justification
T0
Adding logging to dev-only test harness
Non-prod, no data impact
T1
Minor report formatting changes
UI only, no data model changes
T2
New customer matching pipeline
Core business logic, affects data quality
T3
Changes to MDRS scoring rules
Regulatory/compliance impact, PII adjacent

EXAMPLE WORKFLOWS
EXAMPLE A: T2 PIPELINE CHANGE
  Scenario: Add deduplication logic to customer ingestion pipeline
1
Chat A: Run Builder Mode (#1) â†’ implement deduplication + tests
2
Run: make evidence â†’ collect test_evidence.txt, lint_evidence.txt, types_evidence.txt
3
âš ï¸ OPEN NEW CHAT B: Run Verification Mode (#3) â†’ paste spec + diff + tool outputs only
4
Chat B: Run Data-Quality Lens (#10) â†’ pipeline-specific checks
5
Chat B: Run Security-First Lens (#4) â†’ verify no injection risks
6
Create DECISION_*.json with all model attributions
7
Get Lead approval â†’ merge

EXAMPLE B: T3 SECURITY CHANGE
  Scenario: Modify authentication token validation logic
1
Chat A (Claude): Run Builder Mode (#1) â†’ implement token validation changes
2
Run: make evidence â†’ all evidence files
3
âš ï¸ NEW CHAT B (GPT-4): Run ALL 4 Lenses (Security/Perf/Maintain/Resilience)
4
Check agreement: If >80% overlap â†’ Run Devil's Advocate (#8)
5
âš ï¸ NEW CHAT C (Gemini): Run Differential Comparison (#9) â†’ compare outputs
6
Save PERSPECTIVE_*.md (4 files) + COMPARISON_*.md
7
Create DECISION_*.json listing all 3 model families used
8
Get Senior Engineer + Compliance sign-off â†’ merge

IMPLEMENTATION CHECKLIST
Use this checklist to operationalize the SOP. Check off each item as implemented.
CI/CD ENFORCEMENT
â˜  Pre-commit hook to block merges without required evidence files for the declared tier
â˜  CI job to verify DECISION_*.json exists for all AI-assisted PRs
â˜  CI job to verify VERIFICATION_*.md exists for T2+ PRs
â˜  CI job to verify 4x PERSPECTIVE_*.md files exist for T3 PRs
â˜  CI job to validate tier in DECISION_*.json matches PR label
IDE INTEGRATION
â˜  VS Code / Cursor snippet for each Quick Prompt shortcut (#1-#10)
â˜  JetBrains live template for each Quick Prompt shortcut
â˜  Documented macro for "open new chat" workflow enforcement
â˜  Template files for DECISION_*.json, VERIFICATION_*.md, PERSPECTIVE_*.md
MONITORING & REPORTING
â˜  Weekly script that generates model scorecard from DECISION_*.json logs
â˜  Dashboard showing 90-day rolling defect/incident/rollback metrics per model
â˜  Alert for models exceeding defect threshold â†’ trigger role reassignment review
â˜  Quarterly SOP compliance audit checklist
TRAINING & ONBOARDING
â˜  New engineer onboarding includes SOP walkthrough + quiz
â˜  Recorded video demonstrating T1, T2, T3 workflows end-to-end
â˜  Cheat sheet posted in team wiki / Notion / Confluence
â˜  Monthly "SOP office hours" for questions and edge cases


Purpose

This SOP is designed so SparkData's AI-assisted development practices align with emerging "gold standard" guidance on quality management and AI governance, including:

AICPA Quality Management Standards (SQMS No. 1)
Risk-based quality management with documented controls, monitoring, and remediation for engagements that rely on technology. These standards must be implemented by Dec 15, 2025 for firms subject to them.

Key Requirements:
	â€¢	Design, implement, and operate a system of quality management
	â€¢	Risk-based approach to quality controls
	â€¢	Monitoring and remediation processes
	â€¢	Documentation of technology's role in engagements

Our Alignment:
	â€¢	Independent Verification Agent (Â§1.1.1)
	â€¢	Structured Decision Logging (Â§1.1.2)
	â€¢	Automated Quality Feedback Loop (Â§1.1.5)

EU AI Act (High-Risk Systems)
Risk-based obligations on AI providers and professional users, including transparency, technical documentation, human oversight, and risk management for "high-risk" AI systems and general-purpose AI.

Key Requirements:
	â€¢	Technical documentation and logging
	â€¢	Human oversight mechanisms
	â€¢	Risk management and mitigation
	â€¢	Transparency about AI use

Our Alignment:
	â€¢	Structured Decision Logging with JSON schema (Â§1.1.2)
	â€¢	Human Override fields in all decision records
	â€¢	Multi-perspective review for high-risk changes (Â§1.1.3)
	â€¢	Differential Model Comparison Gates (Â§1.1.4)

PCAOB & Audit-Quality Guidance on AI Use
Emphasis on understanding how AI tools affect quality, documenting their role, and implementing risk management controls around technology-assisted analysis.

Key Requirements:
	â€¢	Understand AI's impact on engagement quality
	â€¢	Document AI's role in analysis
	â€¢	Implement risk management controls
	â€¢	Professional skepticism when using AI outputs

Our Alignment:
	â€¢	Independent Verification Agent with adversarial review (Â§1.1.1)
	â€¢	Perspective Diversity Protocol (Â§1.1.3)
	â€¢	Devil's Advocate Review for suspicious agreement
	â€¢	Complete audit trails

Enterprise Agentic AI Patterns (Infosys, Grid Dynamics, VerifAI & Others)
Multi-agent, tool-using AI systems that follow a sense-plan-act loop, with orchestration, guardrails, and assurance built in.

Key Patterns:
	â€¢	Multi-agent architecture with specialized roles
	â€¢	Tool-using agents with verification loops
	â€¢	Guardrails and safety checks
	â€¢	Continuous improvement via feedback

Our Alignment:
	â€¢	Planner/Builder/Reviewer/Orchestrator/Auditor roles (Â§3)
	â€¢	OODA Loop for agentic pipeline construction (Â§17)
	â€¢	Differential Model Comparison Gates (Â§1.1.4)
	â€¢	Automated Quality Feedback Loop (Â§1.1.5)



Important Disclaimer

This SOP is not legal advice. Formal compliance with these frameworks requires legal and compliance review. The goal is to ensure that SparkData's engineering practices naturally support:

	â€¢	âœ“ Independent verification (no single agent is trusted by default)
	â€¢	âœ“ Complete, queryable audit trails for AI-assisted decisions
	â€¢	âœ“ Multi-perspective reasoning and challenge
	â€¢	âœ“ Continuous monitoring and improvement of AI impact on quality



Standards Alignment Matrix


Requirement
AICPA QM
EU AI Act
PCAOB
Agentic AI
SOP Section
Independent verification
âœ“
âœ“
âœ“
âœ“
Â§1.1.1, Â§9
Audit trail / logging
âœ“
âœ“
âœ“
âœ“
Â§1.1.2, Â§13
Multi-perspective review
âœ“
âœ“
âœ“
âœ“
Â§1.1.3, Â§10
Model comparison
 
âœ“
 
âœ“
Â§1.1.4, Â§15
Quality monitoring
âœ“
âœ“
âœ“
âœ“
Â§1.1.5, Â§14
Human oversight
 
âœ“
âœ“
âœ“
Â§15
Risk management
âœ“
âœ“
âœ“
 
Â§8
Technical documentation
 
âœ“
âœ“
 
Â§13




Section 1: Core Principles for Long-Running Projects

The Five Pillars (SparkData)

SparkData Analytics operates on five foundational principles. All AI assistants and workflows must adhere to these.

Pillar 1: Reproducibility
Every action must be repeatable with:
	â€¢	Explicit commands (no "it should work")
	â€¢	Versioned dependencies (lock files required)
	â€¢	Deterministic builds (same input â†’ same output)
	â€¢	Environment fingerprints (tool versions captured)

Pillar 2: Observability
You can't fix what you can't see:
	â€¢	Every operation produces logs
	â€¢	Every artifact has provenance
	â€¢	Every decision is documented
	â€¢	Every metric is captured

Pillar 3: Idempotency
Running the same operation twice must be safe:
	â€¢	Database migrations are reversible
	â€¢	Pipeline re-runs don't duplicate data
	â€¢	Partial failures can be resumed
	â€¢	State is explicitly managed

Pillar 4: Progressive Context
Knowledge compounds over time:
	â€¢	Each session builds on previous work
	â€¢	Links between sessions are explicit
	â€¢	Evolution is tracked in CHANGELOG
	â€¢	Tribal knowledge is captured in docs

Pillar 5: Independent Verification & Multi-Perspective Reasoning âš ï¸ NEW

No single model's output is trusted on its own for material changes.

Critical work (pipelines, workflows, financial/mission-critical logic) must pass through:
	â€¢	An independent verification agent, and
	â€¢	Where appropriate, multiple specialized review perspectives (security-first, performance-first, maintainability-first, resilience-first)

This directly supports:
	â€¢	AICPA QM: Risk-based quality controls, monitoring, and remediation
	â€¢	EU AI Act: Technical robustness, oversight, and documentation of AI-assisted decisions
	â€¢	PCAOB guidance: Understanding and controlling how AI affects quality of work



Section 1.1: Multi-Agent Governance & Quality Controls (SparkData)

SparkData's AI workflows are explicitly multi-agent and evidence-driven. We use five key patterns:

1. Independent Verification Agent (Quick Win #1)
2. Structured Decision Logging (Quick Win #2)
3. Perspective Diversity Protocol (Quick Win #3)
4. Differential Model Comparison Gates (Quick Win #4)
5. Automated Quality Feedback Loop (Quick Win #5)



1.1.1 Independent Verification Agent (Quick Win #1)

Definition

For any non-trivial change (especially data pipelines, workflows, and production services), a Verification Agent reviews the work independently of the Implementing Agent.

How It Works

	â€¢	All candidate solutions (from one or more models) are generated from the same original requirement (ticket, spec, ADR), not from each other's explanations.
	â€¢	The Verification Agent sees:
  - The requirement
  - The git diff / proposed changes
  - Test + lint + pipeline-check output
	â€¢	The Verification Agent does NOT see:
  - The Implementing Agent's reasoning steps
  - Any previous "self-review" text
  - Other reviewers' findings (until consensus phase)

Verification Agent Responsibilities

1. Reconstruct the intended behavior from the requirement (3-5 bullets)
2. Walk the diff hunk-by-hunk and identify:
   - Logic defects, edge cases, error handling gaps
   - Data integrity and schema issues (for pipelines)
   - Security, performance, and reliability concerns
3. Evaluate tests and tooling:
   - Which behaviors are covered?
   - Which tests are missing?
4. Issue a decision:
   - BLOCKING â€“ must be fixed before merge
   - NON-BLOCKING â€“ can merge but follow-ups created
   - APPROVE WITH RISKS â€“ risks documented

Verification Agent Prompt Template


## Independent Verification Agent
## [AICPA QM / EU AI Act / PCAOB Aligned]
 
You are a **Verification Agent** for SparkData Analytics.
 
### Your Independence Guarantee
- You have NOT seen the Implementing Agent's reasoning
- You are NOT trying to confirm their conclusions
- You are independently assessing correctness from first principles
 
### Your Inputs
**REQUIREMENT:** [Original ticket/spec - paste here]
**DIFF:** [git diff output - paste here]
**TOOL OUTPUT:** [test/lint/type results - paste here]
 
### Your Tasks
 
#### Task 1: Reconstruct Intent
From the REQUIREMENT alone, write 3-5 bullet points describing what this change should accomplish.
 
#### Task 2: Diff Analysis
Walk through the diff hunk-by-hunk. For each significant change:
- Does it correctly implement the intent?
- What edge cases might it miss?
- Are there error handling gaps?
- For pipelines: schema issues, data integrity risks?
 
#### Task 3: Test Evaluation
- Which behaviors are tested?
- Which behaviors are NOT tested but should be?
- Are tests meaningful or just coverage padding?
 
#### Task 4: Issue Decision
Choose ONE:
- **BLOCKING:** [List issues that must be fixed]
- **NON-BLOCKING:** [List issues for follow-up tickets]
- **APPROVE WITH RISKS:** [Document accepted risks]
 
### Output Format

Verification Report

Reconstructed Intent
	â€¢	[Bullet 1]
	â€¢	[Bullet 2]
	â€¢	[Bullet 3]

Diff Analysis

Hunk
Assessment
Issues
[location]
[OK/CONCERN]
[description]


Test Coverage
	â€¢	Covered: [list]
	â€¢	Missing: [list]

Decision: [BLOCKING / NON-BLOCKING / APPROVE WITH RISKS]

Issues

#
Severity
Category
Description
Location
1
[H/M/L]
[category]
[description]
[file:line]


Certification
This verification was conducted independently without access to the implementer's reasoning.

Alignment
	â€¢	AICPA QM / PCAOB: Independent checks and risk-based quality controls
	â€¢	EU AI Act: Human-in-the-loop oversight plus technical documentation of decisions



1.1.2 Structured Decision Logging (Quick Win #2)

Definition

Every material AI-assisted decision is captured as a structured JSON record for auditability.

Storage

	â€¢	Directory: logs/ai-decisions/YYYY/MM/DD/
	â€¢	File naming: DECISION_[TIMESTAMP]_[PROJECT]_[ARTIFACT].json

Standard Schema (v1)


{
  "id": "uuid-or-hash",
  "timestamp": "ISO-8601",
  "project": "sparkdata-[repo-or-domain]",
  "artifact_type": "code|pipeline|config|doc",
  "artifact_ref": "path/or/PR-number",
  
  "models_involved": [
    {
      "name": "claude-opus-4",
      "role": "builder|verifier|security-review|performance-review",
      "version": "2025-11",
      "temperature": 0.2
    },
    {
      "name": "gpt-5-codex",
      "role": "verifier",
      "version": "2025-11",
      "temperature": 0.1
    }
  ],
  
  "input_context": {
    "ticket_id": "JIRA-123",
    "summary": "Short business description of the change",
    "files_touched": ["path/file1.py", "etl/some_pipeline.sql"],
    "risk_level": "low|medium|high|critical"
  },
  
  "decision": {
    "type": "approve|reject|request_changes",
    "rationale": "Short natural-language justification",
    "issues_found": [
      {
        "severity": "critical|high|medium|low",
        "category": "correctness|data_quality|security|performance|maintainability",
        "location": "file:line-range",
        "description": "What is wrong and why",
        "suggested_fix": "Concrete change recommendation"
      }
    ]
  },
  
  "metrics_snapshot": {
    "tests": {"passed": 124, "failed": 0, "skipped": 3},
    "coverage": {"line": 0.86, "branch": 0.78},
    "lint_errors": 0,
    "type_errors": 0,
    "pipeline_checks": {"passed": 15, "failed": 0}
  },
  
  "human_override": {
    "status": "none|approved|overruled",
    "approver": "name/email",
    "timestamp": "ISO-8601",
    "notes": "Why human overrode the AI decision"
  },
  
  "outcome_tracking": {
    "deployed_at": null,
    "incidents_30d": null,
    "incidents_60d": null,
    "incidents_90d": null,
    "rollback": false
  }
}


Usage

These records support:
	â€¢	EU AI Act: Technical documentation and traceability obligations
	â€¢	AICPA/PCAOB: Monitoring quality and the impact of technology on engagement quality
	â€¢	Internal analytics: Feed into Automated Quality Feedback Loop (Â§1.1.5)

Decision Log Query Examples


# Find all decisions with critical issues
jq 'select(.decision.issues_found[] | .severity == "critical")' logs/ai-decisions/**/*.json
 
# Find all decisions requiring human override
jq 'select(.human_override.status != "none")' logs/ai-decisions/**/*.json
 
# Find decisions by model
jq 'select(.models_involved[] | .name == "claude-opus-4")' logs/ai-decisions/**/*.json
 
# Find high-risk changes
jq 'select(.input_context.risk_level == "high" or .input_context.risk_level == "critical")' logs/ai-decisions/**/*.json




1.1.3 Perspective Diversity Protocol (Quick Win #3)

Definition

For high-risk changes (core data models, security-sensitive code, critical pipelines), we run multiple specialized review agents, each with a distinct perspective:

	â€¢	Security-First Reviewer
	â€¢	Performance-First Reviewer
	â€¢	Maintainability-First Reviewer
	â€¢	Resilience-First Reviewer (failure modes, backfills, retries, disaster scenarios)

Process

1. All reviewers see the same requirement + diff + tool output
2. Each reviewer is instructed to optimize for its own dimension only, and to assume other concerns will be handled by other reviewers
3. We measure similarity between their issue lists
4. If outputs are nearly identical (cosine similarity > 0.9 on issue descriptions), we treat it as a RED FLAG that perspectives are not diverse enough
5. If similarity is too high, trigger a Devil's Advocate Reviewer

Perspective-Specific Prompts

Security-First Reviewer

You are the SECURITY-FIRST Reviewer. Your ONLY job is to find security issues.
Assume performance, maintainability, and resilience are someone else's problem.
 
Focus areas:
- Authentication and authorization gaps
- Injection vulnerabilities (SQL, command, XSS)
- Secrets and credential exposure
- Data leakage and privacy violations
- OWASP Top 10 issues
- Input validation failures


Performance-First Reviewer

You are the PERFORMANCE-FIRST Reviewer. Your ONLY job is to find performance issues.
Assume security, maintainability, and resilience are someone else's problem.
 
Focus areas:
- Algorithmic complexity (O(nÂ²) or worse)
- N+1 query patterns
- Memory leaks or unbounded growth
- Blocking operations in async contexts
- Missing caching opportunities
- Scale limitations


Maintainability-First Reviewer

You are the MAINTAINABILITY-FIRST Reviewer. Your ONLY job is to find maintainability issues.
Assume security, performance, and resilience are someone else's problem.
 
Focus areas:
- Code clarity and readability
- Adherence to project patterns
- Coupling and cohesion
- Technical debt introduction
- Documentation gaps
- Test quality and coverage


Resilience-First Reviewer

You are the RESILIENCE-FIRST Reviewer. Your ONLY job is to find resilience issues.
Assume security, performance, and maintainability are someone else's problem.
 
Focus areas:
- Failure mode handling
- Timeout and retry logic
- Idempotency guarantees
- Backfill and recovery paths
- Partial failure scenarios
- Disaster recovery implications


Devil's Advocate Reviewer (Triggered When Similarity > 0.9)


## Devil's Advocate Reviewer
## Triggered because other reviewers agreed too strongly
 
You are the DEVIL'S ADVOCATE Reviewer.
 
**Assume the implementation is subtly wrong in SOME way.**
 
Your job is to find risks that others might have missed, even if low-probability but high-impact.
 
Consider:
- What assumptions does this code make that aren't documented?
- What happens at 100x scale?
- What happens with malicious input?
- What happens during network partition?
- What similar code has failed before?
- What would a hostile attacker try?
 
You MUST find at least 2 potential issues, even if unlikely.
"Looks good" is not acceptable from you.


Alignment
	â€¢	Mirrors multi-perspective review practices in audit and safety-critical engineering
	â€¢	Supports EU AI Act's emphasis on risk management and robustness by forcing the system to look for different categories of failure
	â€¢	Implements PCAOB's professional skepticism requirement



1.1.4 Differential Model Comparison Gates (Quick Win #4)

Definition

For certain classes of work, we deliberately use multiple models (e.g., Claude Opus + GPT-5) and compare their outputs before committing.

When to Use

	â€¢	New pattern / technology (e.g., first time using a new data store)
	â€¢	Security-critical code
	â€¢	Fundamental data model changes
	â€¢	Core business logic
	â€¢	Anything rated "high" or "critical" risk

How It Works


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DIFFERENTIAL MODEL COMPARISON GATE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Step 1: PARALLEL PROPOSALS                                                 â”‚
â”‚  â”œâ”€ Model A (Claude) implements from spec                                   â”‚
â”‚  â”œâ”€ Model B (GPT-5) implements from spec                                    â”‚
â”‚  â””â”€ Neither sees the other's work                                           â”‚
â”‚                                                                             â”‚
â”‚  Step 2: DIFF & DIVERGENCE ANALYSIS                                         â”‚
â”‚  â”œâ”€ Comparison Agent highlights where implementations differ                â”‚
â”‚  â””â”€ Classifies differences: correctness, data quality, security,            â”‚
â”‚     performance, style                                                      â”‚
â”‚                                                                             â”‚
â”‚  Step 3: RISK MATRIX                                                        â”‚
â”‚  For each difference:                                                       â”‚
â”‚  â”œâ”€ Which model is more conservative on validation?                         â”‚
â”‚  â”œâ”€ Which introduces more complexity?                                       â”‚
â”‚  â”œâ”€ Which tests are stronger?                                               â”‚
â”‚  â””â”€ Which handles edge cases better?                                        â”‚
â”‚                                                                             â”‚
â”‚  Step 4: GATE DECISION                                                      â”‚
â”‚  Choose:                                                                    â”‚
â”‚  â”œâ”€ "Model A" (use A's implementation)                                      â”‚
â”‚  â”œâ”€ "Model B" (use B's implementation)                                      â”‚
â”‚  â””â”€ "Hybrid" (explicitly combining best parts)                              â”‚
â”‚                                                                             â”‚
â”‚  Decision and reasoning â†’ Structured Decision Log                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Comparison Agent Prompt


## Differential Model Comparison Agent
 
You are comparing implementations from TWO different models for the same requirement.
 
### Inputs
**REQUIREMENT:** [Original spec]
**IMPLEMENTATION A:** [Model A's code]
**IMPLEMENTATION B:** [Model B's code]
 
### Your Tasks
 
#### 1. Identify Differences
List all meaningful differences (ignore whitespace, naming style):
 
| # | Location | Model A | Model B | Category |
|---|----------|---------|---------|----------|
| 1 | [where] | [what A does] | [what B does] | [correctness/security/perf/etc] |
 
#### 2. Assess Each Difference
For each difference:
- Which approach is more correct?
- Which is more secure?
- Which handles errors better?
- Which is more maintainable?
 
#### 3. Risk Matrix
| Difference | Model A Risk | Model B Risk | Recommendation |
|------------|--------------|--------------|----------------|
| [diff 1] | [H/M/L] | [H/M/L] | [A/B/Hybrid] |
 
#### 4. Final Recommendation
- **USE MODEL A** because: [rationale]
- **USE MODEL B** because: [rationale]
- **USE HYBRID** combining: [what from each]
 
### Output

{
  "recommendation": "A|B|HYBRID",
  "rationale": "...",
  "differences_found": 5,
  "model_a_advantages": ["..."],
  "model_b_advantages": ["..."],
  "hybrid_plan": "..." // if HYBRID
}

Alignment
	â€¢	Similar to ensemble and multi-agent patterns seen in production agentic AI design
	â€¢	Reduces the risk of a single-model blind spot driving a critical defect
	â€¢	Supports EU AI Act requirements for robust AI systems



1.1.5 Automated Quality Feedback Loop (Quick Win #5)

Objective

Track real-world outcomes of AI-assisted changes and feed that data back into:
	â€¢	Model selection
	â€¢	Prompt design
	â€¢	Review intensity (how many reviewers, which perspectives)

Data Collected

For each merged change, we log (manually or via automation):


Field
Description
Source
origin_model
Which model(s) authored and verified
Decision Log
risk_level
Low / medium / high / critical
Decision Log
post_deploy_incidents
Number and severity of incidents
Incident tracker
bug_reports
Count within 30/60/90 days
Bug tracker
rollbacks
Whether the change was rolled back
Deploy system
SLO_impact
Any SLO/SLA breaches linked to this change
Monitoring
data_quality_incidents
For pipelines: failed expectations, schema regressions
dbt/Great Expectations


Model Scorecard

Compute per-model stats over rolling windows (e.g., 90 days):


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                           MODEL SCORECARD - Q4 2025                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  Model: claude-opus-4                                                         â•‘
â•‘  Period: 2025-09-01 to 2025-11-30                                            â•‘
â•‘                                                                               â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚ ROLE          â”‚ CHANGES â”‚ DEFECTS â”‚ INCIDENTS â”‚ ROLLBACKS â”‚ SCORE      â”‚ â•‘
â•‘  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ â•‘
â•‘  â”‚ Builder       â”‚ 78      â”‚ 2       â”‚ 0         â”‚ 1         â”‚ 94/100     â”‚ â•‘
â•‘  â”‚ Verifier      â”‚ 120     â”‚ FN: 3   â”‚ -         â”‚ -         â”‚ 92/100     â”‚ â•‘
â•‘  â”‚ Security Rev  â”‚ 45      â”‚ FN: 1   â”‚ -         â”‚ -         â”‚ 96/100     â”‚ â•‘
â•‘  â”‚ Perf Review   â”‚ 38      â”‚ FN: 2   â”‚ -         â”‚ -         â”‚ 91/100     â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘                                                                               â•‘
â•‘  Best For: Security reviews, complex data pipelines                          â•‘
â•‘  Caution: Performance edge cases                                             â•‘
â•‘                                                                               â•‘
â•‘  Trend: STABLE (no significant change from Q3)                               â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


Scorecard Metrics


Metric
Formula
Target
Warning
Defect Rate
defects / changes
< 2%
> 5%
Incident Rate
incidents / changes
< 1%
> 3%
Rollback Rate
rollbacks / changes
< 1%
> 2%
False Negative Rate
missed issues / total issues
< 10%
> 20%
Mean Time to Remediate
avg days to fix related incidents
< 2 days
> 5 days


Policy

For similar future work, prefer:
	â€¢	The model + pattern with the best track record for that domain

If a model shows sustained poor performance:
	â€¢	Reduce its role to low-risk or sandbox work
	â€¢	Adjust prompts or deprecate its use
	â€¢	Document in ADR

Monthly Review Process


# Generate scorecards
./scripts/generate-model-scorecards.sh --period 2025-11
 
# Review trends
./scripts/compare-scorecards.sh --current 2025-11 --previous 2025-10
 
# Update preferences
vim config/model-preferences.json


Alignment
	â€¢	Directly addresses regulator concerns that firms must understand and monitor how AI affects quality, not just "use AI and hope"
	â€¢	Fits AICPA QM / PCAOB expectations for monitoring and remediation
	â€¢	Supports EU AI Act's emphasis on ongoing risk management for AI systems



Section 2: Session Management

Cross-Links to Multi-Agent Governance

Session Initialization Enhancement

When starting a session that will involve AI-assisted code or pipeline changes, identify:


## Session Risk Assessment
 
**Risk Level:** [LOW | MEDIUM | HIGH | CRITICAL]
 
**Governance Requirements:**
- [ ] Independent Verification Agent required? (HIGH/CRITICAL: YES)
- [ ] Perspective Diversity review required? (CRITICAL: YES)
- [ ] Differential Model Comparison required? (CRITICAL: YES)
- [ ] Human override capability required? (CRITICAL: YES)




Section 2.1: Essential Workflows

Workflow 4: Multi-Agent AI Quality Gate

Use for: All material AI-assisted changes (code, pipelines, configurations)


WORKFLOW 4: MULTI-AGENT AI QUALITY GATE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 
Phase 1: IMPLEMENT
â”œâ”€ Builder Agent creates implementation from spec
â”œâ”€ Tests written (TDD preferred)
â”œâ”€ All tool checks run (test, lint, types)
â””â”€ Output: Code + PROGRESS report
 
Phase 2: INDEPENDENT VERIFICATION (Â§1.1.1)
â”œâ”€ Fresh session (no prior context)
â”œâ”€ Verification Agent reviews: spec + diff + tool output
â”œâ”€ Does NOT see Builder's reasoning
â”œâ”€ Issues decision: BLOCKING / NON-BLOCKING / APPROVE WITH RISKS
â””â”€ Output: VERIFICATION report
 
Phase 3: PERSPECTIVE DIVERSITY (if HIGH/CRITICAL risk) (Â§1.1.3)
â”œâ”€ 4 specialized reviewers run in parallel
â”œâ”€ Each sees same inputs
â”œâ”€ Similarity check on outputs
â”œâ”€ Devil's Advocate if similarity > 0.9
â””â”€ Output: 4x PERSPECTIVE reports + DIVERGENCE analysis
 
Phase 4: DIFFERENTIAL GATE (if CRITICAL risk) (Â§1.1.4)
â”œâ”€ Second model implements same spec
â”œâ”€ Comparison Agent analyzes differences
â”œâ”€ Risk matrix generated
â”œâ”€ Choose: Model A / Model B / Hybrid
â””â”€ Output: COMPARISON report
 
Phase 5: DECISION LOGGING (Â§1.1.2)
â”œâ”€ Create structured JSON decision record
â”œâ”€ All models and sessions documented
â”œâ”€ Issues and decisions recorded
â”œâ”€ Human override field populated (if applicable)
â””â”€ Output: DECISION_*.json
 
Phase 6: MERGE OR REWORK
â”œâ”€ If BLOCKING issues: Return to Phase 1
â”œâ”€ If NON-BLOCKING: Create follow-up tickets, proceed
â”œâ”€ If APPROVED: Merge with confidence
â””â”€ Link Decision Log to PR/commit
 
Phase 7: OUTCOME TRACKING (Â§1.1.5)
â”œâ”€ Link Decision Log to deployment
â”œâ”€ Monitor for 90 days
â”œâ”€ Record incidents, defects, rollbacks
â””â”€ Feed into Model Scorecard
 
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




Section 3: AI Coding Roles

Role Definitions (with Governance Alignment)


Role
Governance Function
Standards Alignment
Planner Agent
Defines what to build
AICPA: Design controls
Builder Agent
Creates implementation
-
Verification Agent
Independent review
AICPA/PCAOB: Quality controls
Perspective Reviewers
Multi-lens analysis
EU AI Act: Robustness
Comparison Agent
Cross-model analysis
Agentic AI: Ensemble patterns
Orchestrator Agent
Tool execution
-
Auditor Agent
Compliance check
EU AI Act: Human oversight




Section 4: Testing & QA (Enhanced)

AI Quality Metrics Subsection

When AI is used to generate code or tests:

1. All decisions must be captured in the structured decision log (Â§1.1.2)
2. High-risk changes must involve the Verification Agent (Â§1.1.1)
3. Critical changes must involve multi-perspective reviews (Â§1.1.3)
4. Test coverage is verified by tool output, not model claims
5. Outcome tracking feeds into Model Scorecard (Â§1.1.5)



Section 5: Observability & Monitoring (Enhanced)

AI Quality Metrics

Add to your monitoring dashboard:


Metric
Description
Alert Threshold
ai_decisions_total
Count of AI-assisted decisions
-
ai_decisions_blocked
Decisions with BLOCKING issues
> 20%
ai_model_defect_rate
Defects per model per 100 changes
> 5%
ai_verification_disagreement
Verifier disagreed with Builder
Track
ai_perspective_similarity
Similarity between perspective reviewers
> 0.9 (alert)
ai_human_overrides
Human overrides of AI decisions
Track


This ensures AI-assisted development is itself monitored like any other system.



Part I: Foundation



1. Quick Access to Most Used Prompts {#quick-access}

âš¡ Bookmark These Five Prompts


Use Case
Section
When to Use
ğŸŸ¢ Session Start
[Â§5.1](#session-init-standard)
Beginning ANY work session
ğŸ”´ Adversarial Review
[Â§8.1](#acr-prompt)
Before committing ANY code
ğŸ¤– Agentic Pipeline
[Â§13.1](#ooda-loop)
Building complex data workflows
ğŸ“ Progress Report
[Â§6.1](#workflow-1)
End of work sessions
ğŸ¤ Complete Handoff
[Â§6.2](#workflow-2)
When another person/AI takes over


ğŸš¨ Critical Rule: Never Skip the Adversarial Review

Before ANY code is committed or marked complete:
1. Open a fresh chat session (no prior context)
2. Run the Adversarial Code Review prompt (Â§8.1)
3. Require Proof of Work artifacts (Â§11)
4. Document findings in REVIEW_[timestamp].md



2. Core Principles: The Five Pillars {#five-pillars}

SparkData Analytics operates on five foundational principles. All AI assistants and workflows must adhere to these.

Pillar 1: Reproducibility
Every action must be repeatable with:
	â€¢	Explicit commands (no "it should work")
	â€¢	Versioned dependencies (lock files required)
	â€¢	Deterministic builds (same input â†’ same output)
	â€¢	Environment fingerprints (tool versions captured)

Pillar 2: Observability
You can't fix what you can't see:
	â€¢	Every operation produces logs
	â€¢	Every artifact has provenance
	â€¢	Every decision is documented
	â€¢	Every metric is captured

Pillar 3: Idempotency
Running the same operation twice must be safe:
	â€¢	Database migrations are reversible
	â€¢	Pipeline re-runs don't duplicate data
	â€¢	Partial failures can be resumed
	â€¢	State is explicitly managed

Pillar 4: Progressive Context
Knowledge compounds over time:
	â€¢	Each session builds on previous work
	â€¢	Links between sessions are explicit
	â€¢	Evolution is tracked in CHANGELOG
	â€¢	Tribal knowledge is captured in docs

Pillar 5: Adversarial Independence âš ï¸ NEW
No single model's output is trusted by default.

Every meaningful code or pipeline change is:
	â€¢	Backed by executable tests and invariants
	â€¢	Reviewed by an independent AI auditor that:
  - Does NOT see the previous explanation
  - Only sees: spec + code + test results
  - Is explicitly tasked with finding problems, not agreeing

  **The Echo Chamber Problem:** LLMs are trained to be helpful. If an AI reads another AI's reasoning, it will tend to agree to remain "consistent." This is why Reviewer Agents must operate blind to Builder explanations.



3. AI Coding Roles & Modes {#ai-roles}

SparkData uses role separation to prevent sycophancy and ensure quality. Each role has distinct permissions and constraints.

3.1 Role Definitions


Role
Responsibility
Has Access To
Does NOT Have Access To
Planner Agent
Convert requirements â†’ technical plan
Requirements, existing architecture, ADRs
Implementation details
Builder Agent
Write/modify code per spec
Spec, codebase, test framework
Review authority
Reviewer Agent
Find problems, approve/reject
Spec, diffs, test output, standards
Builder's explanations
Orchestrator Agent
Run tools, coordinate loop
CLI, test runners, linters
Decision authority
Auditor Agent
Compliance, security, governance
All artifacts, policies
Modification authority


3.2 Critical Rule: Separation of Concerns


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEVER DO THIS                                    â”‚
â”‚    Builder Agent reviews its own code in the same session           â”‚
â”‚    Reviewer Agent sees Builder's explanation before reviewing       â”‚
â”‚    Single chat thread handles both implementation AND review        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ALWAYS DO THIS                                   â”‚
â”‚    Builder Agent completes work â†’ artifacts saved                   â”‚
â”‚    Fresh session opened for Reviewer Agent                          â”‚
â”‚    Reviewer only sees: spec + code + test results                   â”‚
â”‚    Different models for Builder vs Reviewer when possible           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


3.2.1 ğŸ”´ HARD RULE: FRESH CHAT SEPARATION

â›” If a chat has ever used Builder Mode in it, you may NOT use Reviewer, Verification, or any Lens prompts in the same chat. Start a new chat.

This is non-negotiable. Mixing roles in the same chat defeats the entire purpose of independent verification and creates the "echo chamber" problem this SOP is designed to prevent.

Recommended Convention: Name your chat windows with role prefixes:
	â€¢	[B] Feature X implementation
	â€¢	[R] Feature X review  
	â€¢	[V] Feature X verification

3.2.2 Inline AI (Autocomplete) Policy

For T2/T3 work: Any AI-generated codeâ€”including inline suggestions from Copilot, Cursor autocomplete, tab-completion, etc.â€”must still be put through:

1. A Builder Mode pass over the final diff, AND
2. The appropriate Reviewer / Verification / Lens prompts before merge

Inline assistance is allowed, but it never bypasses the multi-agent quality gate.

3.3 Mode Declaration Template

At the start of EVERY coding session, declare the mode:


## Session Declaration
 
**Session ID:** [UNIQUE_ID]
**Model:** [MODEL_NAME]
**Mode:** [PLANNER | BUILDER | REVIEWER | ORCHESTRATOR | AUDITOR]
**Timestamp:** [ISO-8601]
 
### Mode-Specific Rules Active:
[Auto-populated based on mode selection]


Mode A: Planner

You are in PLANNER mode.
- You MAY analyze requirements and propose solutions
- You MAY create task breakdowns and test plans
- You MAY NOT write implementation code
- You MUST output: plan.md, test_plan.md, acceptance_criteria.md


Mode B: Builder

You are in BUILDER mode.
- You MAY write and modify code
- You MAY write tests per the test plan
- You MAY NOT approve your own work
- You MUST leave work in "pending review" state
- You MUST create verifiable claims with evidence commands


Mode C: Reviewer

You are in REVIEWER mode.
- You MAY NOT write implementation code (tests only)
- You MAY NOT see Builder's explanations or reasoning
- You MUST verify all claims with actual tool execution
- You MUST use the adversarial review rubric
- You MUST output: REVIEW_[timestamp].md with APPROVE/REJECT/BLOCK verdict


Mode D: Orchestrator

You are in ORCHESTRATOR mode.
- You MAY run CLI tools (tests, linters, type checkers)
- You MAY coordinate between agents
- You MAY NOT make implementation decisions
- You MUST capture all tool output as artifacts


Mode E: Auditor

You are in AUDITOR mode.
- You MAY review all artifacts for compliance
- You MAY flag security, legal, or policy issues
- You MAY NOT modify code or artifacts
- You MUST output: AUDIT_[timestamp].md with findings




4. Decision Tree: What Workflow Do I Need? {#decision-tree}


START
â”‚
â”œâ”€â–º Are you BEGINNING a session?
â”‚   â”‚
â”‚   â”œâ”€â–º YES: Is this a REVIEW session?
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º YES â†’ Use "Blind Review Session Init" (Â§5.3)
â”‚   â”‚   â”‚         Do NOT load previous explanations
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º NO â†’ Use "Standard Session Init" (Â§5.1)
â”‚   â”‚             Load full context
â”‚   â”‚
â”‚   â””â”€â–º NO: Continue below
â”‚
â”œâ”€â–º Are you ENDING a session?
â”‚   â”‚
â”‚   â”œâ”€â–º Need handoff to another person/AI?
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º YES â†’ Use "Workflow 2: Progress + Handoff + State" (Â§6.2)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º NO â†’ Use "Workflow 1: Progress Report" (Â§6.1)
â”‚   â”‚
â”‚   â””â”€â–º Continue below
â”‚
â”œâ”€â–º What TYPE of work?
â”‚   â”‚
â”‚   â”œâ”€â–º NEW FEATURE / PIPELINE
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º Complex (multi-file, multi-step)?
â”‚   â”‚   â”‚   â””â”€â–º Use "Workflow 4: Agentic Coding Loop" (Â§6.4)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º Simple (single file)?
â”‚   â”‚       â””â”€â–º Use "Feature Implementation" (Â§12.1)
â”‚   â”‚
â”‚   â”œâ”€â–º CODE REVIEW / QC
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º Own code (same session)?
â”‚   â”‚   â”‚   â””â”€â–º â›” STOP. Open NEW session for review.
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º Other's code / fresh session?
â”‚   â”‚       â””â”€â–º Use "Adversarial Code Review" (Â§8.1)
â”‚   â”‚
â”‚   â”œâ”€â–º BUG FIX
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º Production incident?
â”‚   â”‚   â”‚   â””â”€â–º Use "Production Incident Response" (Â§16.1)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º Normal bug?
â”‚   â”‚       â””â”€â–º Use "TDD Bug Fix Loop" (Â§8.5)
â”‚   â”‚
â”‚   â”œâ”€â–º ARCHITECTURE DECISION
â”‚   â”‚   â””â”€â–º Use "Architecture Decision Record" (Â§14.1)
â”‚   â”‚
â”‚   â”œâ”€â–º DEBUGGING
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â–º Legacy/undocumented code?
â”‚   â”‚   â”‚   â””â”€â–º Use "Code Archaeology" (Â§19.1)
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â–º Normal debugging?
â”‚   â”‚       â””â”€â–º Use "Rubber Duck Debugging" (Â§18.1)
â”‚   â”‚
â”‚   â””â”€â–º DATA PIPELINE
â”‚       â”‚
â”‚       â”œâ”€â–º New pipeline?
â”‚       â”‚   â””â”€â–º Use "Agentic Pipeline Construction" (Â§13.1)
â”‚       â”‚
â”‚       â””â”€â–º Modify existing?
â”‚           â””â”€â–º Use "Pipeline Modification Checklist" (Â§24.2)
â”‚
â””â”€â–º NONE OF THE ABOVE
    â””â”€â–º Browse Part IV-VII for specific needs




File Naming Standards

All artifacts follow consistent naming for traceability:


Artifact Type
Pattern
Example
Progress Reports
PROGRESS_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION_ID].md
PROGRESS_2025-11-30_14-30-00_claude-opus_abc123.md
Handoff Documents
docs/handoffs/HANDOFF_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION_ID].md
docs/handoffs/HANDOFF_2025-11-30_14-30-00_gpt5_xyz789.md
Review Documents
REVIEW_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION_ID].md
REVIEW_2025-11-30_15-00-00_claude-sonnet_def456.md
Discrepancy Reports
DISCREPANCY_YYYY-MM-DD_HH-MM-SS.md
DISCREPANCY_2025-11-30_15-30-00.md
Decision Records
docs/decisions/ADR_YYYY-MM-DD_[NUMBER]_[TITLE_SLUG].md
docs/decisions/ADR_2025-11-30_001_async-pipeline-architecture.md
Session States
state/session_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION_ID].json
state/session_2025-11-30_14-30-00_claude-opus_abc123.json
Audit Reports
AUDIT_YYYY-MM-DD_HH-MM-SS_[MODEL].md
AUDIT_2025-11-30_16-00-00_claude-opus.md
Test Evidence
evidence/test_YYYY-MM-DD_HH-MM-SS.txt
evidence/test_2025-11-30_14-45-00.txt
Lint Evidence
evidence/lint_YYYY-MM-DD_HH-MM-SS.txt
evidence/lint_2025-11-30_14-46-00.txt


Model Identifiers

Use consistent model identifiers across all artifacts:


Model
Identifier
Claude Opus 4
claude-opus
Claude Sonnet 4
claude-sonnet
GPT-5 Codex
gpt5-codex
GPT-4o
gpt4o
Gemini 2.0
gemini2






Part II: Session Management



5. Session Initialization Protocols {#session-init}

5.1 Standard Session Initialization (Builder/Planner Mode) {#session-init-standard}

Use this when: Starting a new work session in Builder or Planner mode.


## Session Initialization - Builder/Planner Mode
 
Initialize yourself with the complete project context by reading and analyzing these files in order:
 
### Phase 1: Project Overview
Read and internalize (do NOT summarize yet):
- README.md (project purpose, setup, architecture)
- CHANGELOG.md (recent changes and project evolution)
- docs/architecture/overview.md (if exists)
- docs/SPEC.md (ground truth for requirements)
 
### Phase 2: Previous Session Context
 
#### Your Model Lineage
You are [MODEL_NAME]. 
Read YOUR most recent handoff (not handoffs from other models):
- Find: `docs/handoffs/HANDOFF_*_[YOUR_MODEL]_*.md`
- This contains context from your previous sessions
 
#### Last Progress Report
Read the most recent progress report regardless of model:
- `PROGRESS_*.md` (sorted by timestamp, most recent)
 
#### Session State
If available, load the most recent session state:
- `state/session_*.json` (most recent)
 
### Phase 3: Current Project State
 
#### Repository Status
Execute and analyze:

git status -sb
git log --oneline -10
git diff --stat HEAD~1

 
#### Build and Test Status
Execute and capture output:

Build check
make build || npm run build || cargo build 2>&1 | tee build_check.txt

Test check
make test || npm test || cargo test 2>&1 | tee test_check.txt

Lint check
make lint || npm run lint 2>&1 | tee lint_check.txt

 
#### Open Work Items
Review:
- Uncommitted changes (from git status)
- Open pull requests (if accessible)
- TODO comments in recently modified files
 
### Phase 4: Knowledge Base Integration
Read:
- docs/context/KNOWLEDGE_BASE.md (lessons learned)
- docs/decisions/ADR_*.md (most recent 3-5)
- docs/runbooks/*.md (operational procedures)
 
### Phase 5: Synthesis and Confirmation
 
After reading all context, provide:
 
#### A. Executive Summary (2-3 paragraphs)
- What this project does and why
- Current development phase and recent progress
- Your understanding of immediate priorities
 
#### B. Context Verification Checklist
- [ ] Project purpose and architecture understood
- [ ] Recent changes from CHANGELOG reviewed
- [ ] Previous session's accomplishments understood
- [ ] Current branch and uncommitted changes identified
- [ ] Your model's previous context loaded
- [ ] Immediate next steps from handoff clear
- [ ] Any blockers or warnings identified
- [ ] Key architectural decisions understood
 
#### C. Working Memory State
- Mental model of system architecture
- Key patterns and conventions noticed
- Areas of uncertainty needing clarification
- Assumptions being made
 
#### D. Action Plan
Based on context, your next 3 concrete steps:
1. [First action with specific command]
2. [Second action with specific command]
3. [Third action with specific command]
 
### Phase 6: Questions Before Starting
Ask about anything that seems:
- Contradictory between documents
- Unclear or ambiguous
- Missing but necessary
- Potentially outdated
 
---
Only proceed with actual work after completing this initialization and receiving confirmation.




5.2 Quick Session Initialization (Simple Continuations)

Use this when: Resuming simple work in the same day, no handoff occurred.


## Quick Context Refresh
 
1. Read most recent: `PROGRESS_*.md`
2. Check status: `git status -sb`
3. Review your last handoff: `docs/handoffs/HANDOFF_*_[YOUR_MODEL]_*.md`
4. Load session state if available: `state/session_*.json`
 
Provide brief summary (5-7 sentences) confirming:
- What was just completed
- Current branch and changes
- Next 2-3 immediate tasks
- Any blockers mentioned
 
Ready to continue? [Y/N]




5.3 Blind Review Session Initialization (Reviewer Mode) {#blind-review-init}

Use this when: Starting a REVIEW session. Critical: Do NOT load previous explanations.


## Session Initialization - REVIEWER MODE (Blind)
 
âš ï¸ CRITICAL: You are an INDEPENDENT REVIEWER.
You must NOT be influenced by the Builder's reasoning.
 
### What You WILL Receive:
1. Specification / ticket / requirements
2. Code diff or target files
3. Test output (pass/fail + errors)
4. Linter output
5. Type checker output
6. Project coding standards
 
### What You WILL NOT Receive:
- Builder's explanation of their changes
- Previous progress reports describing the work
- Any "here's what I did" narrative
- Handoff documents with reasoning
 
### Your Initialization:
 
#### Step 1: Load Standards Only
Read:
- docs/CODING_STANDARDS.md
- docs/architecture/overview.md (structure only, not implementation notes)
- .cursorrules or CLAUDE.md (project rules)
 
#### Step 2: Receive Review Package
I will now provide:
- SPEC: [requirement/ticket]
- DIFF: [code changes]
- TESTS: [test output]
- LINT: [linter output]
 
#### Step 3: Adopt Adversarial Mindset
Your job is to FIND PROBLEMS, not to agree.
- Assume the code is buggy until proven otherwise
- Assume tests are insufficient until verified
- Assume edge cases are unhandled until checked
- "Looks good" is NOT an acceptable conclusion without evidence
 
#### Step 4: Confirm Independence
Before proceeding, confirm:
- [ ] I have NOT seen the Builder's explanation
- [ ] I have NOT read progress reports about this work
- [ ] I am treating this code as potentially broken
- [ ] I will use the adversarial review rubric
 
Proceed with review using Â§8.1 Adversarial Code Review Protocol.




5.4 Adversarial Context Loading (For Any Mode)

Use this to validate previous handoffs as claims, not truth.

Add this to ANY session initialization when you need to verify prior work:


## Adversarial Context Loading
 
Do NOT treat previous Handoffs as truth. Treat them as CLAIMS.
 
For each claim in the previous handoff, verify independently:
 
| Claim | Verification Method | Actual Result | Match? |
|-------|---------------------|---------------|--------|
| "Feature X is complete" | Check test coverage for X | [RUN TESTS] | â“ |
| "All tests pass" | Run `make test` | [RUN TESTS] | â“ |
| "Architecture is stable" | Check git log for churn | [RUN GIT LOG] | â“ |
| "No lint errors" | Run `make lint` | [RUN LINT] | â“ |
 
### Discrepancy Report
If ANY verification fails, immediately report:
 

DISCREPANCY DETECTED

Claim: [What the handoff said]
Reality: [What verification showed]
Evidence: [Command output or file reference]
Impact: [What this means for the work]
Recommended Action: [What to do about it]

 
Do NOT proceed with work that builds on unverified claims.




5.5 Context Verification Script

Run this script before starting any AI session to gather context:


#!/bin/bash
# gather-context.sh - Run before starting AI session
 
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘         SparkData Analytics - Context Gathering            â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
 
echo -e "\nğŸ“ Recent Progress Reports:"
ls -la PROGRESS_*.md 2>/dev/null | tail -5 || echo "  None found"
 
echo -e "\nğŸ“‹ Recent Handoffs:"
ls -la docs/handoffs/HANDOFF_*.md 2>/dev/null | tail -5 || echo "  None found"
 
echo -e "\nğŸ”´ Recent Reviews:"
ls -la REVIEW_*.md 2>/dev/null | tail -5 || echo "  None found"
 
echo -e "\nğŸ”„ Git Log (last 10 commits):"
git log --oneline -10
 
echo -e "\nğŸ“Š Current Git Status:"
git status -sb
 
echo -e "\nâœ… Test Status:"
make test 2>&1 | tail -20 || npm test 2>&1 | tail -20 || echo "No test command found"
 
echo -e "\nğŸ” Lint Status:"
make lint 2>&1 | tail -20 || npm run lint 2>&1 | tail -20 || echo "No lint command found"
 
echo -e "\nğŸ“ Recent CHANGELOG entries:"
head -30 CHANGELOG.md 2>/dev/null || echo "  No CHANGELOG.md found"
 
echo -e "\nğŸ’¾ Session States Available:"
ls -la state/session_*.json 2>/dev/null | tail -3 || echo "  None found"
 
echo -e "\nğŸ—ï¸ Build Status:"
make build 2>&1 | tail -10 || npm run build 2>&1 | tail -10 || echo "No build command found"
 
echo -e "\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Context gathering complete."
echo "Copy relevant output to provide to AI if needed."
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"




6. Essential Workflows {#essential-workflows}

6.1 Workflow 1: Progress Report with Context Snapshot {#workflow-1}

Trigger: End of any work session.


## Progress Report Generation
 
Generate a comprehensive progress report with this structure:
 
**Filename:** `PROGRESS_[YYYY-MM-DD]_[HH-MM-SS]_[MODEL]_[SESSION_ID].md`
 
### Required Sections:
 
#### 1. Session Metadata

timestamp: [ISO-8601]
duration_hours: [X.X]
model: [MODEL_NAME]
session_id: [UNIQUE_ID]
mode: [BUILDER|PLANNER|REVIEWER|etc]
git_commit_range: [START_SHA..END_SHA]
environment:
  node: [version]
  python: [version]
  [other tools]: [versions]

 
#### 2. Objectives & Outcomes
| Objective | Status | Notes |
|-----------|--------|-------|
| [What you set out to do] | âœ…/ğŸ”„/âŒ | [What actually happened] |
 
Deviations from plan and why:
- [Deviation 1]: [Reason]
 
#### 3. Technical Decisions Made
| Decision | Rationale | Alternatives Rejected | Reversibility |
|----------|-----------|----------------------|---------------|
| [Choice] | [Why] | [Other options] | Easy/Medium/Hard |
 
#### 4. Code Changes
| File | Change Type | Purpose |
|------|-------------|---------|
| [path/file.py] | Modified | [Why changed] |
 
New dependencies added:
- [package@version]: [Why needed]
 
Breaking changes introduced:
- [Change]: [Impact]
 
#### 5. Testing & Validation

Tests written/updated: [count]
Coverage before: [X%]
Coverage after: [Y%]
Performance impact: [measured/not measured]

 
Attach: `evidence/test_[timestamp].txt`
 
#### 6. Knowledge Gained
- Insights about the codebase: [List]
- Gotchas discovered: [List]
- Patterns identified: [List]
 
#### 7. Verification Status âš ï¸ REQUIRED
| Check | Command | Result | Evidence File |
|-------|---------|--------|---------------|
| Tests | `make test` | PASS/FAIL | `evidence/test_*.txt` |
| Lint | `make lint` | PASS/FAIL | `evidence/lint_*.txt` |
| Types | `make typecheck` | PASS/FAIL | `evidence/types_*.txt` |
| Build | `make build` | PASS/FAIL | `evidence/build_*.txt` |
 
**Auditor Risk Score:** [0-10] (if review occurred)
 
#### 8. Obstacles & Blockers
- Unresolved issues: [List]
- External dependencies waiting: [List]
- Knowledge gaps identified: [List]
 
#### 9. Next Session Setup
Immediate next steps with commands:
1. `[command 1]` - [purpose]
2. `[command 2]` - [purpose]
3. `[command 3]` - [purpose]
 
Prerequisites for continuation:
- [Prerequisite 1]
- [Prerequisite 2]
 
Time estimates for remaining work:
- [Task]: [hours]
 
### After Creating Report:
1. Update CHANGELOG.md with entry referencing this report
2. Create git commit: `git commit -m "progress: [summary]"`
3. Push to feature branch (never directly to main)
4. Update project board if applicable




6.2 Workflow 2: Complete Handoff with State Serialization {#workflow-2}

Trigger: When another person or AI needs to take over with full context.

Task 1: Generate Progress Report
Use Workflow 1 template above.

Task 2: State Serialization

Filename: state/session_[TIMESTAMP]_[MODEL]_[SESSION_ID].json


{
  "session_metadata": {
    "timestamp": "ISO-8601",
    "duration_hours": 0.0,
    "model": "MODEL_NAME",
    "session_id": "UNIQUE_ID",
    "mode": "BUILDER|PLANNER|REVIEWER",
    "operator": "NAME/ID"
  },
  "cognitive_context": {
    "problem_understanding": "Current mental model of the problem",
    "attempted_solutions": [
      {"approach": "Description", "outcome": "Result", "learning": "What we learned"}
    ],
    "key_insights": ["Breakthroughs and realizations"],
    "dead_ends": ["What didn't work and why"],
    "assumptions": ["What we're taking as given"],
    "uncertainties": ["What we still don't understand"]
  },
  "code_state": {
    "modified_files": {
      "path/file.py": "purpose of changes"
    },
    "unstaged_changes": ["Files with uncommitted work"],
    "experimental_branches": {
      "branch-name": "purpose"
    },
    "breaking_changes": ["Changes that affect other systems"],
    "technical_debt_added": ["Shortcuts taken that need addressing"]
  },
  "environmental_context": {
    "tool_versions": {
      "node": "version",
      "python": "version"
    },
    "active_services": ["Services that must be running"],
    "environment_variables": ["Required vars (not values)"],
    "external_dependencies": ["APIs, databases, services"]
  },
  "verification_state": {
    "tests_passing": true,
    "lint_clean": true,
    "types_clean": true,
    "coverage_percentage": 0.0,
    "last_verification": "ISO-8601",
    "evidence_files": ["paths to evidence"]
  },
  "decision_log": [
    {
      "timestamp": "ISO-8601",
      "decision": "What was decided",
      "rationale": "Why this choice",
      "alternatives": ["Other options considered"],
      "confidence": 0.8,
      "reversibility": "easy|moderate|hard"
    }
  ],
  "continuation_plan": {
    "immediate_tasks": ["Next 1-3 concrete steps"],
    "investigation_needed": ["Questions to answer"],
    "blocked_items": ["What's waiting on external factors"],
    "time_estimates": {
      "task": "estimated_hours"
    }
  },
  "ai_agent_history": {
    "planner_sessions": ["session_ids"],
    "builder_sessions": ["session_ids"],
    "reviewer_sessions": ["session_ids"],
    "disagreements": ["Any conflicts between agents"],
    "overridden_decisions": ["Decisions changed after review"]
  }
}


Task 3: Handoff Document

Filename: docs/handoffs/HANDOFF_[TIMESTAMP]_[MODEL]_[SESSION_ID].md


# Project Handoff - SparkData Analytics
 
**Generated:** [YYYY-MM-DD HH:MM:SS UTC]
**Handoff From:** [MODEL/PERSON] â†’ **To:** [MODEL/PERSON]
**Session ID:** [UNIQUE_ID]
 
## Quick Orientation
 
You are taking over [PROJECT/FEATURE], a [TYPE] that [ONE_SENTENCE_PURPOSE].
 
Current phase: [design/development/testing/maintenance]
 
## âš ï¸ Critical Information - Read First
 
### Active Issues
[List any ongoing problems, outages, or time-sensitive matters]
 
### Do Not Touch
[List any fragile areas or code that shouldn't be modified]
 
### Key Decisions Already Made
1. [Decision]: [Brief rationale]
2. [Decision]: [Brief rationale]
3. [Decision]: [Brief rationale]
 
## System State
 
### Repository
- URL: [REPO_URL]
- Branch: `[CURRENT_BRANCH]` (from `[PARENT]` at `[SHA]`)
- Last Commit: `[SHA]` - [MESSAGE]
- Uncommitted Changes: [YES/NO - list if yes]
 
### Verification Status
| Check | Status | Evidence |
|-------|--------|----------|
| Build | âœ…/âŒ | `evidence/build_*.txt` |
| Tests | âœ…/âŒ | `evidence/test_*.txt` |
| Lint | âœ…/âŒ | `evidence/lint_*.txt` |
| Types | âœ…/âŒ | `evidence/types_*.txt` |
 
### Data & State (if applicable)
- Database: [TYPE, VERSION, STATUS]
- Cache: [TYPE, STATUS]
- External Services: [List with status]
- Migrations: [CURRENT/LATEST]
 
## Current Sprint/Iteration
 
### Objectives
1. [PRIMARY_GOAL] - [X% complete]
2. [SECONDARY_GOAL] - [X% complete]
 
### Completed This Session
- [Achievement 1 with impact]
- [Achievement 2 with impact]
 
### Work In Progress
| Task | Status | Branch | Blocker | Next Step |
|------|--------|--------|---------|-----------|
| [Task] | [%] | [branch] | [if any] | [action] |
 
## Immediate Action Items
 
### Priority 1: [TASK_NAME] (Est: [TIME])
 
**Why Critical:** [Reason]
 
**Setup:**

[SETUP_COMMANDS]

 
**Execution:**

[WORK_COMMANDS]

 
**Success Criteria:** [Measurable outcome]
 
### Priority 2: [TASK_NAME] (Est: [TIME])
[Same structure]
 
### Priority 3: [TASK_NAME] (Est: [TIME])
[Same structure]
 
## Knowledge Base References
- Architecture Diagrams: `docs/architecture/`
- API Documentation: `docs/api/`
- Decision Records: `docs/decisions/`
- Runbooks: `docs/runbooks/`
 
### Key Files to Understand
1. `[PATH/FILE]` - [Why important]
2. `[PATH/FILE]` - [Why important]
3. `[PATH/FILE]` - [Why important]
 
## State Preservation
- Session State: `state/session_[TIMESTAMP].json`
- Knowledge Base: `docs/context/KNOWLEDGE_BASE.md`
 
## AI Agent History
| Role | Session ID | Model | Outcome |
|------|------------|-------|---------|
| Planner | [id] | [model] | [summary] |
| Builder | [id] | [model] | [summary] |
| Reviewer | [id] | [model] | [verdict] |
 
## Verification Checklist (For Next Session)
Before starting work, verify:
- [ ] Can successfully build the project
- [ ] Tests pass locally
- [ ] Can connect to required services
- [ ] Understand immediate priorities
- [ ] Have access to all referenced resources
- [ ] Environment variables configured
 
## Handoff Certification
I certify this handoff is complete and accurate as of [TIMESTAMP].
 
- Generated by: [MODEL/PERSON]
- Session ID: [UNIQUE_ID]
- Confidence in Completeness: [HIGH/MEDIUM/LOW]
 
## Recovery Information
If something goes wrong:
1. Rollback: `git reset --hard [LAST_GOOD_SHA]`
2. Last Known Good: `[COMMIT_SHA]`
3. Backup Location: [WHERE]
 
---
**Next Session Should Start With:** [Specific first command or check]


Task 4: Version Control


# Stage all changes
git add .
 
# Commit with comprehensive message
git commit -m "handoff: [SUMMARY]
 
Session: [SESSION_ID]
Model: [MODEL_NAME]
Duration: [HOURS]h
 
Changes:
- [Change 1]
- [Change 2]
 
Next steps:
- [Step 1]
- [Step 2]
 
Evidence: evidence/*.txt
State: state/session_[TIMESTAMP].json"
 
# Push to feature branch
git push origin [BRANCH_NAME]
 
# Tag for reference
git tag handoff-[TIMESTAMP]




6.3 Workflow 3: Weekly Consolidation

Trigger: Every Friday, or after completing a significant milestone.


## Weekly Consolidation
 
**Filename:** `WEEKLY_SUMMARY_[YYYY]-W[WW].md`
 
### Tasks:
 
1. **Aggregate Progress Reports**
   - Combine all `PROGRESS_*.md` from this week
   - Extract key themes and accomplishments
 
2. **Pattern Identification**
   - What problems recurred?
   - What solutions worked well?
   - What technical debt accumulated?
 
3. **Knowledge Base Update**
   - Add new learnings to `docs/context/KNOWLEDGE_BASE.md`
   - Update `docs/decisions/` with any informal decisions that should be ADRs
 
4. **Documentation Refactoring**
   - Consolidate scattered notes into coherent docs
   - Update README if architecture changed
   - Refresh getting-started guide if needed
 
5. **Archive Management**
   - Archive session states older than 2 weeks to `archive/`
   - Keep last 2 weeks readily accessible
 
6. **Timeline Update**
   - Update project timeline estimates
   - Flag any scope changes
 
7. **System Snapshot**
   ```bash
   # Create weekly snapshot
   ./scripts/weekly-snapshot.sh
   ```
 
### Output Structure:

Week [WW] Summary - SparkData Analytics

Accomplishments
	â€¢	[Major accomplishment 1]
	â€¢	[Major accomplishment 2]

Metrics

Metric
Start of Week
End of Week
Delta
Test Coverage
X%
Y%
+Z%
Open Issues
N
M
-K
Tech Debt Items
A
B
+C


Patterns Observed
	â€¢	[Pattern 1]: [Implication]
	â€¢	[Pattern 2]: [Implication]

Knowledge Captured
	â€¢	Added to KB: [Topics]
	â€¢	New ADRs: [List]

Next Week Priorities
1. [Priority 1]
2. [Priority 2]
3. [Priority 3]

Risks & Blockers
	â€¢	[Risk 1]: [Mitigation]



6.4 Workflow 4: Agentic Coding Loop {#workflow-4}

Trigger: Complex implementation requiring multi-step autonomous work.

This is the "agentic" pattern: an orchestrated loop with role separation.


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   AGENTIC CODING LOOP                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   PLANNER    â”‚ â—„â”€â”€â”€ Requirements, Tickets, Specs                 â”‚
â”‚  â”‚   AGENT      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚         â”‚ Outputs: Plan, Test Plan, Acceptance Criteria             â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   BUILDER    â”‚ â—„â”€â”€â”€ Plan + Codebase + Test Framework             â”‚
â”‚  â”‚   AGENT      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚         â”‚ Outputs: Code, Tests, Docs                                â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚ ORCHESTRATOR â”‚ â—„â”€â”€â”€ Runs: pytest, ruff, mypy, etc.               â”‚
â”‚  â”‚   AGENT      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚         â”‚ Outputs: Tool reports (JSON/MD)                           â”‚
â”‚         â–¼                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚  REVIEWER    â”‚ â—„â”€â”€â”€ Spec + Diff + Tool Output (NO explanations)  â”‚
â”‚  â”‚   AGENT      â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚         â”‚                                                           â”‚
â”‚         â”œâ”€â”€â”€ APPROVE â”€â”€â”€â–º Proceed to Merge                          â”‚
â”‚         â”‚                                                           â”‚
â”‚         â””â”€â”€â”€ REJECT â”€â”€â”€â”€â–º Back to Builder with specific changes     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Phase 1: Planning (Planner Agent)

Input: Ticket, business context, data schema, existing architecture

Prompt:

You are the PLANNER AGENT for SparkData Analytics.
 
**Input:**
- Requirement: [PASTE TICKET/SPEC]
- Existing Architecture: [REFERENCE TO DOCS]
- Data Schemas: [REFERENCE TO SCHEMAS]
 
**Your Task:**
Create a comprehensive implementation plan.
 
**Output Required:**
 
### 1. Task Breakdown
| # | Task | Files Affected | Complexity | Dependencies |
|---|------|----------------|------------|--------------|
| 1 | [Task] | [files] | Low/Med/High | [task #s] |
 
### 2. Interface Definitions

New functions/classes to create
def new_function(param: Type) -> ReturnType:
    """Docstring with behavior specification"""
    pass

 
### 3. Test Plan
| Test Type | What to Test | Expected Behavior |
|-----------|--------------|-------------------|
| Unit | [function] | [behavior] |
| Integration | [flow] | [behavior] |
| Data Quality | [check] | [constraint] |
 
### 4. Acceptance Criteria
- [ ] [Criterion 1 - measurable]
- [ ] [Criterion 2 - measurable]
- [ ] [Criterion 3 - measurable]
 
### 5. Risk Assessment
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| [Risk] | Low/Med/High | Low/Med/High | [Action] |
 
### 6. Estimated Effort
- Planning: [X hours] âœ“ Complete
- Implementation: [Y hours]
- Testing: [Z hours]
- Review: [W hours]
- Total: [Sum hours]


Constraint: Do NOT write implementation code. Planning only.

Phase 2: Implementation (Builder Agent)

Input: Plan from Phase 1, codebase access, test framework

Prompt:

You are the BUILDER AGENT for SparkData Analytics.
 
**Mode:** BUILDER
**Session ID:** [GENERATE_UNIQUE_ID]
 
**You have received:**
- Implementation Plan: [PASTE PLAN]
- Test Plan: [PASTE TEST PLAN]
- Acceptance Criteria: [PASTE CRITERIA]
 
**Your Task:**
Implement the plan. Write code and tests.
 
**Rules:**
1. Follow the plan exactly. Deviations require justification.
2. Write tests FIRST (TDD). Tests must fail before implementation.
3. Implementation must pass all tests.
4. Do NOT approve your own work. Leave in "pending review" state.
5. Create verifiable claims with evidence commands.
 
**Output Required:**
 
### 1. Tests Written (First)

test_[feature].py
These tests should FAIL until implementation is complete

 
### 2. Test Failure Confirmation

pytest test_[feature].py -v
Expected: X failures

 
### 3. Implementation

[feature].py
Code that makes tests pass

 
### 4. Test Success Confirmation

pytest test_[feature].py -v
Expected: All pass

 
### 5. Verification Commands

Run these to verify claims:
make test        # All tests pass
make lint        # No lint errors
make typecheck   # No type errors
make build       # Build succeeds

 
### 6. Claims for Reviewer
| Claim | Verification Command | Expected Result |
|-------|---------------------|-----------------|
| Tests pass | `pytest -v` | 0 failures |
| Lint clean | `ruff check .` | 0 errors |
| Types clean | `mypy .` | 0 errors |
 
**Constraint:** You MAY NOT mark this work as complete. It must go to Reviewer.


Phase 3: Tool Execution (Orchestrator Agent)

Input: Code from Builder, tool configurations

Prompt:

You are the ORCHESTRATOR AGENT for SparkData Analytics.
 
**Your Task:**
Execute all verification tools and capture output.
 
**Tools to Run:**

Create evidence directory
mkdir -p evidence

Run tests with full output
pytest -v --tb=long 2>&1 | tee evidence/test_$(date +%Y%m%d_%H%M%S).txt
echo "Exit Code: $?" >> evidence/test_$(date +%Y%m%d_%H%M%S).txt

Run linter
ruff check . 2>&1 | tee evidence/lint_$(date +%Y%m%d_%H%M%S).txt
echo "Exit Code: $?" >> evidence/lint_$(date +%Y%m%d_%H%M%S).txt

Run type checker
mypy . 2>&1 | tee evidence/types_$(date +%Y%m%d_%H%M%S).txt
echo "Exit Code: $?" >> evidence/types_$(date +%Y%m%d_%H%M%S).txt

Run coverage
pytest --cov=. --cov-report=term-missing 2>&1 | tee evidence/coverage_$(date +%Y%m%d_%H%M%S).txt

Generate diff
git diff > evidence/diff_$(date +%Y%m%d_%H%M%S).txt

 
**Output Required:**
 
### Tool Results Summary
| Tool | Exit Code | Issues Found | Evidence File |
|------|-----------|--------------|---------------|
| pytest | [0/1] | [N failures] | `evidence/test_*.txt` |
| ruff | [0/1] | [N errors] | `evidence/lint_*.txt` |
| mypy | [0/1] | [N errors] | `evidence/types_*.txt` |
| coverage | N/A | [X%] | `evidence/coverage_*.txt` |
 
### Pass/Fail Gate
- [ ] All tests pass (exit code 0)
- [ ] Lint clean (exit code 0)
- [ ] Types clean (exit code 0)
- [ ] Coverage >= 80%
 
**Gate Status:** PASS / FAIL
 
If FAIL, do NOT proceed to Reviewer. Return to Builder with specific failures.


Phase 4: Review (Reviewer Agent)

Input: Spec, diff, tool output. NO Builder explanations.

Prompt:

You are the REVIEWER AGENT for SparkData Analytics.
 
**Mode:** REVIEWER (Blind)
**Session ID:** [GENERATE_UNIQUE_ID]
 
âš ï¸ CRITICAL: You have NOT seen the Builder's explanation.
You are reviewing this code independently.
 
**You have received:**
1. SPEC: [PASTE ORIGINAL REQUIREMENT]
2. DIFF: [PASTE GIT DIFF OR CODE]
3. TOOL OUTPUT: [PASTE ORCHESTRATOR SUMMARY]
4. EVIDENCE FILES: [LIST PATHS]
 
**You have NOT received:**
- Builder's reasoning
- Progress reports
- Any "here's what I did" narrative
 
**Your Task:**
Perform adversarial review using the full protocol in Â§8.1.
 
**Output Required:**
Full REVIEW_[timestamp].md per Â§8.1 template.
 
**Verdict Options:**
- APPROVE: All checks pass, risk score â‰¤ 2
- REQUEST_CHANGES: Issues found, specific fixes required
- BLOCK: Critical issues, cannot proceed
 
If REQUEST_CHANGES or BLOCK:
- List specific required changes
- Return to Builder Agent
- Builder implements changes
- Return to Orchestrator
- Return to Reviewer (you) for re-review


Phase 5: Merge & Document

Trigger: Reviewer verdict = APPROVE


# Merge to main branch
git checkout main
git merge --no-ff feature/[BRANCH] -m "feat: [DESCRIPTION]
 
Reviewed-by: [REVIEWER_MODEL]
Review-ID: [REVIEW_SESSION_ID]
Risk-Score: [0-10]
 
Closes #[TICKET]"
 
# Tag release if appropriate
git tag v[VERSION]
 
# Push
git push origin main --tags
 
# Generate documentation
./scripts/generate-docs.sh




7. Universal Handoff Template {#handoff-template}

See Â§6.2 Task 3 for the complete handoff document template.

Quick Handoff Checklist:
â˜  Progress report generated
â˜  Session state serialized to JSON
â˜  Handoff document created
â˜  All evidence files attached
â˜  Git commit with comprehensive message
â˜  Feature branch pushed
â˜  Handoff tagged for reference





Part III: Quality Control (Critical)



âš ï¸ READ THIS FIRST

This section addresses the core problem: AI models summarizing rather than critically auditing.

Why Models Parrot Instead of Audit:
1. LLMs are trained to be helpful and agreeable
2. When Model B reads Model A's explanation, it anchors on that reasoning
3. "Read and summarize" prompts trigger summarization behavior
4. Same context window = same conclusions

The Fix:
1. Role Separation: Builder â‰  Reviewer
2. Blind Reviews: Reviewer never sees Builder's explanations
3. Adversarial Prompts: Explicitly task the AI with finding problems
4. Tool Verification: Claims must be proven with actual command output
5. Proof of Work: No claim accepted without evidence



8. The Adversarial Code Review Protocol {#adversarial-review}

8.1 ACR Prompt (Primary QC Tool) {#acr-prompt}

Trigger: Before ANY code is committed or marked as complete.

Setup:
1. Clear the current context window OR open a new chat session
2. Do NOT feed previous session context or explanations
3. Only provide: spec + code + test output


## Adversarial Code Review - SparkData Analytics
 
You are **CriticalArchitect-Alpha**, a senior code auditor with a reputation for being harsh, pedantic, and strictly standards-obsessed.
 
### Your Mandate
**DESTROY THIS CODE.** Find security vulnerabilities, inefficiencies, anti-patterns, and logic gaps.
 
### Constraints (Non-Negotiable)
1. Do NOT summarize what the code does. The author knows what it does.
2. Do NOT be polite. Be direct and critical.
3. Do NOT fix the code yet. Only list defects.
4. If the code is perfect (it isn't), output "NO DEFECTS FOUND"â€”but if you miss a bug, you fail.
5. Do NOT reference any previous explanations. Judge the code independently.
 
### Review Dimensions
You MUST review for each of these:
 
| Dimension | What to Check |
|-----------|---------------|
| **Correctness** | Does logic match spec? Edge cases handled? |
| **Idempotency** | Safe to run twice? State management correct? |
| **Memory/Resources** | Leaks? Unbounded growth? Cleanup? |
| **Performance** | N+1 queries? Unnecessary loops? Blocking calls? |
| **Security** | Injection? Auth gaps? Secrets exposed? |
| **Error Handling** | All failure modes covered? Meaningful errors? |
| **Data Quality** | Nulls handled? Types validated? Contracts enforced? |
| **Observability** | Logging adequate? Metrics captured? Tracing? |
| **Maintainability** | Clear naming? DRY? Documented? |
 
### Input Package
**SPEC:** [Paste requirement/ticket]
**CODE:** [Paste code or file paths]
**TEST OUTPUT:** [Paste test results]
**LINT OUTPUT:** [Paste lint results]
 
### Required Output Format
 
#### 1. Defect Table
| # | File:Line | Severity | Category | Issue | Evidence |
|---|-----------|----------|----------|-------|----------|
| 1 | `file.py:42` | CRITICAL | Security | SQL injection via string concat | `query = f"SELECT * FROM {user_input}"` |
| 2 | `file.py:78` | HIGH | Performance | N+1 query in loop | `for item in items: db.get(item.id)` |
| 3 | `file.py:103` | MEDIUM | Error Handling | Bare except clause | `except: pass` |
 
Severity Levels:
- **CRITICAL:** Security vulnerability or data corruption risk. Blocks deployment.
- **HIGH:** Significant bug or performance issue. Requires fix before merge.
- **MEDIUM:** Code smell or minor bug. Should fix, not blocking.
- **LOW:** Style issue or improvement opportunity. Nice to have.
 
#### 2. Risk Score
**Overall Risk Score: [0-10]**
 
| Score | Meaning |
|-------|---------|
| 0-2 | Low risk, minor issues only |
| 3-5 | Moderate risk, requires attention |
| 6-8 | High risk, significant concerns |
| 9-10 | Critical risk, do not deploy |
 
**Justification:** [2-5 sentences explaining the score with specific evidence]
 
#### 3. Verdict
 
Choose exactly ONE:
- **APPROVE:** Risk â‰¤ 2, no blocking issues. Ready for merge.
- **REQUEST_CHANGES:** Risk 3-5, specific changes required. List them.
- **BLOCK:** Risk â‰¥ 6, critical issues. Cannot proceed until resolved.
 
If not APPROVE, list minimum required changes:
1. [PRIORITY: CRITICAL] - [Specific change] - [Why it matters]
2. [PRIORITY: HIGH] - [Specific change] - [Why it matters]
3. [PRIORITY: MEDIUM] - [Specific change] - [Why it matters]
 
#### 4. Missing Tests
Tests that SHOULD exist but DON'T:
1. [Test description] - [What it would catch]
2. [Test description] - [What it would catch]
3. [Test description] - [What it would catch]
 
#### 5. Assumptions Made
List assumptions this code makes that could fail:
| Assumption | What Breaks If Wrong | Validated? |
|------------|---------------------|------------|
| [Assumption] | [Failure mode] | Yes/No |
 
### Final Certification

Review completed: [TIMESTAMP]
Reviewer: CriticalArchitect-Alpha
Model: [MODEL_NAME]
Session: [SESSION_ID]
Verdict: [APPROVE/REQUEST_CHANGES/BLOCK]
Risk Score: [0-10]

Evidence files reviewed:
â˜  test_*.txt
â˜  lint_*.txt
â˜  types_*.txt
â˜  coverage_*.txt



8.2 The Non-Summarizing Reviewer Prompt

Use this when you need a focused code review without any fluff.


## Code Auditor - No Summary Mode
 
You are the Code Auditor Agent for SparkData Analytics.
 
**Your job:** Independently verify whether this code change satisfies the SPEC and TEST RESULTS.
 
**Critical Rule:** Treat all existing code as potentially buggy. Treat all outputs from other models as UNTRUSTED.
 
### You Have Access To:
- SPEC: [paste ticket/requirements]
- CODE: [paste relevant files or diff]
- TOOL_RESULTS: [test + lint + typecheck output]
 
### You Do NOT See:
- Any prior model explanations
- Any rationale from the implementer
- Any "here's what I did" narrative
 
### Your Tasks:
 
#### Task 1: Checklist Review
For each item, mark Pass/Fail/Unclear and justify briefly:
 
| Check | Status | Evidence |
|-------|--------|----------|
| Correctness vs SPEC | P/F/? | [proof] |
| Edge cases handled | P/F/? | [proof] |
| Error handling complete | P/F/? | [proof] |
| Test quality adequate | P/F/? | [proof] |
| Security risks addressed | P/F/? | [proof] |
| Performance acceptable | P/F/? | [proof] |
| Conventions followed | P/F/? | [proof] |
| Observability present | P/F/? | [proof] |
| Backwards compatible | P/F/? | [proof] |
 
#### Task 2: Risk Score
**Score: [0-10]**
 
Explanation (reference specific evidence in CODE and TOOL_RESULTS):
[2-5 sentences]
 
#### Task 3: Required Changes
If Risk > 2, list required changes:
1. [Priority: H/M/L] - [Change] - [Why]
 
If Risk â‰¤ 2, explain what you checked and why you're confident.
 
### Constraints:
- Do NOT summarize any prior text
- Do NOT rewrite code unless explicitly asked
- You MUST identify at least 3 concerns OR explicitly argue why fewer exist




8.3 The Rubric-Based Review

For formal code reviews with scoring across multiple dimensions.


## SparkData Code Review Rubric
 
### Scoring Instructions
Rate each dimension 0-10. Provide one sentence justification with line references.
 
### Dimension Scores
 
| Dimension | Score (0-10) | Justification |
|-----------|--------------|---------------|
| **Correctness** | [?] | [Does it work? Edge cases?] |
| **Robustness** | [?] | [Error handling? Failure modes?] |
| **Data Quality** | [?] | [Validation? Contracts? Nulls?] |
| **Performance** | [?] | [Efficiency? Scalability?] |
| **Security** | [?] | [Vulnerabilities? Auth? Secrets?] |
| **Maintainability** | [?] | [Readable? Documented? DRY?] |
| **Testability** | [?] | [Test coverage? Test quality?] |
| **Observability** | [?] | [Logging? Metrics? Tracing?] |
 
### Aggregate Score
**Total: [Sum] / 80**
**Percentage: [Pct]%**
 
### Grade
| Range | Grade | Action |
|-------|-------|--------|
| 90-100% | A | Approve immediately |
| 80-89% | B | Approve with minor suggestions |
| 70-79% | C | Request changes (non-blocking) |
| 60-69% | D | Request changes (blocking) |
| <60% | F | Block, significant rework needed |
 
### Final Grade: [A/B/C/D/F]
### Verdict: [APPROVE/REQUEST_CHANGES/BLOCK]




8.4 The Logic Stress Test

Simulate execution to find logic bugs.


## Logic Stress Test
 
**Purpose:** Verify code behavior by tracing execution manually.
 
### Setup
You will simulate being the Python Interpreter.
Trace execution with specific inputs.
Output variable state at each significant step.
 
### Inputs to Test
| Input | Type | Purpose |
|-------|------|---------|
| Input A | [Normal case] | Happy path |
| Input B | [Edge case] | Boundary condition |
| Input C | [Invalid input] | Error handling |
| Input D | [Empty/null] | Null safety |
| Input E | [Large scale] | Performance |
 
### Execution Trace Format

=== Input A: [value] ===
Line 10: variable_x = [value]
Line 15: Entering loop, iterations expected: [N]
Line 20: variable_y = [value]
Line 25: Condition [expr] evaluates to [True/False]
Line 30: RETURN [value]
=== Result: [SUCCESS/FAILURE/EXCEPTION] ===

 
### Questions to Answer
1. Does the pipeline crash for any input?
2. Does data transform correctly?
3. Are there infinite loops?
4. Are there off-by-one errors?
5. What happens at boundaries?
 
### Constraint
Do NOT write new code.
Only trace execution of existing code.




8.5 TDD Bug Fix Loop

Force test-first approach for bug fixes.


## TDD Bug Fix Protocol
 
**Trigger:** Bug report received, need to fix.
 
### Step 1: Reproduce
Write a test that FAILS with the current code.
 

def test_bug_reproduction_issue_XXX():
    """
    Bug: [Description from ticket]
    Expected: [What should happen]
    Actual: [What happens instead]
    """
    # Arrange
    input_data = [the problematic input]

    # Act
    result = function_under_test(input_data)

    # Assert - This should FAIL currently
    assert result == expected_value, f"Bug XXX: {result}"

 
### Step 2: Confirm Failure

pytest test_file.py::test_bug_reproduction_issue_XXX -v
Expected output: 1 FAILED

 
### Step 3: Implement Fix
Write the minimal code change to make the test pass.
 
### Step 4: Confirm Fix

pytest test_file.py::test_bug_reproduction_issue_XXX -v
Expected output: 1 PASSED

 
### Step 5: Regression Check

pytest -v
Expected output: All tests pass (including new one)

 
### Step 6: Evidence

Capture evidence
pytest -v 2>&1 | tee evidence/bugfix_XXX_$(date +%Y%m%d_%H%M%S).txt

 
### Constraint
Do NOT modify the test after writing it.
The test is the specification. Code must satisfy it.




9. Red Team Quality Control {#red-team-qc}

9.1 The "Hostile Reviewer" Frame

Adopt the mindset of someone trying to break the code.


## Red Team Review
 
You are a **hostile code reviewer** who WANTS to find problems.
Your job security depends on finding real issues.
 
You are reviewing work from a different session/model.
You should NOT assume their work is correct.
 
### Attack Vectors to Explore
 
#### 1. Assumption Attack
List every assumption the code makes.
For each, explain how it could be wrong in production.
 
| Assumption | How It Could Fail | Impact |
|------------|------------------|--------|
| [Assumption] | [Failure scenario] | [Consequence] |
 
#### 2. Edge Case Audit
Identify 5 edge cases NOT covered by existing tests.
Write failing test stubs for each.
 

def test_edge_case_1():
    """Edge: [description]"""
    # This should fail or behave unexpectedly
    pass

def test_edge_case_2():
    """Edge: [description]"""
    pass

 
#### 3. Silent Failure Search
What could go wrong that would NOT produce an obvious error?
- Race conditions
- Data corruption without exceptions
- Security holes
- Performance cliffs
- Memory leaks
 
#### 4. Contradiction Hunt
What in this code contradicts:
- [ ] The README/documentation
- [ ] The stated requirements
- [ ] The architectural decisions
- [ ] Common sense / industry practice
 
#### 5. Replication Challenge
WITHOUT looking at implementation:
1. Describe what you THINK this code does based on function names only
2. Describe what you THINK based on test names only
3. Describe what you THINK based on comments only
4. NOW look at implementation - where do expectations differ from reality?
 
### Verdict
- **BLOCK:** Critical issues found (list them)
- **CONCERN:** Non-critical issues found (list them)
- **PASS:** No issues found (explain WHY you're confident)
 
IMPORTANT: "PASS" requires EVIDENCE, not just absence of found problems.




9.2 The "Junior Developer" Frame

Review as if code was submitted by someone who misses edge cases.


## Junior Developer Review Frame
 
Review this code as if submitted by a junior developer who often misses:
- Edge cases
- Error handling
- Performance implications
- Security basics
 
### Focus Questions
 
1. **Production Failure Points**
   Where will this fail under high load?
   Where will this fail with unexpected data?
   Where will this fail during network issues?
 
2. **Missing Error Handling**
   What exceptions could occur that aren't caught?
   What error messages would confuse users/operators?
   What failures would be silent?
 
3. **Happy Path Assumptions**
   What "happy path" assumptions are made?
   What happens when those assumptions are violated?
 
4. **Resource Management**
   Are connections properly closed?
   Are files properly closed?
   Are locks properly released?
   Is memory properly freed?
 
### Output
List findings as if providing mentorship:
1. [Issue] - [What junior likely missed] - [Why it matters] - [How to fix]




9.3 The Security Audit Frame

Specialized review for security vulnerabilities.


## Security Audit Review
 
You are a security auditor. Find vulnerabilities.
 
### OWASP Top 10 Checklist
 
| Vulnerability | Present? | Evidence | Severity |
|--------------|----------|----------|----------|
| Injection (SQL, Command, etc.) | Y/N | [location] | [H/M/L] |
| Broken Authentication | Y/N | [location] | [H/M/L] |
| Sensitive Data Exposure | Y/N | [location] | [H/M/L] |
| XML External Entities | Y/N | [location] | [H/M/L] |
| Broken Access Control | Y/N | [location] | [H/M/L] |
| Security Misconfiguration | Y/N | [location] | [H/M/L] |
| Cross-Site Scripting | Y/N | [location] | [H/M/L] |
| Insecure Deserialization | Y/N | [location] | [H/M/L] |
| Known Vulnerabilities | Y/N | [location] | [H/M/L] |
| Insufficient Logging | Y/N | [location] | [H/M/L] |
 
### Data Flow Analysis
Trace sensitive data through the code:
1. Where does sensitive data enter?
2. How is it processed?
3. Where is it stored?
4. How is it transmitted?
5. Is it ever logged?
 
### Secrets Check
- [ ] No hardcoded passwords
- [ ] No hardcoded API keys
- [ ] No hardcoded tokens
- [ ] Secrets use environment variables
- [ ] .env files in .gitignore
 
### Authentication/Authorization
- [ ] Auth required on sensitive endpoints
- [ ] Proper session management
- [ ] CSRF protection where needed
- [ ] Rate limiting in place




10. Blind Audit Protocols {#blind-audit}

10.1 Setting Up a Blind Review

The Problem: When Reviewer sees Builder's explanation, they anchor on that reasoning.

The Solution: Reviewer operates blind to explanations.


## Blind Audit Setup
 
### Step 1: Builder Creates Artifacts
Builder session completes and creates:
- Code changes (committed to branch)
- Test files
- Evidence files (test output, lint output, etc.)
 
Builder does NOT create:
- Explanation documents for reviewer
- "Here's what I did" summaries
 
### Step 2: Prepare Review Package
Create a review package containing ONLY:
 

review_package/
â”œâ”€â”€ SPEC.md           # Original requirement (from ticket)
â”œâ”€â”€ diff.txt          # git diff output
â”œâ”€â”€ test_output.txt   # pytest output
â”œâ”€â”€ lint_output.txt   # linter output
â”œâ”€â”€ types_output.txt  # type checker output
â”œâ”€â”€ coverage.txt      # coverage report

 
### Step 3: Open Fresh Session
- Close all existing chat windows
- Open new session
- Do NOT reference previous conversations
- Load ONLY the review package
 
### Step 4: Run Adversarial Review
Use Â§8.1 ACR Prompt with the review package.
 
### Step 5: Document Independently
Reviewer creates REVIEW_*.md with their own analysis.

â””â”€â”€ FILES.md          # List of files to review (no explanations)



10.2 The Blind Audit Prompt


## Blind Code Audit
 
I am providing you with:
1. A requirement specification
2. A code file or diff
3. Test and tool output
 
You have NO other context. You do NOT know:
- Who wrote this code
- What their intentions were
- What problems they were trying to solve
- What approach they took
 
### Your Task
Audit this code as if found in the wild.
 
### Constraints
- Do NOT assume the code works
- Do NOT be polite
- Do NOT provide fixes yet
- ONLY list defects
 
### Output Format
Markdown table of defects:
| File | Line | Severity | Issue | Suggested Fix |
 
### Context
**SPEC:**
[Paste requirement]
 
**CODE:**
[Paste code/diff]
 
**TOOL OUTPUT:**
[Paste test/lint/type results]




10.3 Cross-Model Verification

Use different models for implementation vs review.


## Cross-Model Verification Protocol
 
### Recommended Pairings
| Builder Model | Reviewer Model | Rationale |
|--------------|----------------|-----------|
| GPT-5 Codex | Claude Opus | Different training, different biases |
| Claude Sonnet | GPT-4o | Speed vs depth balance |
| Gemini | Claude | Alternative perspective |
 
### Setup
1. Builder session: Use Model A
2. Save artifacts (code, tests, evidence)
3. Reviewer session: Use Model B
4. Provide review package only (no explanations)
5. Run adversarial review
 
### Benefits
- Different models have different blind spots
- Disagreements surface genuine uncertainty
- Forces more robust justifications




11. Proof of Work Requirements {#proof-of-work}

Principle: No claim is accepted without evidence.

11.1 Evidence Requirements by Claim Type


Claim
Required Evidence
Invalid Evidence
"Tests pass"
Full pytest output with timestamps
"I ran the tests"
"Build succeeds"
Full build log + artifact hash
"It compiled"
"No lint errors"
Full linter output
"Lint is clean"
"No type errors"
Full mypy/pyright output
"Types check out"
"Coverage is X%"
Coverage report with breakdown
"Coverage is good"
"No regressions"
Before/after comparison
"Everything still works"
"Performance OK"
Benchmark results
"It's fast enough"




11.2 Evidence Capture Scripts

Test Evidence

#!/bin/bash
# capture-test-evidence.sh
 
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT="evidence/test_${TIMESTAMP}.txt"
 
mkdir -p evidence
 
{
  echo "=== TEST EVIDENCE ==="
  echo "Timestamp: $(date -Iseconds)"
  echo "Git SHA: $(git rev-parse HEAD)"
  echo "Branch: $(git branch --show-current)"
  echo ""
  echo "=== TEST OUTPUT ==="
  pytest -v --tb=long 2>&1
  echo ""
  echo "Exit Code: $?"
  echo ""
  echo "=== COVERAGE ==="
  pytest --cov=. --cov-report=term-missing 2>&1
} | tee "$OUTPUT"
 
echo "Evidence saved to: $OUTPUT"


Lint Evidence

#!/bin/bash
# capture-lint-evidence.sh
 
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT="evidence/lint_${TIMESTAMP}.txt"
 
mkdir -p evidence
 
{
  echo "=== LINT EVIDENCE ==="
  echo "Timestamp: $(date -Iseconds)"
  echo "Git SHA: $(git rev-parse HEAD)"
  echo ""
  echo "=== RUFF OUTPUT ==="
  ruff check . 2>&1
  echo "Exit Code: $?"
  echo ""
  echo "=== MYPY OUTPUT ==="
  mypy . 2>&1
  echo "Exit Code: $?"
} | tee "$OUTPUT"
 
echo "Evidence saved to: $OUTPUT"


Build Evidence

#!/bin/bash
# capture-build-evidence.sh
 
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
OUTPUT="evidence/build_${TIMESTAMP}.txt"
 
mkdir -p evidence
 
{
  echo "=== BUILD EVIDENCE ==="
  echo "Timestamp: $(date -Iseconds)"
  echo "Git SHA: $(git rev-parse HEAD)"
  echo ""
  echo "=== ENVIRONMENT ==="
  echo "Python: $(python --version)"
  echo "Node: $(node --version 2>/dev/null || echo 'N/A')"
  echo "NPM: $(npm --version 2>/dev/null || echo 'N/A')"
  echo ""
  echo "=== BUILD OUTPUT ==="
  make build 2>&1 || npm run build 2>&1 || python setup.py build 2>&1
  echo ""
  echo "Exit Code: $?"
} | tee "$OUTPUT"
 
echo "Evidence saved to: $OUTPUT"




11.3 Discrepancy Report Template

When verification fails, document the discrepancy.


# DISCREPANCY REPORT
 
**Session:** [SESSION_ID]
**Timestamp:** [ISO-8601]
**Reviewer Model:** [MODEL_NAME]
**Reviewing Work From:** [BUILDER_SESSION_ID]
 
## Claims vs Reality
 
| # | Claim (from handoff) | Verification Method | Actual Result | Match? |
|---|---------------------|---------------------|---------------|--------|
| 1 | "All tests pass" | `pytest -v` | 3 failures | âŒ NO |
| 2 | "On branch feature-x" | `git branch` | main | âŒ NO |
| 3 | "No lint errors" | `ruff check .` | 0 errors | âœ… YES |
| 4 | "Coverage > 80%" | `pytest --cov` | 72% | âŒ NO |
 
## Unverified Claims
Claims that cannot be verified with available tools:
 
1. [Claim] - Why unverifiable: [Reason]
2. [Claim] - Why unverifiable: [Reason]
 
## Impact Assessment
 
| Discrepancy | Impact | Blocking? |
|-------------|--------|-----------|
| 3 test failures | Work incomplete | YES |
| Wrong branch | Merge target wrong | YES |
| Coverage below target | Technical debt | NO |
 
## Recommended Actions
 
1. [BLOCKING] Return to Builder: Fix failing tests
2. [BLOCKING] Clarify: Intended branch for merge
3. [NON-BLOCKING] Note: Coverage debt to address
 
## Verdict
- [X] BLOCK - Cannot proceed with review until discrepancies resolved
- [ ] PROCEED WITH CAUTION - Non-blocking discrepancies noted
- [ ] PROCEED - All claims verified
 
## Evidence Files
- `evidence/test_20251130_143000.txt` - Test output
- `evidence/lint_20251130_143100.txt` - Lint output




11.4 Mandatory Verification Checklist

Before any handoff or progress report is complete, verify:


## Pre-Handoff Verification Checklist
 
### Code Quality Gates (All Must Pass)
- [ ] **Tests:** `make test` exits with code 0
  - Evidence: `evidence/test_[timestamp].txt`
  - Result: [PASS/FAIL]
  
- [ ] **Lint:** `make lint` exits with code 0
  - Evidence: `evidence/lint_[timestamp].txt`
  - Result: [PASS/FAIL]
 
- [ ] **Types:** `make typecheck` exits with code 0
  - Evidence: `evidence/types_[timestamp].txt`
  - Result: [PASS/FAIL]
 
- [ ] **Build:** `make build` exits with code 0
  - Evidence: `evidence/build_[timestamp].txt`
  - Result: [PASS/FAIL]
 
### Coverage Gate
- [ ] **Coverage:** >= 80% overall
  - Current: [X]%
  - Evidence: Coverage section in test output
 
### State Verification
- [ ] **Actual vs Claimed:**
  - `git diff --stat` matches claimed changes
  - Evidence: `evidence/diff_[timestamp].txt`
 
### Gate Status
- [ ] ALL GATES PASS â†’ Proceed with handoff
- [ ] ANY GATE FAILS â†’ Fix before handoff
 
### Signature

Verification completed: [TIMESTAMP]
Verified by: [MODEL_NAME]
Session: [SESSION_ID]
All gates: [PASS/FAIL]





Part IV: Development



12. Development & Implementation Prompts {#dev-prompts}

12.1 Feature Implementation


## Feature Implementation - SparkData Analytics
 
Implement [FEATURE_NAME] with the following specifications:
 
### Requirements Analysis
1. User Stories affected: [List]
2. Acceptance Criteria: [Specific, measurable criteria]
3. Non-functional requirements: [Performance, security, etc.]
 
### Design Approach
1. Pattern to use: [e.g., Strategy, Observer, Factory]
2. New components needed: [List with responsibilities]
3. Existing components to modify: [List with change scope]
 
### Implementation Plan
1. Create feature branch: `git checkout -b feature/[name]`
2. Implement in order: [Ordered task list]
3. Testing strategy: [Unit, integration, e2e approach]
 
### Code Structure
- Follow existing patterns in [EXAMPLE_FILE]
- Use naming convention: [PROJECT_STANDARD]
- Add comprehensive docstrings
- Include error handling for [EXPECTED_ERROR_CASES]
 
### Quality Gates (Must Pass Before Handoff)
- [ ] Unit test coverage > 80%
- [ ] Integration tests for all APIs
- [ ] Documentation updated
- [ ] No performance regression
- [ ] Security scan passed
- [ ] Lint clean
- [ ] Type check clean
 
### Output Required
1. Implementation code with docstrings
2. Comprehensive test suite
3. Documentation updates
4. Migration script if needed
5. Evidence files for all quality gates




12.2 API Endpoint Creation


## API Endpoint Design & Implementation
 
Design and implement API endpoint for [PURPOSE]:
 
### Specification
- Method: [GET/POST/PUT/DELETE]
- Path: [/api/v1/resource]
- Authentication: [Method and requirements]
- Rate Limiting: [Requests per window]
 
### Request Schema

{
  "field1": "string (required)",
  "field2": "integer (optional, default: 0)",
  "field3": {
    "nested": "object"
  }
}

 
### Response Schema

{
  "success": {
    "data": {},
    "meta": {"timestamp": "ISO-8601"}
  },
  "error": {
    "code": "ERROR_CODE",
    "message": "Human readable message",
    "details": {}
  }
}

 
### Error Cases
| Status | Code | When |
|--------|------|------|
| 400 | BAD_REQUEST | Invalid input |
| 401 | UNAUTHORIZED | Missing/invalid auth |
| 403 | FORBIDDEN | Insufficient permissions |
| 404 | NOT_FOUND | Resource doesn't exist |
| 429 | RATE_LIMITED | Too many requests |
| 500 | INTERNAL_ERROR | Unexpected server error |
 
### Implementation Requirements
1. Input validation with clear error messages
2. Idempotency key support if mutating
3. Audit logging of all requests
4. Response caching strategy
5. OpenAPI documentation
6. Version compatibility
 
### Output
1. Endpoint implementation
2. Request/response models
3. Validation middleware
4. Test suite (unit + integration)
5. OpenAPI spec update




12.3 Database Schema Migration


## Database Migration
 
Create database migration for [PURPOSE]:
 
### Current State
[Describe or provide current schema]
 
### Target State
[Describe desired schema]
 
### Migration Requirements
1. Must be reversible (up and down migrations)
2. Zero downtime deployment
3. Handle existing data correctly
4. Maintain referential integrity
5. Index strategy defined
 
### Migration Steps
| Step | Action | Reversible? | Data Impact |
|------|--------|-------------|-------------|
| 1 | [Action] | Yes/No | [Impact] |
 
### Rollback Plan

-- Explicit rollback steps
-- Step 1: ...
-- Step 2: ...

 
### Verification Queries

-- Verify migration success
SELECT COUNT(*) FROM table WHERE condition;

-- Verify data integrity
SELECT * FROM table WHERE integrity_check_fails;

 
### Output
1. Forward migration script
2. Rollback migration script  
3. Data verification queries
4. Performance impact analysis
5. Migration runbook




12.4 Refactoring with Safety


## Safe Refactoring Protocol
 
Refactor [COMPONENT] to [GOAL]:
 
### Pre-Refactor Baseline

Capture current state
pytest -v --tb=short > baseline_tests.txt
pytest --cov=. --cov-report=term > baseline_coverage.txt

 
### Refactoring Strategy
1. Identify seams (natural boundaries)
2. Add characterization tests if missing
3. Make incremental changes
4. Verify after each change
 
### Safety Constraints
- [ ] All existing tests must continue to pass
- [ ] Coverage must not decrease
- [ ] Public API must remain stable (or versioned)
- [ ] No new dependencies without justification
 
### Change Log
| Change | Why | Verified? |
|--------|-----|-----------|
| [Change 1] | [Rationale] | âœ…/âŒ |
 
### Post-Refactor Verification

Compare with baseline
pytest -v --tb=short > refactored_tests.txt
diff baseline_tests.txt refactored_tests.txt

pytest --cov=. --cov-report=term > refactored_coverage.txt
Coverage must be >= baseline

 
### Output
1. Refactored code
2. Updated tests
3. Migration notes (if API changed)
4. Before/after evidence files




13. Agentic Pipeline Construction {#agentic-pipelines}

13.1 The OODA Loop for Data Pipelines {#ooda-loop}

Use this for building complex SparkData data workflows.


## Agentic Pipeline Construction - OODA Loop
 
You are acting as an Autonomous Dev Agent for SparkData Analytics.
Stop asking permission for every line. Follow this loop:
 
### Phase 1: OBSERVE
Gather information about the task:
- What data sources are involved?
- What transformations are needed?
- What is the output format/destination?
- What are the quality requirements?
 
### Phase 2: ORIENT
Create the implementation plan:
 

Pipeline Plan: [NAME]

Data Flow

[Source A] â”€â”€â–º [Transform 1] â”€â”€â–º [Transform 2] â”€â”€â–º [Destination]
[Source B] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Steps

#
Step
Input
Output
Validation
1
Extract from A
Raw
DataFrame
Schema check
2
Extract from B
Raw
DataFrame
Schema check
3
Join A + B
2 DataFrames
1 DataFrame
Row count
4
Transform
DataFrame
DataFrame
Business rules
5
Load
DataFrame
Database
Integrity check


Data Contracts
Input Schema A:

{
    "field1": str,
    "field2": int,
    "field3": Optional[float]
}


Output Schema:

{
    "id": str,
    "computed_field": float,
    "timestamp": datetime
}


Quality Checks
â˜  No nulls in required fields
â˜  Referential integrity maintained
â˜  Row counts within expected range
â˜  No duplicates on key fields

 
**â¸ï¸ STOP: Wait for approval on the plan.**
 
### Phase 3: DECIDE & ACT
After plan approval, implement each step:
 
#### Step Implementation Pattern

Step 1: Extract from Source A
def extract_source_a() -> pd.DataFrame:
    """
    Extract data from Source A.

    Returns:
        DataFrame with schema: {field1: str, field2: int, field3: float}

    Raises:
        ExtractionError: If source is unavailable
    """
    # Implementation
    pass

Immediately write test
def test_extract_source_a():
    """Test extraction from Source A."""
    result = extract_source_a()

    # Schema validation
    assert set(result.columns) == {"field1", "field2", "field3"}
    assert result["field1"].dtype == object  # str
    assert result["field2"].dtype == int

    # Quality checks
    assert not result["field1"].isna().any()
    assert len(result) > 0

 
### Phase 4: LOOP (Self-Correct)
After each step:
1. Run the test
2. If it fails, fix the implementation
3. Only output code that passes tests
 

Run test for current step
pytest test_pipeline.py::test_extract_source_a -v

If fails, iterate. If passes, continue to next step.

 
### Constraints
- Do NOT output code that assumes external variables exist (define them)
- Do NOT skip the test phase
- Do NOT mark step complete until test passes
- DO capture evidence of passing tests
 
### Final Output
1. Complete pipeline code
2. Complete test suite
3. Data contract definitions
4. Evidence files showing all tests pass




13.2 Pipeline Implementation Checklist


## SparkData Pipeline Checklist
 
Before marking any pipeline as complete:
 
### Data Contracts âœ“
- [ ] Input schemas defined and enforced
- [ ] Output schemas defined and enforced
- [ ] Null handling specified for each field
- [ ] Valid value ranges documented
- [ ] Uniqueness constraints documented
 
### Data Quality Tests âœ“
- [ ] Row count assertions (min/max expected)
- [ ] Null checks on required fields
- [ ] Referential integrity checks
- [ ] Duplicate detection on key fields
- [ ] Business rule validations
- [ ] Distribution checks (outlier detection)
 
### Operational Guarantees âœ“
- [ ] **Idempotency:** Running twice produces same result
- [ ] **Backfill:** Can reprocess historical data
- [ ] **Resume:** Can continue after partial failure
- [ ] **Cleanup:** Failed runs don't leave corrupt state
 
### Observability âœ“
- [ ] Logging at each stage (with correlation IDs)
- [ ] Metrics captured:
  - Input row count
  - Output row count
  - Error count
  - Null counts per field
  - Processing duration
- [ ] Alerts configured for anomalies
 
### Error Handling âœ“
- [ ] All exceptions caught and logged
- [ ] Meaningful error messages
- [ ] Retry logic for transient failures
- [ ] Dead letter queue for poison records
- [ ] Graceful degradation where possible
 
### Documentation âœ“
- [ ] Pipeline purpose documented
- [ ] Data flow diagram created
- [ ] SLA/SLO defined
- [ ] Runbook for operations
- [ ] On-call procedures




13.3 Data Quality Assertions Template


"""
SparkData Analytics - Data Quality Assertions
Use these patterns for pipeline quality gates.
"""
 
from typing import Any, Dict, List
import pandas as pd
 
class DataQualityError(Exception):
    """Raised when data quality check fails."""
    pass
 
def assert_schema(df: pd.DataFrame, expected: Dict[str, type]) -> None:
    """Verify DataFrame matches expected schema."""
    actual_cols = set(df.columns)
    expected_cols = set(expected.keys())
    
    missing = expected_cols - actual_cols
    extra = actual_cols - expected_cols
    
    if missing:
        raise DataQualityError(f"Missing columns: {missing}")
    if extra:
        raise DataQualityError(f"Unexpected columns: {extra}")
    
    # Type checking would go here
 
def assert_no_nulls(df: pd.DataFrame, columns: List[str]) -> None:
    """Verify specified columns have no null values."""
    for col in columns:
        null_count = df[col].isna().sum()
        if null_count > 0:
            raise DataQualityError(
                f"Column '{col}' has {null_count} null values"
            )
 
def assert_unique(df: pd.DataFrame, columns: List[str]) -> None:
    """Verify no duplicate values on specified columns."""
    duplicates = df.duplicated(subset=columns, keep=False)
    if duplicates.any():
        dup_count = duplicates.sum()
        raise DataQualityError(
            f"Found {dup_count} duplicate rows on columns {columns}"
        )
 
def assert_row_count_range(
    df: pd.DataFrame, 
    min_rows: int, 
    max_rows: int
) -> None:
    """Verify row count is within expected range."""
    actual = len(df)
    if actual < min_rows:
        raise DataQualityError(
            f"Row count {actual} below minimum {min_rows}"
        )
    if actual > max_rows:
        raise DataQualityError(
            f"Row count {actual} exceeds maximum {max_rows}"
        )
 
def assert_referential_integrity(
    df: pd.DataFrame,
    column: str,
    reference_df: pd.DataFrame,
    reference_column: str
) -> None:
    """Verify all values in column exist in reference."""
    values = set(df[column].dropna())
    reference_values = set(reference_df[reference_column])
    
    missing = values - reference_values
    if missing:
        raise DataQualityError(
            f"Orphan values in '{column}': {list(missing)[:10]}..."
        )




14. Architecture & Design Prompts {#architecture}

14.1 Architecture Decision Record (ADR)


# ADR-[NUMBER]: [TITLE]
 
**Status:** [PROPOSED | ACCEPTED | DEPRECATED | SUPERSEDED]
**Date:** [YYYY-MM-DD]
**Decision Makers:** [Names/Roles]
 
## Context
 
### Problem Statement
What is the issue we're facing?
 
### Constraints
- [Constraint 1]
- [Constraint 2]
 
### Business Drivers
- [Driver 1]
- [Driver 2]
 
## Decision Drivers
1. [Driver 1: e.g., Must support 10x growth]
2. [Driver 2: e.g., Team familiarity with technology]
3. [Driver 3: e.g., Cost constraints]
4. [Driver 4: e.g., Time to market]
 
## Considered Options
 
### Option 1: [NAME]
**Description:** [What this option entails]
 
| Aspect | Assessment |
|--------|------------|
| Pros | [List] |
| Cons | [List] |
| Cost | [Time/money/complexity] |
| Risk | [What could go wrong] |
| Effort | [Team-weeks] |
 
### Option 2: [NAME]
[Same structure]
 
### Option 3: [NAME]
[Same structure]
 
## Decision Outcome
 
**Chosen Option:** [OPTION NAME]
 
**Rationale:** [Detailed reasoning referencing decision drivers]
 
### Consequences
 
| Type | Description |
|------|-------------|
| Positive | [What improves] |
| Negative | [Trade-offs accepted] |
| Technical Debt | [What we'll need to address later] |
 
## Implementation Plan
1. [Step 1 with timeline]
2. [Step 2 with timeline]
3. [Step 3 with timeline]
 
## Validation
How will we know this decision was correct?
 
| Metric | Target | Measure Date |
|--------|--------|--------------|
| [Metric 1] | [Target] | [Date] |
| [Metric 2] | [Target] | [Date] |
 
## Review Date
**[DATE]** - When we'll revisit this decision
 
## Reversal Cost
If we need to change course:
- Effort required: [LOW | MEDIUM | HIGH]
- Data migration needed: [YES | NO]
- Breaking changes: [YES | NO]
- Reversal strategy: [Brief description]
 
## References
- [Link to relevant docs]
- [Link to prior ADRs]




14.2 System Design Document


# System Design: [COMPONENT/FEATURE]
 
## Executive Summary
[2-3 sentences: what and why]
 
## Goals and Non-Goals
 
### Goals
- [What this system will accomplish]
- [What this system will accomplish]
 
### Non-Goals
- [What this system explicitly won't do]
- [What this system explicitly won't do]
 
## Design
 
### High-Level Architecture

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Component  â”‚â”€â”€â”€â”€â–ºâ”‚  Component  â”‚â”€â”€â”€â”€â–ºâ”‚  Component  â”‚
â”‚      A      â”‚     â”‚      B      â”‚     â”‚      C      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â–¼                   â–¼                   â–¼

 
### Detailed Design
 
#### Component A
- **Responsibility:** [Single, clear purpose]
- **Interface:** [How others interact with it]
- **Implementation:** [Key algorithms/patterns]
- **Dependencies:** [What it needs]
- **Scaling:** [How it grows]
 
#### Component B
[Same structure]
 
### Data Model

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Store                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-- Key entities and relationships
CREATE TABLE entity (
    id UUID PRIMARY KEY,
    field1 VARCHAR(255) NOT NULL,
    field2 INTEGER,
    created_at TIMESTAMP DEFAULT NOW()
);

 
- **Storage:** [Where and how]
- **Consistency:** [Strong | Eventual]
- **Retention:** [How long we keep data]
 
### API Design
| Endpoint | Method | Purpose |
|----------|--------|---------|
| /api/v1/resource | GET | List resources |
| /api/v1/resource | POST | Create resource |
 
### Security Considerations
- **Attack Vectors:** [What we protect against]
- **Mitigation:** [How we protect]
- **Encryption:** [At rest and in transit]
- **Audit:** [What we log]
 
## Alternatives Considered
[Other approaches and why rejected]
 
## Performance Implications
- **Expected Load:** [Requests/second]
- **Latency Targets:** [p50, p95, p99]
- **Resource Requirements:** [CPU, memory, storage]
- **Bottlenecks:** [Potential limiting factors]
 
## Testing Strategy
- **Unit Tests:** [Approach and coverage]
- **Integration Tests:** [What we verify]
- **Load Tests:** [How we validate scale]
- **Chaos Tests:** [Failure scenarios]
 
## Rollout Plan
| Phase | What | When | Success Criteria |
|-------|------|------|------------------|
| 1 | [Scope] | [Date] | [Metrics] |
| 2 | [Scope] | [Date] | [Metrics] |
 
## Open Questions
1. [Question] â†’ [Who can answer]
2. [Question] â†’ [Who can answer]




14.3 Technical Spec Template


# Technical Specification: [FEATURE]
 
## Overview
**Author:** [Name]
**Reviewers:** [Names]
**Status:** [Draft | Review | Approved]
 
## Background
[Why are we building this?]
 
## Requirements
 
### Functional Requirements
| ID | Requirement | Priority |
|----|-------------|----------|
| FR1 | [Requirement] | Must |
| FR2 | [Requirement] | Should |
| FR3 | [Requirement] | Could |
 
### Non-Functional Requirements
| ID | Requirement | Target |
|----|-------------|--------|
| NFR1 | Latency | < 100ms p99 |
| NFR2 | Availability | 99.9% |
| NFR3 | Throughput | 1000 RPS |
 
## Technical Design
 
### Architecture
[Diagrams and descriptions]
 
### Data Flow
1. [Step 1]
2. [Step 2]
3. [Step 3]
 
### API Contracts
[Request/response schemas]
 
### Database Changes
[Schema changes, migrations]
 
## Implementation Plan
 
### Milestones
| Milestone | Tasks | Estimate |
|-----------|-------|----------|
| M1 | [Tasks] | [Days] |
| M2 | [Tasks] | [Days] |
 
### Dependencies
- [Dependency 1]: [Status]
- [Dependency 2]: [Status]
 
### Risks
| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| [Risk] | H/M/L | H/M/L | [Action] |
 
## Testing Plan
[How we'll verify correctness]
 
## Monitoring & Alerts
[What we'll track]
 
## Rollback Plan
[How to revert if needed]






Part V: Testing, QA, and Operations



15. Testing & Quality Assurance {#testing-qa}

15.1 Test Strategy Design


## Comprehensive Test Strategy for [COMPONENT/FEATURE]
 
### Test Pyramid
 

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   E2E Tests  â”‚  10%
                  â”‚  (Critical    â”‚
                 â”‚    Paths)      â”‚
                â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
               â”‚ Integration Tests â”‚  20%
              â”‚  (Component         â”‚
             â”‚   Interactions)       â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤

 
### Unit Tests (70% of tests)
For each function/method, test:
- [ ] Happy path (normal inputs)
- [ ] Edge cases (null, empty, maximum)
- [ ] Error conditions (invalid inputs)
- [ ] Boundary values (limits, thresholds)
 
### Integration Tests (20% of tests)
- [ ] Component interactions
- [ ] Database operations
- [ ] External service calls
- [ ] Message queue operations
- [ ] API contracts
 
### E2E Tests (10% of tests)
- [ ] Critical user journeys
- [ ] Cross-system flows
- [ ] Performance under realistic load
 
### Coverage Requirements
| Type | Minimum | Target |
|------|---------|--------|
| Line Coverage | 80% | 90% |
| Branch Coverage | 75% | 85% |
| Critical Paths | 100% | 100% |
| Error Handlers | 100% | 100% |
 
### Test Data Strategy
- **Fixtures:** [How we generate test data]
- **Factories:** [Patterns for creating test objects]
- **Cleanup:** [How we reset state between tests]
- **Seeds:** [Reproducible data sets]
 
### Mock Strategy
| Dependency | Mock Approach |
|------------|---------------|
| External APIs | Mock with recorded responses |
| Database | In-memory or test container |
| Time/Date | Freeze with library |
| Random values | Seeded generators |

           â”‚       Unit Tests         â”‚  70%
          â”‚   (Functions/Classes)      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜



15.2 Test Plan Template (LLM-Generated)


## Test Plan: [FEATURE/BUGFIX]
 
**Purpose:** Design tests BEFORE implementation (TDD).
 
### 1. Functional Test Cases
| # | Scenario | Given | When | Then |
|---|----------|-------|------|------|
| 1 | Normal case | [Precondition] | [Action] | [Expected] |
| 2 | Alternative path | [Precondition] | [Action] | [Expected] |
| 3 | Another case | [Precondition] | [Action] | [Expected] |
 
### 2. Edge Cases
| # | Edge Condition | Input | Expected Behavior |
|---|----------------|-------|-------------------|
| 1 | Empty input | `[]` or `""` | [Behavior] |
| 2 | Null/None | `None` | [Behavior] |
| 3 | Max size | [Large input] | [Behavior] |
| 4 | Invalid type | [Wrong type] | [Error expected] |
| 5 | Boundary value | [At limit] | [Behavior] |
 
### 3. Failure Modes
| # | Error Condition | Trigger | Expected Response |
|---|-----------------|---------|-------------------|
| 1 | Network failure | Timeout | [Retry? Error?] |
| 2 | Invalid auth | Bad token | [HTTP 401] |
| 3 | Resource not found | Bad ID | [HTTP 404] |
 
### 4. Non-Functional Tests
| Type | Test | Target |
|------|------|--------|
| Performance | [What to measure] | [< X ms] |
| Security | [What to validate] | [Criteria] |
| Load | [Scenario] | [RPS/concurrent] |
 
### 5. Coverage Plan
| Module | Unit Tests | Integration Tests |
|--------|------------|-------------------|
| [module.py] | Required | If external deps |
| [api.py] | Required | Required |
 
### 6. Test Implementation Order
1. Write failing test for core functionality
2. Implement minimal code to pass
3. Write edge case tests
4. Implement edge case handling
5. Write error handling tests
6. Implement error handling
7. Write integration tests
8. Verify full integration




15.3 Property-Based Testing


## Property-Based Testing for [MODULE]
 
### Properties to Test
 
#### Invariants (Always True)
| Property | Description | Example |
|----------|-------------|---------|
| Size bound | Output â‰¤ input | `len(compress(x)) <= len(x)` |
| Idempotency | `f(f(x)) == f(x)` | `normalize(normalize(x)) == normalize(x)` |
| Round-trip | `decode(encode(x)) == x` | Serialization |
 
#### Relationships
| Property | Description |
|----------|-------------|
| Inverse | `reverse(reverse(x)) == x` |
| Associative | `(a + b) + c == a + (b + c)` |
| Commutative | `merge(a, b) == merge(b, a)` |
 
### Implementation

from hypothesis import given, strategies as st

@given(st.lists(st.integers()))
def test_sort_preserves_length(xs):
    """Sorting doesn't change list length."""
    assert len(sort(xs)) == len(xs)

@given(st.lists(st.integers()))
def test_sort_is_idempotent(xs):
    """Sorting twice equals sorting once."""
    assert sort(sort(xs)) == sort(xs)

@given(st.text())
def test_roundtrip_encoding(s):
    """Encoding then decoding returns original."""
    assert decode(encode(s)) == s

 
### Shrinking Strategy
When test fails, minimize the failing case:
1. Hypothesis automatically shrinks
2. Log minimal failing example
3. Add as explicit regression test




15.4 Test Evidence Collection


#!/bin/bash
# collect-test-evidence.sh
# Run this to generate complete test evidence package
 
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
EVIDENCE_DIR="evidence/test_run_${TIMESTAMP}"
 
mkdir -p "$EVIDENCE_DIR"
 
echo "=== SparkData Test Evidence Collection ===" | tee "$EVIDENCE_DIR/summary.txt"
echo "Timestamp: $(date -Iseconds)" | tee -a "$EVIDENCE_DIR/summary.txt"
echo "Git SHA: $(git rev-parse HEAD)" | tee -a "$EVIDENCE_DIR/summary.txt"
echo "" | tee -a "$EVIDENCE_DIR/summary.txt"
 
# Unit tests with verbose output
echo "=== Unit Tests ===" | tee -a "$EVIDENCE_DIR/summary.txt"
pytest tests/unit -v --tb=short 2>&1 | tee "$EVIDENCE_DIR/unit_tests.txt"
echo "Exit Code: $?" | tee -a "$EVIDENCE_DIR/unit_tests.txt"
 
# Integration tests
echo "=== Integration Tests ===" | tee -a "$EVIDENCE_DIR/summary.txt"
pytest tests/integration -v --tb=short 2>&1 | tee "$EVIDENCE_DIR/integration_tests.txt"
echo "Exit Code: $?" | tee -a "$EVIDENCE_DIR/integration_tests.txt"
 
# Coverage report
echo "=== Coverage ===" | tee -a "$EVIDENCE_DIR/summary.txt"
pytest --cov=src --cov-report=term-missing --cov-report=html:"$EVIDENCE_DIR/htmlcov" 2>&1 | tee "$EVIDENCE_DIR/coverage.txt"
 
# Generate coverage badge
coverage-badge -o "$EVIDENCE_DIR/coverage.svg" 2>/dev/null || echo "Badge generation skipped"
 
# Summary
echo "" | tee -a "$EVIDENCE_DIR/summary.txt"
echo "=== Evidence Files ===" | tee -a "$EVIDENCE_DIR/summary.txt"
ls -la "$EVIDENCE_DIR" | tee -a "$EVIDENCE_DIR/summary.txt"
 
echo ""
echo "Evidence collected in: $EVIDENCE_DIR"




16. Production Operations & Incident Response {#prod-ops}

16.1 Production Incident Response


## INCIDENT RESPONSE PROTOCOL
 
**INCIDENT DETECTED:** [DESCRIPTION]
**Severity:** [P0 | P1 | P2 | P3]
**Detected at:** [TIMESTAMP]
 
### Severity Definitions
| Level | Impact | Response Time | Examples |
|-------|--------|---------------|----------|
| P0 | Complete outage | < 15 min | Service down, data loss |
| P1 | Major degradation | < 30 min | Feature broken, errors > 5% |
| P2 | Minor degradation | < 4 hours | Slow performance, edge case bug |
| P3 | Minimal impact | < 24 hours | Cosmetic issue, minor bug |
 
---
 
### Phase 1: Immediate Actions (First 5 minutes)
 
#### 1.1 Verify & Assess
- [ ] Confirm incident is real (not false positive)
- [ ] Check monitoring dashboards
- [ ] Assess current user impact
- [ ] Determine if this is a recurrence
 
#### 1.2 Communicate
- [ ] Create incident channel: `#incident-[TIMESTAMP]`
- [ ] Notify on-call engineer
- [ ] Update status page (if customer-facing)
- [ ] Inform stakeholders per escalation matrix
 
#### 1.3 Quick Mitigations
Consider these immediate actions:
- [ ] Scale up capacity?
- [ ] Enable circuit breaker?
- [ ] Rollback recent deployment?
- [ ] Redirect traffic?
- [ ] Enable maintenance mode?
 
---
 
### Phase 2: Investigation (5-30 minutes)
 
#### 2.1 Recent Changes
Check last 24-48 hours:
| Change Type | Time | Details |
|-------------|------|---------|
| Deployments | [time] | [what] |
| Config changes | [time] | [what] |
| Traffic patterns | [time] | [anomaly?] |
| Dependency updates | [time] | [what] |
 
#### 2.2 System Analysis

CPU/Memory
top -bn1 | head -20

Disk usage
df -h

Network connections
netstat -tuln | grep LISTEN

Application logs (last 100 lines)
tail -100 /var/log/app/error.log

Database connections
psql -c "SELECT count(*) FROM pg_stat_activity;"

 
#### 2.3 Root Cause Identification
| Symptoms | Hypothesis | Evidence | Confidence |
|----------|------------|----------|------------|
| [What we see] | [What we think] | [Supporting data] | H/M/L |
 
---
 
### Phase 3: Resolution
 
#### 3.1 Fix Implementation

Document each step taken
Step 1: [Description]
[COMMAND]

Step 2: [Description]
[COMMAND]

 
#### 3.2 Validation Checklist
- [ ] Service responding normally
- [ ] Error rates back to baseline
- [ ] No data loss/corruption
- [ ] Dependencies healthy
- [ ] Monitoring shows green
 
---
 
### Phase 4: Post-Incident
 
#### 4.1 Timeline
| Time | Event |
|------|-------|
| [T+0] | Incident detected |
| [T+X] | [Action taken] |
| [T+Y] | [Resolution] |
| [T+Z] | Incident closed |
 
#### 4.2 Impact Assessment
- **Duration:** [Total time]
- **Users affected:** [Number/percentage]
- **Data impact:** [Any loss/corruption]
- **SLA impact:** [Percentage]
- **Revenue impact:** [If applicable]
 
#### 4.3 Five Whys Analysis
1. Why did [SYMPTOM] occur? â†’ [CAUSE 1]
2. Why did [CAUSE 1] occur? â†’ [CAUSE 2]
3. Why did [CAUSE 2] occur? â†’ [CAUSE 3]
4. Why did [CAUSE 3] occur? â†’ [CAUSE 4]
5. Why did [CAUSE 4] occur? â†’ [ROOT CAUSE]
 
#### 4.4 Action Items
| Action | Owner | Due Date | Priority |
|--------|-------|----------|----------|
| [Fix root cause] | [Who] | [When] | P0 |
| [Add monitoring] | [Who] | [When] | P1 |
| [Update runbook] | [Who] | [When] | P2 |




16.2 Deployment Checklist


## Deployment Checklist: [SERVICE/FEATURE] to [ENVIRONMENT]
 
### Pre-Deployment (T-1 hour)
 
#### Code Readiness
- [ ] All tests passing in CI
- [ ] Code review approved
- [ ] Security scan clean
- [ ] Documentation updated
- [ ] Version bumped appropriately
 
#### Environment Readiness
- [ ] Database migrations tested
- [ ] Config changes staged
- [ ] Feature flags configured
- [ ] Secrets rotated if needed
- [ ] Rollback plan documented
 
#### Team Readiness
- [ ] Team notified of deployment window
- [ ] On-call engineer aware
- [ ] Stakeholders informed
 
### Deployment Execution
 
#### Stage 1: Canary (5% traffic)

Deploy canary
kubectl set image deployment/app app=image:new --record

Monitor for 15 minutes
watch -n5 'curl -s http://metrics/dashboard'

 
**Success Criteria:**
- Error rate < 0.1%
- Latency p99 < baseline + 10%
- No new error types
 
#### Stage 2: Partial (25% traffic)

Scale canary
kubectl scale deployment/app-canary --replicas=3

 
**Hold for:** 30 minutes
**Monitor:** Same criteria as Stage 1
 
#### Stage 3: Full (100% traffic)

Complete rollout
kubectl rollout status deployment/app

 
### Post-Deployment (T+1 hour)
 
#### Functional Checks
- [ ] Key features working
- [ ] API endpoints responding
- [ ] Background jobs running
- [ ] Data flowing correctly
 
#### Performance Checks
- [ ] Response time within SLA
- [ ] Error rate within SLA
- [ ] Resource usage stable
- [ ] No memory leaks observed
 
### Rollback Triggers
Initiate rollback if:
- Error rate > 1%
- Response time > 2x baseline
- Any P0 bug discovered
- Data corruption detected
 
### Rollback Procedure

Immediate rollback
kubectl rollout undo deployment/app

Verify rollback
kubectl rollout status deployment/app



17. Performance Engineering {#performance}

17.1 Performance Analysis


## Performance Analysis: [SYSTEM/COMPONENT]
 
### Baseline Metrics
| Metric | Current | Target | Gap |
|--------|---------|--------|-----|
| Throughput | [X] RPS | [Y] RPS | [Z%] |
| Latency p50 | [X] ms | [Y] ms | [Z%] |
| Latency p95 | [X] ms | [Y] ms | [Z%] |
| Latency p99 | [X] ms | [Y] ms | [Z%] |
| Error rate | [X]% | [Y]% | [Z%] |
| CPU usage | [X]% | [Y]% | [Z%] |
| Memory usage | [X] GB | [Y] GB | [Z%] |
 
### Profiling Results
 
#### CPU Profiling
| Function | % Time | Calls | Avg Time |
|----------|--------|-------|----------|
| [func1] | [X]% | [N] | [Y ms] |
| [func2] | [X]% | [N] | [Y ms] |
 
#### Memory Profiling
| Object Type | Count | Size |
|-------------|-------|------|
| [type1] | [N] | [X MB] |
| [type2] | [N] | [X MB] |
 
#### I/O Analysis
| Operation | Count | Avg Latency |
|-----------|-------|-------------|
| DB queries | [N] | [X ms] |
| API calls | [N] | [X ms] |
| File ops | [N] | [X ms] |
 
### Bottleneck Identification
| Location | Impact | Cause | Fix Complexity |
|----------|--------|-------|----------------|
| [file:line] | [X%] | [Why slow] | Easy/Med/Hard |
 
### Optimization Plan
#### Quick Wins (< 2 hours)
1. [Optimization]: Expected [X%] improvement
2. [Optimization]: Expected [X%] improvement
 
#### Medium Efforts (2-8 hours)
1. [Optimization]: Expected [X%] improvement
 
#### Major Refactors (> 8 hours)
1. [Refactor]: Expected [X%] improvement
 
### Load Testing Plan

Run load test
k6 run --vus 100 --duration 5m loadtest.js

 
**Scenarios:**
| Scenario | Users | Duration | Target |
|----------|-------|----------|--------|
| Baseline | 10 | 5 min | Establish baseline |
| Normal | 100 | 10 min | < 200ms p99 |
| Peak | 500 | 10 min | < 500ms p99 |
| Stress | 1000 | 5 min | Graceful degradation |




18. Debugging & Problem Solving {#debugging}

18.1 Rubber Duck Debugging Session


## Systematic Debugging: [PROBLEM]
 
### Problem Statement
**Expected:** [What should happen]
**Actual:** [What actually happens]
**Started:** [When]
**Frequency:** [Always | Sometimes | Rarely]
 
### Context
- **Environment:** [Where it occurs]
- **Affected users:** [Who/how many]
- **Business impact:** [What's at stake]
 
### Reproduction Steps
1. [Step 1 - specific action]
2. [Step 2 - specific action]
3. [Step 3 - observe problem]
 
**Minimal reproduction:**

Smallest code that shows the bug

 
### Investigation So Far
 
| # | Attempted | Result | Learning |
|---|-----------|--------|----------|
| 1 | [What I did] | [What happened] | [What this tells us] |
| 2 | [What I did] | [What happened] | [What this tells us] |
 
### Current Hypothesis
**I think:** [HYPOTHESIS]
**Because:** [EVIDENCE]
**Uncertain about:** [GAPS]
 
### Binary Search Strategy
- **Known good state:** [When/where it works]
- **Known bad state:** [When/where it fails]
- **Midpoint to test:** [What to check next]
 
### Variable Isolation
| Variable | Current Value | To Test |
|----------|---------------|---------|
| [var1] | [value] | [alternatives] |
| [var2] | [value] | [alternatives] |
 
### Debug Points
Add logging at:
1. [Location 1]: To verify [what]
2. [Location 2]: To verify [what]
3. [Location 3]: To verify [what]
 
### Questions to Answer
1. What's the last point where things are correct?
2. What's the first point where things go wrong?
3. What changes between those points?




18.2 Complex Bug Investigation


## Bug Investigation: [BUG_ID]
 
### Characterization
| Aspect | Details |
|--------|---------|
| Primary symptom | [What users see] |
| Error messages | [Exact text] |
| Stack trace | [If available] |
| Frequency | [Pattern] |
| Environment | [Where it occurs] |
 
### Data Collection
 
#### Enhanced Logging

import logging
logging.basicConfig(level=logging.DEBUG)

Before suspicious operation
logger.debug(f"State before: {relevant_state}")

After operation  
logger.debug(f"State after: {relevant_state}")

 
#### Metrics to Capture
- [ ] Request/response timing
- [ ] Memory usage over time
- [ ] Connection pool status
- [ ] Queue depths
 
### Hypothesis Testing
 
#### Hypothesis 1: [DESCRIPTION]
**Test:** [How to verify]
**If true:** [What we'd see]
**If false:** [What we'd see]
**Result:** [Outcome]
 
#### Hypothesis 2: [DESCRIPTION]
[Same structure]
 
### Root Cause Analysis
 
#### Five Whys
1. Why [SYMPTOM]? â†’ [CAUSE 1]
2. Why [CAUSE 1]? â†’ [CAUSE 2]
3. Why [CAUSE 2]? â†’ [CAUSE 3]
4. Why [CAUSE 3]? â†’ [CAUSE 4]
5. Why [CAUSE 4]? â†’ **ROOT CAUSE**
 
### Fix & Verification
1. Implement fix
2. Write regression test
3. Verify fix in test environment
4. Deploy with monitoring
5. Confirm in production
 
### Documentation
- [ ] Bug report updated
- [ ] Runbook updated
- [ ] Monitoring added
- [ ] Post-mortem completed




19. Legacy Code & Technical Debt {#legacy}

19.1 Code Archaeology


## Code Archaeology: [MODULE/DIRECTORY]
 
### Discovery Phase
 
#### Code Mapping

File count and types
find . -type f | sed 's/.*\.//' | sort | uniq -c | sort -rn

Lines of code
cloc .

Age distribution
git log --format='%ai' --name-only | sort | uniq -c

Change frequency (hot spots)
git log --format=format: --name-only | sort | uniq -c | sort -rn | head -20

 
#### Dependency Analysis

Python imports
grep -r "^import\|^from" --include="*.py" | sort | uniq -c | sort -rn

Circular dependencies
pydeps --cluster --max-bacon 2 src/

 
### Knowledge Extraction
 
#### Business Logic Location
| Logic | File | Line Range | Confidence |
|-------|------|------------|------------|
| [Rule 1] | [file] | [lines] | H/M/L |
| [Rule 2] | [file] | [lines] | H/M/L |
 
#### Magic Numbers
| Value | Location | Meaning |
|-------|----------|---------|
| [42] | [file:line] | [Purpose] |
 
#### Implicit Assumptions
| Assumption | Location | Risk |
|------------|----------|------|
| [Assumption] | [file] | H/M/L |
 
### Risk Assessment
 
#### Technical Risks
| Component | Risk | Issue | Mitigation |
|-----------|------|-------|------------|
| [Component] | H/M/L | [Problem] | [Fix] |
 
#### Security Risks
| Issue | Location | Severity |
|-------|----------|----------|
| SQL injection? | [file] | [H/M/L] |
| Hardcoded secrets? | [file] | [H/M/L] |
 
### Modernization Strategy
 
#### Phase 1: Stabilization (Week 1-2)
1. Add comprehensive logging
2. Create test harness
3. Document current behavior
4. Set up monitoring
 
#### Phase 2: Incremental Refactoring (Week 3-8)
1. Extract interfaces
2. Add type hints
3. Introduce dependency injection
4. Apply design patterns
 
#### Phase 3: Component Replacement (Week 9-12)
1. Identify replacement boundaries
2. Implement new components
3. Create adapter layer
4. Gradual migration




19.2 Technical Debt Management


## Technical Debt Inventory: [PROJECT]
 
### Debt Categories
 
#### Code Quality
| Item | Location | Impact (1-10) | Effort (hrs) | ROI |
|------|----------|---------------|--------------|-----|
| Duplicated code | [files] | [7] | [8] | 0.88 |
| Complex functions | [files] | [6] | [4] | 1.50 |
 
#### Architecture
| Item | Location | Impact | Effort | ROI |
|------|----------|--------|--------|-----|
| Tight coupling | [modules] | [8] | [24] | 0.33 |
 
#### Testing
| Item | Gap | Impact | Effort | ROI |
|------|-----|--------|--------|-----|
| Missing unit tests | [X% uncovered] | [7] | [16] | 0.44 |
 
#### Documentation
| Item | Location | Impact | Effort | ROI |
|------|----------|--------|--------|-----|
| Outdated docs | [files] | [4] | [4] | 1.00 |
 
### Prioritization Matrix
 

        â”‚ Low Effort    â”‚ High Effort
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
High    â”‚ DO FIRST      â”‚ PLAN
Impact  â”‚ - [item]      â”‚ - [item]
        â”‚ - [item]      â”‚ - [item]
â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Low     â”‚ QUICK WINS    â”‚ AVOID
Impact  â”‚ - [item]      â”‚ - [item]

 
### Payment Plan
 
#### Sprint Allocation
- **Debt budget:** 20% of velocity
- **Points per sprint:** [X]
- **Debt points per sprint:** [X * 0.2]
 
#### Q1 Goals
1. [Debt item]: [Target date]
2. [Debt item]: [Target date]
 
### Success Metrics
| Metric | Current | Target | Timeline |
|--------|---------|--------|----------|
| Code coverage | [X%] | [Y%] | [Q1] |
| Complexity avg | [X] | [Y] | [Q2] |
| Bug rate | [X/month] | [Y/month] | [Q3] |






Part VI: Infrastructure & IDE Integration



20. Observability & Monitoring {#observability}

20.1 Observability Implementation


## Observability Stack for [SERVICE]
 
### The Three Pillars
 
#### 1. Metrics (Quantitative)
**Golden Signals:**
| Signal | Metric Name | Labels |
|--------|-------------|--------|
| Latency | `request_duration_seconds` | method, endpoint, status |
| Traffic | `request_total` | method, endpoint |
| Errors | `request_errors_total` | method, endpoint, error_type |
| Saturation | `resource_utilization_percent` | resource_type |
 
**Implementation:**

from prometheus_client import Counter, Histogram

REQUEST_COUNT = Counter(
    'request_total', 
    'Total requests',
    ['method', 'endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'request_duration_seconds',
    'Request latency',
    ['method', 'endpoint'],
    buckets=[.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10]
)

 
#### 2. Logs (Qualitative)
**Structured Format:**

{
  "timestamp": "2025-11-30T12:00:00Z",
  "level": "INFO",
  "service": "pipeline-service",
  "trace_id": "abc123",
  "span_id": "def456",
  "message": "Pipeline completed",
  "context": {
    "pipeline_id": "pipe-001",
    "duration_ms": 1234,
    "rows_processed": 10000
  }
}

 
**Log Levels:**
| Level | Use For |
|-------|---------|
| DEBUG | Detailed diagnostic info |
| INFO | Normal operations |
| WARN | Unexpected but recoverable |
| ERROR | Errors needing investigation |
| FATAL | Critical failures |
 
#### 3. Traces (Request Flow)

from opentelemetry import trace

tracer = trace.get_tracer(__name__)

async def process_pipeline(request):
    with tracer.start_as_current_span("process-pipeline") as span:
        span.set_attribute("pipeline.id", request.pipeline_id)
        span.set_attribute("pipeline.source", request.source)

        # Business logic
        result = await do_work()

        span.set_attribute("pipeline.rows", result.row_count)
        return result

 
### Alerting Rules
 

alert_rules.yml
groups:
  - name: sparkdata
    rules:
      - alert: HighErrorRate
        expr: rate(request_errors_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          runbook: "https://wiki/runbooks/high-error-rate"

      - alert: PipelineStalled
        expr: pipeline_last_success_timestamp < (time() - 3600)
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pipeline hasn't succeeded in 1 hour"

 
### Dashboard Essentials
 
**Service Overview:**
- Request rate (RPS)
- Error rate (%)
- Latency (p50, p95, p99)
- Active pipelines
- Resource utilization
 
**Pipeline Health:**
- Throughput (rows/minute)
- Success/failure rate
- Backlog size
- Processing latency




21. Project Sustainability {#sustainability}

21.1 Knowledge Base Maintenance


## Knowledge Base Update Protocol
 
### Weekly Review
- [ ] Update `docs/context/KNOWLEDGE_BASE.md` with new learnings
- [ ] Convert informal decisions to ADRs
- [ ] Archive old session states
- [ ] Refresh stale documentation
 
### Knowledge Capture Template

Learning: [TITLE]
Date: [YYYY-MM-DD]
Context: [What we were working on]

The Problem
[What issue we encountered]

What We Learned
[Key insight or solution]

Implications
[How this affects future work]

Evidence
[Links to commits, files, or discussions]

 
### Documentation Health Check
| Document | Last Updated | Status | Owner |
|----------|--------------|--------|-------|
| README.md | [date] | Current/Stale | [who] |
| ARCHITECTURE.md | [date] | Current/Stale | [who] |
| API docs | [date] | Current/Stale | [who] |




22. Infrastructure & Automation {#infrastructure}

22.1 Environment Configuration


## Environment Management
 
### Environment Definitions
 
#### Development

docker-compose.dev.yml
services:
  app:
    build: .
    environment:
      - ENV=development
      - DATABASE_URL=postgres://localhost/dev
      - LOG_LEVEL=DEBUG
    volumes:
      - .:/app
    ports:
      - "3000:3000"

 
#### Staging
- Database: [Connection method]
- Feature flags: [Settings]
- Resource limits: [Specs]
 
#### Production
- Database: [Connection method]
- Feature flags: [Settings]
- Resource limits: [Specs]
- Replicas: [Count]
 
### Secret Management
| Secret Type | Storage | Rotation |
|-------------|---------|----------|
| API Keys | Vault | 90 days |
| DB Credentials | Vault | 30 days |
| Certificates | Vault | Before expiry |
 
### Required Environment Variables

.env.example
Application
APP_ENV=development
APP_PORT=3000
LOG_LEVEL=debug

Database
DATABASE_URL=
DATABASE_POOL_SIZE=10

External Services
OPENAI_API_KEY=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

Feature Flags
FEATURE_NEW_PIPELINE=false



22.2 CI/CD Pipeline


# .github/workflows/ci.yml
name: CI/CD Pipeline
 
on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
 
jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Lint
        run: ruff check .
      
      - name: Type Check
        run: mypy .
 
  test:
    needs: validate
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Tests
        run: pytest -v --cov=src --cov-report=xml
      
      - name: Upload Coverage
        uses: codecov/codecov-action@v3
 
  build:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Build Container
        run: docker build -t app:${{ github.sha }} .
      
      - name: Push to Registry
        run: docker push app:${{ github.sha }}
 
  deploy:
    needs: build
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to Staging
        run: ./scripts/deploy.sh staging
      
      - name: Smoke Tests
        run: ./scripts/smoke-test.sh staging
      
      - name: Deploy to Production
        if: success()
        run: ./scripts/deploy.sh production




23. IDE Integration & Configuration {#ide-integration}

23.1 Repository Configuration Files

`.cursorrules` / `CLAUDE.md`

Create this file in your repository root:


# SparkData Analytics - AI Coding Rules
 
## Role
You are a senior data engineer at SparkData Analytics.
 
## Anti-Sycophancy Rules (CRITICAL)
1. **Never blindly agree** with user code or previous context
2. **Flag potential issues immediately** - even if uncertain
3. **Challenge assumptions** - ask clarifying questions
4. **No fluff** - start directly with solution or critique
 
## Forbidden Behaviors
- Do NOT start responses with "Here is the code" or "I have analyzed..."
- Do NOT summarize code unless explicitly asked to "explain"
- Do NOT agree that code "looks good" without evidence
- Do NOT skip test writing
 
## Required Behaviors
- PROVE correctness via tests before claiming something works
- ASSUME code is broken until tests pass
- RUN linters and type checkers, don't just reason about them
- CAPTURE evidence of passing checks
 
## Tech Stack
- Python 3.11+
- pytest for testing
- ruff for linting
- mypy for type checking
- PostgreSQL for database
- Docker for containers
 
## Code Style
- Type hints required on all functions
- Docstrings required on all public functions
- Maximum function length: 50 lines
- Maximum file length: 500 lines
 
## Testing Requirements
- Write tests BEFORE implementation
- Minimum 80% coverage
- All edge cases must have tests
- No mocking of core business logic
 
## Before Committing
Always run:

make lint      # Must pass
make typecheck # Must pass
make test      # Must pass

 
## Documentation
- Check `docs/decisions/` before architectural changes
- Update CHANGELOG.md for all changes
- Reference ADRs when relevant
 
## Data Pipeline Rules
- All pipelines must be idempotent
- All pipelines must have data quality checks
- All pipelines must capture row counts and metrics
- Schema validation required on all inputs/outputs




23.2 VS Code / Cursor Settings


// .vscode/settings.json
{
  "editor.formatOnSave": true,
  "editor.codeActionsOnSave": {
    "source.fixAll.ruff": "explicit",
    "source.organizeImports.ruff": "explicit"
  },
  "python.analysis.typeCheckingMode": "strict",
  "python.testing.pytestEnabled": true,
  "python.testing.pytestArgs": [
    "tests"
  ],
  "[python]": {
    "editor.defaultFormatter": "charliermarsh.ruff"
  },
  "files.exclude": {
    "**/__pycache__": true,
    "**/.pytest_cache": true,
    "**/htmlcov": true,
    "**/*.egg-info": true
  },
  "search.exclude": {
    "**/node_modules": true,
    "**/venv": true,
    "**/.git": true
  }
}




23.3 Pre-commit Hooks


# .pre-commit-config.yaml
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.6
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
 
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.7.0
    hooks:
      - id: mypy
        additional_dependencies:
          - types-requests
 
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
      - id: detect-private-key
 
  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: pytest tests/unit -v --tb=short
        language: system
        pass_filenames: false
        always_run: true


Install: pre-commit install



23.4 Makefile for Common Tasks


# Makefile
 
.PHONY: install lint typecheck test coverage build clean
 
# Install dependencies
install:
	pip install -r requirements.txt
	pip install -r requirements-dev.txt
	pre-commit install
 
# Linting
lint:
	ruff check .
	ruff format --check .
 
# Type checking
typecheck:
	mypy src/
 
# Run tests
test:
	pytest tests/ -v --tb=short
 
# Run tests with coverage
coverage:
	pytest tests/ -v --cov=src --cov-report=term-missing --cov-report=html
 
# Full quality check (use before commit)
check: lint typecheck test
 
# Build container
build:
	docker build -t sparkdata-app:latest .
 
# Clean artifacts
clean:
	rm -rf __pycache__ .pytest_cache .mypy_cache htmlcov .coverage
	find . -type d -name __pycache__ -exec rm -rf {} +
 
# Evidence collection for handoffs
evidence:
	mkdir -p evidence
	make test 2>&1 | tee evidence/test_$$(date +%Y%m%d_%H%M%S).txt
	make lint 2>&1 | tee evidence/lint_$$(date +%Y%m%d_%H%M%S).txt
	make typecheck 2>&1 | tee evidence/types_$$(date +%Y%m%d_%H%M%S).txt
 
# Generate context for AI sessions
context:
	./scripts/gather-context.sh




23.5 IDE Quick Commands

Add these to your IDE's command palette or keyboard shortcuts:


Command
Action
Shortcut Suggestion
ai-plan
Open Planner Agent session
Ctrl+Shift+P
ai-build
Open Builder Agent session
Ctrl+Shift+B
ai-review
Open Reviewer Agent session
Ctrl+Shift+R
ai-test
Generate tests for selection
Ctrl+Shift+T
ai-explain
Explain selected code
Ctrl+Shift+E


Planner Quick Prompt:

You are PLANNER mode. Create implementation plan for: [selection]
Include: task breakdown, test plan, acceptance criteria.
Do NOT write implementation code.


Builder Quick Prompt:

You are BUILDER mode. Implement: [selection]
Rules: Write tests FIRST. Leave in pending-review state.
Run `make check` before claiming complete.


Reviewer Quick Prompt:

You are REVIEWER mode (blind). Review this code:
[selection]
You have NOT seen the builder's explanation.
Find problems. Use adversarial review rubric from SOP Â§8.1.




23.6 Multi-Agent Session Setup


#!/bin/bash
# setup-agent-sessions.sh
# Sets up separate terminal tabs for multi-agent workflow
 
# Create session directories
mkdir -p .agent_sessions/{planner,builder,reviewer}
 
# Terminal 1: Planner
echo "Opening Planner terminal..."
gnome-terminal --tab --title="PLANNER" -- bash -c "
  echo '=== PLANNER AGENT ==='
  echo 'Mode: Planning only, no implementation'
  echo ''
  cd $(pwd)
  exec bash
"
 
# Terminal 2: Builder
echo "Opening Builder terminal..."
gnome-terminal --tab --title="BUILDER" -- bash -c "
  echo '=== BUILDER AGENT ==='
  echo 'Mode: Implementation + tests, no self-approval'
  echo ''
  cd $(pwd)
  exec bash
"
 
# Terminal 3: Reviewer
echo "Opening Reviewer terminal..."
gnome-terminal --tab --title="REVIEWER" -- bash -c "
  echo '=== REVIEWER AGENT (BLIND) ==='
  echo 'Mode: Adversarial review, no implementation'
  echo 'WARNING: Do NOT load builder explanations!'
  echo ''
  cd $(pwd)
  exec bash
"
 
echo "Agent sessions ready."
echo "Use separate Claude/GPT sessions in each terminal."






Part VII: Data Engineering Standards



24. Pipeline-Specific Standards {#pipeline-standards}

24.1 Pipeline Implementation Requirements

Every data pipeline at SparkData Analytics must satisfy these requirements:


## Pipeline Requirements Checklist
 
### Data Contracts (Required)
- [ ] Input schema defined (`schemas/input_[pipeline].json`)
- [ ] Output schema defined (`schemas/output_[pipeline].json`)
- [ ] Null handling specified for each field
- [ ] Valid value ranges documented
- [ ] Uniqueness constraints documented
- [ ] Version number for schema compatibility
 
### Data Quality Tests (Required)
- [ ] Row count assertions (expected range)
- [ ] Null checks on NOT NULL fields
- [ ] Referential integrity checks
- [ ] Duplicate detection on key fields
- [ ] Business rule validations
- [ ] Distribution/outlier checks
 
### Operational Guarantees (Required)
- [ ] **Idempotent:** Running twice = same result
- [ ] **Backfillable:** Can reprocess historical data
- [ ] **Resumable:** Can continue after failure
- [ ] **Clean failures:** No corrupt state on error
 
### Observability (Required)
- [ ] Structured logging with correlation IDs
- [ ] Metrics captured:
  - `pipeline_input_rows`
  - `pipeline_output_rows`
  - `pipeline_error_count`
  - `pipeline_duration_seconds`
  - `pipeline_null_rate_by_field`
- [ ] Alerts for anomalies
 
### Error Handling (Required)
- [ ] All exceptions caught and logged
- [ ] Retry logic for transient failures
- [ ] Dead letter queue for poison records
- [ ] Graceful degradation where possible
 
### Documentation (Required)
- [ ] Purpose documented in docstring
- [ ] Data flow diagram in `docs/`
- [ ] SLA/SLO defined
- [ ] Runbook in `docs/runbooks/`




24.2 Pipeline Modification Checklist

Before modifying an existing pipeline:


## Pipeline Modification Protocol
 
### Pre-Modification
- [ ] Understand current behavior (read tests)
- [ ] Document current metrics baseline
- [ ] Identify downstream consumers
- [ ] Plan backward compatibility
 
### During Modification
- [ ] Add tests for new behavior FIRST
- [ ] Modify implementation
- [ ] Verify existing tests still pass
- [ ] Update schema versions if needed
- [ ] Update documentation
 
### Post-Modification
- [ ] Run full test suite
- [ ] Verify metrics in staging
- [ ] Canary deployment
- [ ] Monitor for 24-48 hours
- [ ] Update runbook if behavior changed
 
### Breaking Change Protocol
If schema or behavior changes in breaking way:
1. Add version to schema (v1 â†’ v2)
2. Support both versions during transition
3. Notify downstream consumers
4. Deprecation timeline (30+ days)
5. Remove old version after migration




25. Data Quality & Contracts {#data-quality}

25.1 Schema Definition Template


# schemas/pipeline_example.py
"""
Schema definitions for Example Pipeline.
Version: 1.0.0
Last Updated: 2025-11-30
"""
 
from dataclasses import dataclass
from typing import Optional
from datetime import datetime
from enum import Enum
 
class ProcessingStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
 
@dataclass
class InputRecord:
    """Input schema for Example Pipeline."""
    
    # Required fields (NOT NULL)
    id: str                          # Unique identifier, UUID format
    source_system: str               # Origin system code
    timestamp: datetime              # Event timestamp, UTC
    
    # Optional fields
    value: Optional[float] = None    # Numeric value, range: 0-1000000
    category: Optional[str] = None   # Category code, max 50 chars
    metadata: Optional[dict] = None  # Additional attributes
 
    def validate(self) -> list[str]:
        """Validate record against schema constraints."""
        errors = []
        
        if not self.id:
            errors.append("id is required")
        if self.value is not None and (self.value < 0 or self.value > 1000000):
            errors.append(f"value {self.value} out of range [0, 1000000]")
        if self.category and len(self.category) > 50:
            errors.append(f"category exceeds 50 chars")
            
        return errors
 
 
@dataclass
class OutputRecord:
    """Output schema for Example Pipeline."""
    
    id: str
    processed_value: float
    status: ProcessingStatus
    processed_at: datetime
    source_id: str  # Foreign key to input.id
    
    # Computed fields
    quality_score: float  # 0.0 - 1.0
 
 
# Schema version for compatibility checks
SCHEMA_VERSION = "1.0.0"
INPUT_SCHEMA = InputRecord
OUTPUT_SCHEMA = OutputRecord




25.2 Data Quality Checks Implementation


# quality/checks.py
"""
SparkData Analytics - Standard Data Quality Checks
"""
 
import pandas as pd
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging
 
logger = logging.getLogger(__name__)
 
 
@dataclass
class QualityCheckResult:
    """Result of a quality check."""
    check_name: str
    passed: bool
    message: str
    details: Optional[Dict[str, Any]] = None
 
 
class PipelineQualityChecker:
    """Runs standard quality checks on pipeline data."""
    
    def __init__(self, pipeline_name: str):
        self.pipeline_name = pipeline_name
        self.results: List[QualityCheckResult] = []
    
    def check_row_count(
        self, 
        df: pd.DataFrame, 
        min_rows: int, 
        max_rows: int
    ) -> QualityCheckResult:
        """Verify row count is within expected range."""
        actual = len(df)
        passed = min_rows <= actual <= max_rows
        
        result = QualityCheckResult(
            check_name="row_count",
            passed=passed,
            message=f"Row count {actual} {'within' if passed else 'outside'} range [{min_rows}, {max_rows}]",
            details={"actual": actual, "min": min_rows, "max": max_rows}
        )
        self.results.append(result)
        
        if not passed:
            logger.warning(f"[{self.pipeline_name}] Row count check failed: {result.message}")
        
        return result
    
    def check_no_nulls(
        self, 
        df: pd.DataFrame, 
        columns: List[str]
    ) -> QualityCheckResult:
        """Verify specified columns have no null values."""
        null_counts = {col: df[col].isna().sum() for col in columns}
        has_nulls = any(count > 0 for count in null_counts.values())
        
        result = QualityCheckResult(
            check_name="no_nulls",
            passed=not has_nulls,
            message=f"Null check {'passed' if not has_nulls else 'failed'}",
            details={"null_counts": null_counts}
        )
        self.results.append(result)
        
        if has_nulls:
            logger.warning(f"[{self.pipeline_name}] Null check failed: {null_counts}")
        
        return result
    
    def check_unique(
        self, 
        df: pd.DataFrame, 
        columns: List[str]
    ) -> QualityCheckResult:
        """Verify no duplicates on specified columns."""
        duplicates = df.duplicated(subset=columns, keep=False)
        dup_count = duplicates.sum()
        
        result = QualityCheckResult(
            check_name="unique",
            passed=dup_count == 0,
            message=f"Uniqueness check: {dup_count} duplicates found",
            details={"columns": columns, "duplicate_count": dup_count}
        )
        self.results.append(result)
        
        if dup_count > 0:
            logger.warning(f"[{self.pipeline_name}] Duplicates found on {columns}")
        
        return result
    
    def check_referential_integrity(
        self,
        df: pd.DataFrame,
        column: str,
        reference_values: set
    ) -> QualityCheckResult:
        """Verify all values exist in reference set."""
        values = set(df[column].dropna())
        orphans = values - reference_values
        
        result = QualityCheckResult(
            check_name="referential_integrity",
            passed=len(orphans) == 0,
            message=f"Referential integrity: {len(orphans)} orphan values",
            details={"column": column, "orphan_count": len(orphans), "sample_orphans": list(orphans)[:5]}
        )
        self.results.append(result)
        
        if orphans:
            logger.warning(f"[{self.pipeline_name}] Orphan values in {column}: {list(orphans)[:5]}")
        
        return result
    
    def check_value_range(
        self,
        df: pd.DataFrame,
        column: str,
        min_val: float,
        max_val: float
    ) -> QualityCheckResult:
        """Verify values are within expected range."""
        out_of_range = df[(df[column] < min_val) | (df[column] > max_val)]
        count = len(out_of_range)
        
        result = QualityCheckResult(
            check_name="value_range",
            passed=count == 0,
            message=f"Value range check for {column}: {count} out of range",
            details={"column": column, "min": min_val, "max": max_val, "out_of_range_count": count}
        )
        self.results.append(result)
        
        return result
    
    def get_summary(self) -> Dict[str, Any]:
        """Get summary of all quality checks."""
        return {
            "pipeline": self.pipeline_name,
            "total_checks": len(self.results),
            "passed": sum(1 for r in self.results if r.passed),
            "failed": sum(1 for r in self.results if not r.passed),
            "all_passed": all(r.passed for r in self.results),
            "results": [
                {
                    "check": r.check_name,
                    "passed": r.passed,
                    "message": r.message
                }
                for r in self.results
            ]
        }
    
    def assert_all_passed(self) -> None:
        """Raise exception if any check failed."""
        if not all(r.passed for r in self.results):
            failed = [r for r in self.results if not r.passed]
            raise DataQualityError(
                f"Quality checks failed: {[r.check_name for r in failed]}"
            )
 
 
class DataQualityError(Exception):
    """Raised when data quality check fails."""
    pass




26. Schema Management {#schema-management}

26.1 Schema Evolution Guidelines


## Schema Evolution Policy
 
### Backward Compatible Changes (Safe)
âœ… Can deploy without coordination:
- Adding new optional columns
- Adding new tables
- Widening column types (int â†’ bigint)
- Adding new enum values at the end
- Adding indexes
 
### Backward Incompatible Changes (Breaking)
âš ï¸ Require migration plan:
- Removing columns
- Renaming columns
- Changing column types
- Removing enum values
- Changing NOT NULL constraints
- Changing primary keys
 
### Migration Process for Breaking Changes
 
1. **Version the schema** (v1 â†’ v2)
2. **Add new version support** without removing old
3. **Notify consumers** with timeline
4. **Parallel run period** (minimum 30 days)
5. **Migration complete** when all consumers updated
6. **Remove old version** after confirmation
 
### Schema Versioning Convention

schemas/
â”œâ”€â”€ pipeline_example/
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ input.json
â”‚   â”‚   â””â”€â”€ output.json
â”‚   â””â”€â”€ v2/
â”‚       â”œâ”€â”€ input.json
â”‚       â”œâ”€â”€ output.json
â”‚       â””â”€â”€ MIGRATION.md



Part VIII: Meta



27. Prompt Engineering Best Practices {#prompt-engineering}

27.1 Effective Prompt Structure


## Prompt Anatomy for SparkData
 
### Required Elements
1. **Role Declaration** - Who is the AI
2. **Context** - What they're working on
3. **Task** - Specific action required
4. **Constraints** - Rules and limitations
5. **Output Format** - Expected structure
 
### Template

Role
You are [ROLE] for SparkData Analytics.

Context
[Background information needed]

Task
[Specific action to perform]

Constraints
	â€¢	[Rule 1]
	â€¢	[Rule 2]
	â€¢	[Rule 3]

Output Format
[Specify structure of expected output]

 
### Anti-Patterns to Avoid
âŒ "Please help me with..."
âŒ "Can you summarize..."
âŒ "I think the code should..."
âŒ Open-ended questions without structure
 
### Patterns That Work
âœ… "You are [ROLE]. Your task is [SPECIFIC]."
âœ… "Output format: [STRUCTURE]"
âœ… "Constraints: Do NOT [THING]"
âœ… "Evidence required: [WHAT]"




27.2 Model-Specific Tuning


## Model-Specific Prompt Adjustments
 
### Claude (Anthropic)
**Strengths:**
- Long context understanding
- Following complex instructions
- Structured output
- Code analysis
 
**Adjustments:**
- Use XML-like tags for structure
- Be explicit about constraints
- Provide examples for edge cases
 
### GPT-4/5 (OpenAI)
**Strengths:**
- Code generation
- Creative solutions
- Quick iterations
 
**Adjustments:**
- Use markdown for structure
- Break complex tasks into steps
- Provide clear success criteria
 
### Model Selection by Task
| Task | Recommended Model |
|------|------------------|
| Planning | Claude |
| Implementation | GPT |
| Code Review | Claude |
| Documentation | Either |
| Debugging | Claude |




28. Prompt Evolution Tracking {#prompt-evolution}


## Prompt Version Control
 
### Prompt Metadata

prompt_id: ACR-001
name: Adversarial Code Review
version: 2.1.0
created: 2025-10-01
last_modified: 2025-11-30
author: SparkData Engineering
category: quality-control

 
### Version History
| Version | Date | Changes | Effectiveness |
|---------|------|---------|---------------|
| 1.0.0 | 2025-10 | Initial version | 3/5 |
| 2.0.0 | 2025-11 | Added rubric scoring | 4/5 |
| 2.1.0 | 2025-11 | Added blind audit rules | 5/5 |
 
### Effectiveness Tracking
| Metric | Before | After | Delta |
|--------|--------|-------|-------|
| Issues found | 2.1/review | 4.7/review | +124% |
| False positives | 35% | 12% | -66% |
| Time to review | 45 min | 20 min | -56% |
 
### A/B Testing
When modifying prompts:
1. Run both versions on same input
2. Compare outputs
3. Measure against success criteria
4. Document learnings




29. Quick Reference Cards {#quick-reference}

29.1 Daily Workflow Card


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SPARKDATA DAILY WORKFLOW                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  ğŸŸ¢ START SESSION                                                 â•‘
â•‘  â”œâ”€ Run: ./scripts/gather-context.sh                             â•‘
â•‘  â”œâ”€ Load: Session Initialization (Â§5.1 or Â§5.3)                  â•‘
â•‘  â””â”€ Verify: Previous claims match reality                        â•‘
â•‘                                                                   â•‘
â•‘  ğŸ”¨ IMPLEMENTATION (Builder Mode)                                 â•‘
â•‘  â”œâ”€ Write tests FIRST (TDD)                                      â•‘
â•‘  â”œâ”€ Implement to pass tests                                      â•‘
â•‘  â”œâ”€ Run: make check                                              â•‘
â•‘  â””â”€ Leave in "pending review" state                              â•‘
â•‘                                                                   â•‘
â•‘  ğŸ”´ QUALITY CONTROL (Reviewer Mode - SEPARATE SESSION)           â•‘
â•‘  â”œâ”€ Open FRESH session (no prior context)                        â•‘
â•‘  â”œâ”€ Load: Blind Review Init (Â§5.3)                               â•‘
â•‘  â”œâ”€ Run: Adversarial Code Review (Â§8.1)                          â•‘
â•‘  â””â”€ Output: REVIEW_*.md with verdict                             â•‘
â•‘                                                                   â•‘
â•‘  ğŸ“ END SESSION                                                   â•‘
â•‘  â”œâ”€ Run: make evidence                                           â•‘
â•‘  â”œâ”€ Generate: Progress Report (Â§6.1)                             â•‘
â•‘  â”œâ”€ If handoff: Complete Handoff (Â§6.2)                          â•‘
â•‘  â””â”€ Commit with comprehensive message                            â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




29.2 Git Workflow Card


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                       GIT WORKFLOW                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  # Start feature                                                  â•‘
â•‘  git checkout -b feature/[name]                                   â•‘
â•‘                                                                   â•‘
â•‘  # Save work in progress                                          â•‘
â•‘  git add . && git commit -m "WIP: [description]"                  â•‘
â•‘                                                                   â•‘
â•‘  # Update from main                                               â•‘
â•‘  git fetch origin && git rebase origin/main                       â•‘
â•‘                                                                   â•‘
â•‘  # Push for review                                                â•‘
â•‘  git push -u origin feature/[name]                                â•‘
â•‘                                                                   â•‘
â•‘  # After approval                                                 â•‘
â•‘  git checkout main && git pull                                    â•‘
â•‘  git merge --no-ff feature/[name] -m "feat: [description]"        â•‘
â•‘  git push origin main                                             â•‘
â•‘  git branch -d feature/[name]                                     â•‘
â•‘                                                                   â•‘
â•‘  # Commit message format                                          â•‘
â•‘  [type]([scope]): [subject]                                       â•‘
â•‘                                                                   â•‘
â•‘  [body - what and why]                                            â•‘
â•‘                                                                   â•‘
â•‘  Reviewed-by: [model]                                             â•‘
â•‘  Risk-Score: [0-10]                                               â•‘
â•‘  Closes #[ticket]                                                 â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




29.3 Emergency Response Card


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   EMERGENCY RESPONSE                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  ğŸš¨ INCIDENT DETECTED                                             â•‘
â•‘                                                                   â•‘
â•‘  1. [ ] Verify incident is real (not false positive)              â•‘
â•‘  2. [ ] Assess impact scope                                       â•‘
â•‘  3. [ ] Create #incident-[timestamp] channel                      â•‘
â•‘  4. [ ] Notify on-call engineer                                   â•‘
â•‘  5. [ ] Update status page                                        â•‘
â•‘  6. [ ] Implement quick mitigation                                â•‘
â•‘  7. [ ] Begin root cause analysis                                 â•‘
â•‘  8. [ ] Document everything                                       â•‘
â•‘                                                                   â•‘
â•‘  QUICK MITIGATIONS:                                               â•‘
â•‘  - Rollback: kubectl rollout undo deployment/app                  â•‘
â•‘  - Scale: kubectl scale deployment/app --replicas=10              â•‘
â•‘  - Circuit break: Enable feature flag CIRCUIT_BREAK=true          â•‘
â•‘  - Redirect: Update load balancer to backup                       â•‘
â•‘                                                                   â•‘
â•‘  ESCALATION:                                                      â•‘
â•‘  - P0: Immediate page + status page + stakeholders                â•‘
â•‘  - P1: 30min response + team channel                              â•‘
â•‘  - P2: 4hr response + email                                       â•‘
â•‘  - P3: 24hr response + ticket                                     â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




29.4 Adversarial Review Quick Reference


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  ADVERSARIAL REVIEW QUICK REF                      â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  âš ï¸  BEFORE REVIEWING:                                            â•‘
â•‘  - [ ] Fresh session (no prior context)                           â•‘
â•‘  - [ ] NO builder explanations loaded                             â•‘
â•‘  - [ ] Only have: spec + code + test output                       â•‘
â•‘                                                                   â•‘
â•‘  ğŸ“‹ REVIEW CHECKLIST:                                             â•‘
â•‘  - [ ] Correctness vs spec                                        â•‘
â•‘  - [ ] Edge cases handled                                         â•‘
â•‘  - [ ] Error handling complete                                    â•‘
â•‘  - [ ] Tests adequate                                             â•‘
â•‘  - [ ] Security checked                                           â•‘
â•‘  - [ ] Performance acceptable                                     â•‘
â•‘  - [ ] Observability present                                      â•‘
â•‘                                                                   â•‘
â•‘  ğŸ¯ RISK SCORE:                                                   â•‘
â•‘  0-2: APPROVE (low risk)                                          â•‘
â•‘  3-5: REQUEST_CHANGES (moderate)                                  â•‘
â•‘  6-8: BLOCK (high risk)                                           â•‘
â•‘  9-10: CRITICAL (do not deploy)                                   â•‘
â•‘                                                                   â•‘
â•‘  ğŸ“ REQUIRED OUTPUT:                                              â•‘
â•‘  - Defect table with severity                                     â•‘
â•‘  - Risk score with justification                                  â•‘
â•‘  - Verdict: APPROVE / REQUEST_CHANGES / BLOCK                     â•‘
â•‘  - Missing tests list                                             â•‘
â•‘  - Assumptions list                                               â•‘
â•‘                                                                   â•‘
â•‘  âŒ NEVER SAY:                                                    â•‘
â•‘  - "Looks good" (without evidence)                                â•‘
â•‘  - "The author did..." (you don't know the author)                â•‘
â•‘  - "As mentioned before..." (no prior context)                    â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




29.5 File Naming Quick Reference


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FILE NAMING STANDARDS                           â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  Progress Reports:                                                â•‘
â•‘  PROGRESS_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION_ID].md             â•‘
â•‘  Example: PROGRESS_2025-11-30_14-30-00_claude-opus_abc123.md      â•‘
â•‘                                                                   â•‘
â•‘  Handoff Documents:                                               â•‘
â•‘  docs/handoffs/HANDOFF_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION].md   â•‘
â•‘                                                                   â•‘
â•‘  Review Documents:                                                â•‘
â•‘  REVIEW_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION_ID].md               â•‘
â•‘                                                                   â•‘
â•‘  Discrepancy Reports:                                             â•‘
â•‘  DISCREPANCY_YYYY-MM-DD_HH-MM-SS.md                               â•‘
â•‘                                                                   â•‘
â•‘  Decision Records:                                                â•‘
â•‘  docs/decisions/ADR_YYYY-MM-DD_[NUM]_[title-slug].md              â•‘
â•‘                                                                   â•‘
â•‘  Session States:                                                  â•‘
â•‘  state/session_YYYY-MM-DD_HH-MM-SS_[MODEL]_[SESSION].json         â•‘
â•‘                                                                   â•‘
â•‘  Evidence Files:                                                  â•‘
â•‘  evidence/[type]_YYYYMMDD_HHMMSS.txt                              â•‘
â•‘  Types: test, lint, types, build, coverage                        â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




29.6 Commands Quick Reference


# Quality Gates
make lint        # Run linter
make typecheck   # Run type checker
make test        # Run tests
make coverage    # Run tests with coverage
make check       # Run all above
 
# Evidence Collection
make evidence    # Collect all evidence files
 
# Context Gathering
./scripts/gather-context.sh   # Prepare for AI session
 
# Deployment
./scripts/deploy.sh staging   # Deploy to staging
./scripts/deploy.sh prod      # Deploy to production
 
# Rollback
kubectl rollout undo deployment/app
git reset --hard [sha]        # Rollback to commit




Document Certification


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     DOCUMENT CERTIFICATION                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                   â•‘
â•‘  Document: SparkData Analytics AI-Assisted Development SOP        â•‘
â•‘  Version: 5.2.0 (Gold Standard Compliance Edition)                â•‘
â•‘  Last Updated: November 30, 2025                                  â•‘
â•‘                                                                   â•‘
â•‘  This document has been designed to:                              â•‘
â•‘  âœ“ Eliminate AI sycophancy in code review                        â•‘
â•‘  âœ“ Enable multi-agent agentic workflows                          â•‘
â•‘  âœ“ Enforce proof-of-work quality gates                           â•‘
â•‘  âœ“ Maintain context across long-running projects                 â•‘
â•‘  âœ“ Support SparkData's data engineering focus                    â•‘
â•‘                                                                   â•‘
â•‘  Core Improvements Over Previous Versions:                        â•‘
â•‘  â€¢ Added Fifth Pillar: Adversarial Independence                  â•‘
â•‘  â€¢ Added Role Separation (Builder/Reviewer/etc.)                 â•‘
â•‘  â€¢ Added Blind Audit Protocols                                   â•‘
â•‘  â€¢ Added Proof of Work Requirements                              â•‘
â•‘  â€¢ Added Agentic Coding Loop (Workflow 4)                        â•‘
â•‘  â€¢ Added Pipeline-Specific Standards                             â•‘
â•‘  â€¢ Added IDE Integration (.cursorrules)                          â•‘
â•‘                                                                   â•‘
â•‘  Approved By: SparkData Engineering                               â•‘
â•‘  Next Review: Q1 2026                                             â•‘
â•‘                                                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




APPENDIX E: T1 Worked Example (Minor UI Change)

This example demonstrates the lightweight T1 workflow for a typical minor change.

E.1 Scenario

Change: Fix a small front-end data-binding bug in the user dashboard.

E.2 Risk Tier Declaration


## Risk Tier Declaration
 
**Assigned Tier:** T1 (Medium)
**Justification:** UI change affecting non-core feature display. No data model changes, no auth impact, no pipeline changes.
**Elevating factors:** None
**Required controls per Â§1.2.2:**
- [x] Builder Agent
- [x] Adversarial Reviewer (blind, fresh chat)
- [x] test_evidence.txt
- [x] lint_evidence.txt  
- [x] types_evidence.txt
 
**Escalation (if applicable):** N/A


E.3 Builder Mode Output (Chat 1)


### Metadata Block
Model: claude-opus-4
Timestamp: 2025-11-30T14:30:00Z
Risk Tier: T1
Session ID: bld-abc123
 
### Implementation
 


+++ b/src/components/Dashboard.tsx
@@ -42,7 +42,7 @@ export function Dashboard({ user }: DashboardProps) {
   return (
     <div className="dashboard">
       <h1>Welcome, {user.name}</h1>
	â€¢	<MetricsCard value={metrics.count} />
+      <MetricsCard value={metrics?.count ?? 0} />
     </div>
   );
 }

 
### Summary:
- Added null-safe access to `metrics.count` with fallback to 0
- Prevents crash when metrics data hasn't loaded yet
- Added test for null metrics case
- Assumption: 0 is acceptable default while loading


E.4 Evidence Collection


# test_evidence.txt
=== TEST EVIDENCE ===
Timestamp: 2025-11-30T14:35:00Z
Git SHA: a1b2c3d
 
pytest tests/ -v --tb=short
========================
23 passed, 0 failed, 0 skipped
========================
Exit Code: 0
 
# lint_evidence.txt  
ruff check .
All checks passed!
Exit Code: 0
 
# types_evidence.txt
mypy src/
Success: no issues found in 42 source files
Exit Code: 0


E.5 Reviewer Mode Output (Chat 2 - FRESH SESSION)


### Metadata Block
Model: gpt-4o
Timestamp: 2025-11-30T14:45:00Z
Risk Tier: T1
Session ID: rev-xyz789
Review Type: Adversarial Review (Blind)
 
### Checklist
| Check | Status | Evidence |
|-------|--------|----------|
| Correctness vs SPEC | Pass | Null-safe access matches bug description |
| Edge cases handled | Pass | Fallback to 0 covers loading state |
| Test quality | Pass | New test covers null metrics case |
| Security concerns | Pass | No security implications for UI null check |
| Performance | Pass | Optional chaining is O(1) |
| Patterns | Pass | Follows existing null-safe patterns in codebase |
| Logging | N/A | UI component, no logging needed |
| Backwards compat | Pass | No API changes |
 
### Risk Score
**Score: 1/10**
 
Low risk change. Single-line null-safe fix with appropriate test coverage. The optional chaining pattern is consistent with the rest of the codebase. Default value of 0 is reasonable during loading state.
 
### Required Changes
None required. Risk â‰¤ 2.
 
Verified:
- Fix addresses the bug report correctly
- Test covers the specific failure case
- No regressions in test suite (23 passed)
- Lint and type checks clean
 
### Verdict
**APPROVE**


E.6 Decision Log


{
  "decision_id": "DECISION_2025-11-30T14-50-00Z_dashboard-fix",
  "tier": "T1",
  "timestamp": "2025-11-30T14:50:00Z",
  "models": [
    {"name": "claude-opus-4", "role": "builder"},
    {"name": "gpt-4o", "role": "reviewer"}
  ],
  "risk_score": 1,
  "verdict": "APPROVE",
  "issues": [],
  "human_override": {"status": "none", "approver": null, "notes": null}
}


E.7 Merge


git checkout main && git pull
git merge --no-ff feature/dashboard-null-fix -m "fix: null-safe metrics access in dashboard
 
Reviewed-by: gpt-4o
Review-ID: rev-xyz789
Risk-Score: 1/10
 
Closes #142"


Total time: ~30 minutes from start to merge.



APPENDIX F: AI Orchestrator Script

F.1 Purpose

This script guides developers through the tier-appropriate workflow, checking that required evidence files exist before proceeding.

F.2 Script: `scripts/ai_orchestrator.sh`


#!/bin/bash
# SparkData Analytics - AI Orchestrator
# Guides developers through tier-appropriate AI workflow
 
set -e
 
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color
 
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘              SPARKDATA AI ORCHESTRATOR v1.0                        â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
 
# Step 1: Get Risk Tier
echo -e "${YELLOW}Step 1: What is the Risk Tier for this change?${NC}"
echo "  T0 - Low (docs, comments, non-prod scripts)"
echo "  T1 - Medium (non-core features, UI, minor pipelines) [DEFAULT]"
echo "  T2 - High (core logic, data models, auth, key pipelines)"
echo "  T3 - Critical (security, PII, billing, regulatory)"
echo ""
read -p "Enter tier [T0/T1/T2/T3]: " TIER
TIER=${TIER:-T1}
 
echo ""
echo -e "${GREEN}Selected: $TIER${NC}"
echo ""
 
# Step 2: Show required workflow
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "REQUIRED WORKFLOW FOR $TIER:"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
 
case $TIER in
  T0)
    echo "âœ… Builder Mode (#1) - optional"
    echo "   No formal review required"
    ;;
  T1)
    echo "1ï¸âƒ£  Run Builder Mode (#1) in Chat Window A"
    echo "    â†’ Save output to PROGRESS_*.md"
    echo ""
    echo "2ï¸âƒ£  Run: make evidence"
    echo "    â†’ Creates test_evidence.txt, lint_evidence.txt, types_evidence.txt"
    echo ""
    echo "3ï¸âƒ£  âš ï¸  OPEN NEW CHAT WINDOW B (fresh session!)"
    echo "    â†’ Run Reviewer Mode (#2)"
    echo "    â†’ Paste: spec + diff + tool results"
    echo "    â†’ Do NOT paste builder's explanation!"
    ;;
  T2)
    echo "1ï¸âƒ£  Run Builder Mode (#1) in Chat Window A"
    echo ""
    echo "2ï¸âƒ£  Run: make evidence"
    echo ""
    echo "3ï¸âƒ£  âš ï¸  OPEN NEW CHAT WINDOW B (fresh session!)"
    echo "    â†’ Run Verification Mode (#3)"
    echo "    â†’ Save to reports/VERIFICATION_*.md"
    echo ""
    echo "4ï¸âƒ£  Run at least 1 Lens (Security #4 OR Performance #5)"
    echo "    â†’ Save to reports/PERSPECTIVE_*.md"
    echo ""
    echo "5ï¸âƒ£  For pipelines: Run Data-Quality Lens (#10)"
    echo ""
    echo "6ï¸âƒ£  Create DECISION_*.json in logs/ai-decisions/"
    ;;
  T3)
    echo "1ï¸âƒ£  Run Builder Mode (#1) in Chat Window A"
    echo ""
    echo "2ï¸âƒ£  Run: make evidence"
    echo ""
    echo "3ï¸âƒ£  âš ï¸  Run ALL 4 Lenses (ideally different model families):"
    echo "    â†’ Security-First (#4)"
    echo "    â†’ Performance-First (#5)"
    echo "    â†’ Maintainability-First (#6)"
    echo "    â†’ Resilience-First (#7)"
    echo "    â†’ For pipelines: Data-Quality-First (#10)"
    echo "    â†’ Save each to reports/PERSPECTIVE_*.md"
    echo ""
    echo "4ï¸âƒ£  Check agreement: If >80% overlap, run Devil's Advocate (#8)"
    echo ""
    echo "5ï¸âƒ£  Run Differential Comparison (#9) with 2nd model family"
    echo "    â†’ Save to reports/COMPARISON_*.md"
    echo ""
    echo "6ï¸âƒ£  Create DECISION_*.json with all models listed"
    echo ""
    echo "7ï¸âƒ£  Get senior human approval before merge"
    ;;
esac
 
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
 
# Step 3: Check existing evidence
echo ""
echo -e "${YELLOW}Step 3: Checking for existing evidence files...${NC}"
 
check_file() {
  if [ -f "$1" ]; then
    echo -e "  ${GREEN}âœ“${NC} $1 exists"
    return 0
  else
    echo -e "  ${RED}âœ—${NC} $1 missing"
    return 1
  fi
}
 
MISSING=0
 
if [[ "$TIER" != "T0" ]]; then
  check_file "evidence/test_*.txt" 2>/dev/null || MISSING=1
  check_file "evidence/lint_*.txt" 2>/dev/null || MISSING=1
  check_file "evidence/types_*.txt" 2>/dev/null || MISSING=1
fi
 
if [[ "$TIER" == "T2" || "$TIER" == "T3" ]]; then
  ls reports/VERIFICATION_*.md &>/dev/null && echo -e "  ${GREEN}âœ“${NC} VERIFICATION_*.md exists" || { echo -e "  ${RED}âœ—${NC} VERIFICATION_*.md missing"; MISSING=1; }
  ls logs/ai-decisions/DECISION_*.json &>/dev/null && echo -e "  ${GREEN}âœ“${NC} DECISION_*.json exists" || { echo -e "  ${RED}âœ—${NC} DECISION_*.json missing"; MISSING=1; }
fi
 
if [[ "$TIER" == "T3" ]]; then
  PERSP_COUNT=$(ls reports/PERSPECTIVE_*.md 2>/dev/null | wc -l)
  if [ "$PERSP_COUNT" -ge 4 ]; then
    echo -e "  ${GREEN}âœ“${NC} $PERSP_COUNT PERSPECTIVE_*.md files found"
  else
    echo -e "  ${RED}âœ—${NC} Only $PERSP_COUNT PERSPECTIVE_*.md files (need 4+)"
    MISSING=1
  fi
fi
 
echo ""
if [ $MISSING -eq 1 ]; then
  echo -e "${RED}âš ï¸  Some required evidence is missing. Complete the workflow above before merging.${NC}"
else
  echo -e "${GREEN}âœ“ All required evidence present for $TIER!${NC}"
fi
 
echo ""
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "Quick Commands:"
echo "  make evidence      - Collect test/lint/type evidence"
echo "  make check         - Run all quality gates"
echo "  ./scripts/gather-context.sh - Prepare context for AI session"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"


F.3 Usage


# Make executable
chmod +x scripts/ai_orchestrator.sh
 
# Run at start of any coding task
./scripts/ai_orchestrator.sh




APPENDIX G: Compliance Certification


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          DOCUMENT CERTIFICATION                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  Document: SparkData Analytics AI-Assisted Development SOP                    â•‘
â•‘  Version: 5.2.0 (Gold Standard Final Edition)                                 â•‘
â•‘  Last Updated: November 30, 2025                                              â•‘
â•‘                                                                               â•‘
â•‘  COMPANION DOCUMENTS:                                                         â•‘
â•‘  â€¢ Risk Tier Table v2.1 (SparkData_Risk_Tier_Table_v2.1.md)                  â•‘
â•‘  â€¢ IDE Quick Prompts v2.1 (SparkData_IDE_Quick_Prompts_v2.1.md)              â•‘
â•‘                                                                               â•‘
â•‘  EXPLICIT STANDARDS ALIGNMENT:                                                â•‘
â•‘  âœ“ AICPA Quality Management Standards (SQMS No. 1)                           â•‘
â•‘  âœ“ EU Artificial Intelligence Act (High-Risk Guidance)                       â•‘
â•‘  âœ“ PCAOB AI Risk Management Guidance                                         â•‘
â•‘  âœ“ Agentic AI Industry Patterns (2025)                                       â•‘
â•‘                                                                               â•‘
â•‘  FIVE QUICK WINS IMPLEMENTED:                                                 â•‘
â•‘  1. âœ“ Independent Verification Agent (Â§1.1.1)                                â•‘
â•‘  2. âœ“ Structured Decision Logging (Â§1.1.2)                                   â•‘
â•‘  3. âœ“ Perspective Diversity Protocol (Â§1.1.3)                                â•‘
â•‘  4. âœ“ Differential Model Comparison Gates (Â§1.1.4)                           â•‘
â•‘  5. âœ“ Automated Quality Feedback Loop (Â§1.1.5)                               â•‘
â•‘                                                                               â•‘
â•‘  RISK TIERING IMPLEMENTED:                                                    â•‘
â•‘  âœ“ T0-T3 tier definitions with clear criteria                                â•‘
â•‘  âœ“ Escalating controls by tier                                               â•‘
â•‘  âœ“ Evidence requirements by tier                                             â•‘
â•‘  âœ“ Human oversight requirements by tier                                      â•‘
â•‘  âœ“ CI enforcement hooks defined                                              â•‘
â•‘  âœ“ Tier dispute resolution process                                           â•‘
â•‘                                                                               â•‘
â•‘  IDE QUICK PROMPTS IMPLEMENTED:                                               â•‘
â•‘  âœ“ 10 standardized prompts for all roles (including Data-Quality Lens)       â•‘
â•‘  âœ“ Metadata blocks for audit trail                                           â•‘
â•‘  âœ“ Falsification-oriented verification                                       â•‘
â•‘  âœ“ Multi-model diversity requirements                                        â•‘
â•‘  âœ“ Agreement measurement triggers                                            â•‘
â•‘  âœ“ Approved model families for T3                                            â•‘
â•‘                                                                               â•‘
â•‘  KEY ENHANCEMENTS IN v5.2:                                                    â•‘
â•‘  â€¢ Fixed version metadata consistency                                        â•‘
â•‘  â€¢ Explicit fresh chat separation rule (Â§3.2.1)                              â•‘
â•‘  â€¢ Inline AI (autocomplete) policy (Â§3.2.2)                                  â•‘
â•‘  â€¢ Tier dispute resolution process                                           â•‘
â•‘  â€¢ Data-Quality-First Lens for pipelines                                     â•‘
â•‘  â€¢ T1 worked example appendix                                                â•‘
â•‘  â€¢ AI orchestrator script                                                    â•‘
â•‘  â€¢ Approved model families table                                             â•‘
â•‘  â€¢ Future automation roadmap notes                                           â•‘
â•‘                                                                               â•‘
â•‘  Approved By: SparkData Engineering                                           â•‘
â•‘  Next Review: Q1 2026                                                         â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




END OF DOCUMENT



B.1 Purpose

This tiering and control matrix implements SparkData's alignment with:
	â€¢	AICPA QM standards (risk-based quality controls)
	â€¢	EU AI Act requirements (logging and human oversight)
	â€¢	PCAOB guidance (independent verification and professional skepticism)
	â€¢	Production agentic AI patterns (multi-agent, multi-model verification)

B.2 Risk Tier Definitions


Tier
Name
Examples
Default?
T0
Low
Docs, comments, non-prod scripts
No
T1
Medium
Non-core features, UI, minor pipelines
YES
T2
High
Core logic, data models, auth, key pipelines
No
T3
Critical
Security, PII, billing, regulatory, MDRS/RAI
No


B.3 Required Controls Summary


Tier
AI Agents
Evidence
Human Oversight
T0
Builder (optional)
None
None
T1
Builder + Adversarial Reviewer (blind)
test + lint + types
Human merges
T2
Planner + Builder + Verification Agent + 1 Lens
T1 + DECISION_*.json
Lead approval
T3
All T2 + 4-Lens + Devil's Advocate + Differential Gate (â‰¥2 models)
All + PERSPECTIVE_.md + COMPARISON_.md
Senior + Compliance


B.4 Key Rules

1. Default to T1 unless clear reason otherwise
2. Elevate to T2 for: core logic, data models, auth, pipelines
3. Elevate to T3 for: security, PII, billing, regulatory
4. Lowering tier requires justification in decision log
5. âš ï¸ ESCALATION: Time pressure cannot lower tier - escalate to Lead/CTO

B.5 Agreement Measurement (Devil's Advocate Trigger)

Trigger when: >80% issue overlap AND risk score spread <1

B.6 CI Enforcement

T2/T3 PRs should verify via CI:
	â€¢	DECISION_*.json exists
	â€¢	VERIFICATION_*.md exists (T2+)
	â€¢	4x PERSPECTIVE_*.md exists (T3)
	â€¢	Tier in decision log matches PR label

Full details: See SparkData_Risk_Tier_Table_v2.md



APPENDIX C: IDE Quick Prompts Reference (v2.0)

C.1 Available Prompts


#
Prompt
Tier
Purpose
1
Builder Mode
All
Implement changes + tests
2
Reviewer Mode
T1+
Adversarial review (blind)
3
Verification Mode
T2/T3
Independent gate
4
Security-First Lens
T2/T3
Security perspective
5
Performance-First Lens
T2/T3
Performance perspective
6
Maintainability-First Lens
T2/T3
Maintainability perspective
7
Resilience-First Lens
T2/T3
Resilience perspective
8
Devil's Advocate
T3
Challenge high agreement
9
Differential Comparison
T3
Compare model outputs


C.2 Critical Rules

	â€¢	ğŸ”´ ALWAYS start Reviewer/Verification in FRESH CHAT
	â€¢	ğŸ”´ NEVER show Builder's explanation to Reviewer
	â€¢	ğŸ”´ ALWAYS paste actual TOOL RESULTS
	â€¢	ğŸ”´ ALWAYS include METADATA block
	â€¢	ğŸ”´ T3: Use DIFFERENT model families for each lens
	â€¢	ğŸ”´ Agreement >80% = Trigger Devil's Advocate

C.3 Free-Form Policy

T1-T3: Free-form prompts NOT allowed - use only approved prompts
T0: Free-form acceptable but discouraged

Full prompts: See SparkData_IDE_Quick_Prompts_v2.md



APPENDIX D: Compliance Certification


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                          DOCUMENT CERTIFICATION                                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  Document: SparkData Analytics AI-Assisted Development SOP                    â•‘
â•‘  Version: 5.2.0 (Gold Standard Final Edition)                                 â•‘
â•‘  Last Updated: November 30, 2025                                              â•‘
â•‘                                                                               â•‘
â•‘  EXPLICIT STANDARDS ALIGNMENT:                                                â•‘
â•‘  âœ“ AICPA Quality Management Standards (SQMS No. 1)                           â•‘
â•‘  âœ“ EU Artificial Intelligence Act (High-Risk Guidance)                       â•‘
â•‘  âœ“ PCAOB AI Risk Management Guidance                                         â•‘
â•‘  âœ“ Agentic AI Industry Patterns (2025)                                       â•‘
â•‘                                                                               â•‘
â•‘  FIVE QUICK WINS IMPLEMENTED:                                                 â•‘
â•‘  1. âœ“ Independent Verification Agent (Â§1.1.1)                                â•‘
â•‘  2. âœ“ Structured Decision Logging (Â§1.1.2)                                   â•‘
â•‘  3. âœ“ Perspective Diversity Protocol (Â§1.1.3)                                â•‘
â•‘  4. âœ“ Differential Model Comparison Gates (Â§1.1.4)                           â•‘
â•‘  5. âœ“ Automated Quality Feedback Loop (Â§1.1.5)                               â•‘
â•‘                                                                               â•‘
â•‘  RISK TIERING IMPLEMENTED:                                                    â•‘
â•‘  âœ“ T0-T3 tier definitions with clear criteria                                â•‘
â•‘  âœ“ Escalating controls by tier                                               â•‘
â•‘  âœ“ Evidence requirements by tier                                             â•‘
â•‘  âœ“ Human oversight requirements by tier                                      â•‘
â•‘  âœ“ CI enforcement hooks defined                                              â•‘
â•‘                                                                               â•‘
â•‘  IDE QUICK PROMPTS IMPLEMENTED:                                               â•‘
â•‘  âœ“ 9 standardized prompts for all roles                                      â•‘
â•‘  âœ“ Metadata blocks for audit trail                                           â•‘
â•‘  âœ“ Falsification-oriented verification                                       â•‘
â•‘  âœ“ Multi-model diversity requirements                                        â•‘
â•‘  âœ“ Agreement measurement triggers                                            â•‘
â•‘                                                                               â•‘
â•‘  KEY ENHANCEMENTS IN v5.2:                                                    â•‘
â•‘  â€¢ Explicit independence requirements in control matrix                      â•‘
â•‘  â€¢ Evidence file schemas defined                                             â•‘
â•‘  â€¢ Agreement measurement methods specified                                   â•‘
â•‘  â€¢ Escalation rule for time pressure                                         â•‘
â•‘  â€¢ Data pipeline special rules                                               â•‘
â•‘  â€¢ CI enforcement hooks                                                      â•‘
â•‘  â€¢ Differential Comparison prompt added                                      â•‘
â•‘  â€¢ Metadata blocks in all prompts                                            â•‘
â•‘  â€¢ Test requirement in reviewer output                                       â•‘
â•‘                                                                               â•‘
â•‘  Approved By: SparkData Engineering                                           â•‘
â•‘  Next Review: Q1 2026                                                         â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•




END OF DOCUMENT



