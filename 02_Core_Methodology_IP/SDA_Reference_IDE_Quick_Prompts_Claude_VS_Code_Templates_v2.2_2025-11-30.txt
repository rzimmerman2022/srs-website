
SparkData IDE Quick Prompts
One-Page Reference for Cursor / VS Code / Copilot Chat
Version:
 2.2
Last Updated:
 December 1, 2025
Usage:
 Copy-paste these prompts into your IDE's AI chat panel
Compliance:
 AICPA QM | EU AI Act | PCAOB | Agentic AI Patterns

  ðŸš€  HOW TO START YOUR DAY (5-Step Daily Workflow)
1. Pick tier (T0â€“T3) â†’ paste Risk Tier Declaration
2. Run Builder prompt (#1)
3. New chat â†’ Reviewer / Verification / lenses per tier
4. Save outputs to the right evidence files
5. Update DECISION_*.json and push

  ðŸŽ¯  SHORTCUTS BY RISK TIER
Tier
Required Prompts / Shortcuts
T0
"Build this" only (review optional)
T1
"Build this" + "Review this" (new chat!)
T2
"Build this" + "Review this" + "Verify T2/T3" + at least 1 lens
For pipelines: "Data quality" lens required
T3
All of the above + "Security check" + "Perf check" + "Maintain check" + "Resilience check"
+ "Compare models" + "Challenge this" (if agreement >80%)

  ðŸ“‹  WHAT TO PASTE (AND WHAT NOT TO)
âŒ  DON'T PASTE
âœ…  DO PASTE
	â€¢	Prior chat logs
	â€¢	"Here's what I did" prose from Builder
	â€¢	Screenshots of reasoning
	â€¢	Summaries from other agents
	â€¢	Verbal claims about test results
	â€¢	Requirement / spec / ticket text
	â€¢	Actual diff or file contents
	â€¢	Test output from terminal
	â€¢	Lint / type-check results
	â€¢	Coverage report numbers
âš ï¸ Why this matters: Reviewers who see Builder explanations will just summarize them instead of independently verifying the code. This defeats the entire purpose of multi-agent review.

  âš ï¸  BEFORE USING ANY PROMPT
Step 1: Determine Risk Tier (T0/T1/T2/T3) per SOP Â§1.2. If unsure, choose the higher tier.
Step 2: Paste the Risk Tier Declaration so the agent sees context:
## Risk Tier Declaration
**Tier:** [T0 | T1 | T2 | T3]
**Justification:** [Why this tier]
**Required Controls:** [List from Â§1.2.2]
Step 3: Use ONLY the prompts below for T1-T3 changes. Free-form prompts are NOT allowed.

  ðŸš«  HARD RULE: FRESH CHAT SEPARATION
â›” If a chat has ever used Builder Mode (#1) in it, you may NOT use Reviewer (#2), Verification (#3), or any Lens prompts (#4-10) in the same chat. Start a new chat.
This is non-negotiable. Mixing roles in the same chat defeats the entire purpose of independent verification.
Recommended Convention: Name your chat windows with role prefixes:
	â€¢	[B] Feature X implementation
	â€¢	[R] Feature X review
	â€¢	[V] Feature X verification

  ðŸ¢  APPROVED MODEL FAMILIES FOR T3
For T3 (Critical) work, use at least 2 distinct model families across your agents:
Family
Models
Recommended For
Anthropic
Claude Opus 4, Claude Sonnet 4, Claude Haiku 4
Planning, Code Review, Security Review
OpenAI
GPT-5 Codex, GPT-4o, o1-pro
Implementation, Performance Review
Google
Gemini 2.0 Pro, Gemini 2.0 Ultra
Alternative perspective, Resilience Review
Meta
Llama 3.1 405B (if self-hosted)
Secondary verification
T3 Minimum Requirement: Builder and at least one Verification/Lens agent MUST use different model families.

  ðŸ¤–  INLINE AI (AUTOCOMPLETE) POLICY
For T2/T3 work, any AI-generated code (including inline suggestions from Copilot, Cursor autocomplete, etc.) must still be put through:
	â€¢	A Builder Mode pass over the final diff, AND
	â€¢	The appropriate Reviewer / Verification / Lens prompts before merge
Autocomplete is allowed, but it never bypasses the multi-agent quality gate.

  ðŸ”§  1. BUILDER MODE (Implement Change + Tests)
	â€¢	When to use: Any time you're changing code (T0-T3)
	â€¢	Risk Tier: All tiers
	â€¢	Inputs needed: Tier declaration + Spec/ticket + selected file(s)
Prompt:
You are the **Builder Agent** for SparkData Analytics.
 
## METADATA (fill in):
- Model: [your model name/version]
- Date/Time (UTC): [current timestamp]
- Risk Tier: [T0 | T1 | T2 | T3]
- Session ID: [unique identifier]
 
## RISK TIER DECLARATION:
[PASTE: tier declaration from above]
 
## SPEC (requirement):
[PASTE: ticket / acceptance criteria / 2-10 lines max]
 
## CODE CONTEXT:
[The IDE already shows the file or diff]
 
## Your Tasks:
1. Implement the **minimal** code changes needed to satisfy the SPEC
2. Add or update tests to cover:
   - Main happy paths
   - Edge cases and invalid inputs
   - Any non-functional requirements (perf/security/logging) if in SPEC
3. Keep changes **small and focused** - do NOT refactor unrelated code
 
## Constraints:
- Follow existing patterns and style in this repo
- Do NOT evaluate or "approve" your own work - a separate reviewer will audit it
- Do NOT change public APIs unless SPEC clearly requires it
- For T2/T3: Ensure tests are comprehensive enough to catch regressions
 
## Output Format:
 
### Metadata Block
```
Model: [name/version]
Timestamp: [ISO-8601 UTC]
Risk Tier: [T0-T3]
Session ID: [ID]
```
 
### Implementation
1. A single ```diff``` block with the proposed patch
 
### Summary (max 5 bullets):
- What changed and why
- What tests you added/updated
- Any assumptions you made
- For T2/T3: Explicitly list edge cases covered by tests

  ðŸ”  2. REVIEWER MODE (Adversarial Review - Blind)
	â€¢	When to use: T1/T2/T3 changes for code review
	â€¢	Risk Tier: T1 minimum, required for T2/T3
	â€¢	âš ï¸ CRITICAL: Start a fresh chat - do NOT include Builder's explanation
	â€¢	Inputs needed: Tier declaration + Spec + diff + tool results (from terminal)
Prompt:
You are the **Adversarial Reviewer Agent** for SparkData Analytics.
 
## METADATA (fill in):
- Model: [your model name/version]
- Date/Time (UTC): [current timestamp]
- Risk Tier: [T0 | T1 | T2 | T3]
- Session ID: [unique identifier]
 
Your job is to independently review this code change using a skeptical, proof-driven mindset.
 
## RISK TIER DECLARATION:
[PASTE: tier declaration]
 
## SPEC:
[PASTE: requirement or key parts of plan]
 
## CODE DIFF:
[PASTE: git diff output or relevant file sections]
 
## TOOL RESULTS:
[PASTE: e.g., "tests: 152 passed; coverage 82%â†’85%; lint clean; mypy clean"]
 
You do **NOT** see any explanation from the original author.
 
## Tasks:
 
### 1. Checklist Review
For each item, mark **Pass / Fail / Unclear** and justify briefly:
- [ ] Correctness vs SPEC
- [ ] Edge cases & error handling
- [ ] Test quality (coverage + meaningfulness)
- [ ] Security / data-integrity concerns
- [ ] Performance / complexity concerns
- [ ] Consistency with project patterns
- [ ] Logging / observability
- [ ] Backwards compatibility
 
### 2. Risk Score
- Overall Risk Score: **0-10** (0=minimal, 10=extremely risky)
- Explain in 2-5 sentences, citing SPEC, CODE, and TOOL RESULTS
 
### 3. Required Changes
- If Risk > 2: List numbered REQUIRED changes
  - "[Priority: High/Med/Low] â€“ [Concrete change] â€“ [Why]"
  - **âš ï¸ At least one REQUIRED change must be either:**
    - (a) A new/updated test, OR
    - (b) Additional logging/metrics
    - Unless you explicitly explain why tests/logging are already sufficient
- If Risk â‰¤ 2: Explain what you verified and why acceptable
 
## Constraints:
- Do NOT summarize what the code does - author knows that
- Do NOT be polite or vague - be specific and concrete
- Identify at least 3 issues OR explain why fewer than 3 exist
- Do NOT restate other reviewers' findings; focus on NEW issues or deeper evidence
 
## Output Format:
 
### Metadata Block
```
Model: [name/version]
Timestamp: [ISO-8601 UTC]
Risk Tier: [T0-T3]
Session ID: [ID]
Review Type: Adversarial Review (Blind)
```
 
### Checklist
[Pass/Fail/Unclear + justification for each]
 
### Risk Score
[0-10] + explanation
 
### Required Changes
[Numbered list with priorities]
 
### Verdict
[APPROVE / REQUEST_CHANGES / BLOCK]

  ðŸ“  EXAMPLE WORKFLOWS
These examples show the complete prompt sequence for real-world changes.
Example 1: T2 Pipeline Change
Scenario: Adding a new customer matching pipeline with schema changes
Step
Chat Window
Action
1
[B] Customer Match
Paste Risk Tier Declaration (T2) + Builder prompt + spec
2
[R] Customer Match
NEW CHAT: Paste Reviewer prompt + spec + diff + test output
3
[V] Customer Match
NEW CHAT: Paste Verification prompt + Data-Quality Lens
4
Terminal
Save outputs â†’ VERIFICATION_*.md, PERSPECTIVE_data-quality_*.md
5
Terminal
Create DECISION_*.json with model attribution + risk scores â†’ git push
Example 2: T3 Security Change
Scenario: Updating authentication logic for PII access controls
Step
Chat / Model
Action
1
[B] Auth (Claude)
Builder prompt with T3 declaration
2
[R] Auth (GPT-4o)
NEW CHAT, DIFFERENT MODEL: Reviewer prompt
3
[SEC] Auth (Gemini)
Security-First Lens prompt
4-6
[PERF/MAINT/RES]
Performance, Maintainability, Resilience lens prompts (vary models)
7
[CMP] Auth
Differential Model Comparison (if 2+ implementations)
8
[DA] Auth
Devil's Advocate (if agreement >80% across lenses)
9
Human
Final sign-off required for T3. Save all outputs + DECISION_*.json
ðŸ’¡ Key Insight: Notice how T3 uses different model families (Claude â†’ GPT â†’ Gemini) to catch blind spots that any single model family might miss.

  ðŸ“‹  Quick Reference Card
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      SPARKDATA IDE QUICK REFERENCE v2.2                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                               â•‘
â•‘  STEP 1: Determine Risk Tier (if unsure, choose higher)                       â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  T0 (Low):      Docs, comments, non-prod scripts                              â•‘
â•‘  T1 (Medium):   Non-core features, UI, minor pipelines  â† DEFAULT             â•‘
â•‘  T2 (High):     Core logic, data models, auth, key pipelines                  â•‘
â•‘  T3 (Critical): Security, PII, billing, regulatory, MDRS/RAI                  â•‘
â•‘                                                                               â•‘
â•‘  STEP 2: Select Prompts by Tier                                               â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  T0: Builder only (optional)                                                  â•‘
â•‘  T1: Builder â†’ Adversarial Reviewer (blind, fresh chat)                       â•‘
â•‘  T2: Builder â†’ Verification Agent (blind) + 1 Lens                            â•‘
â•‘      For pipelines: Data-Quality Lens (#10) required                          â•‘
â•‘  T3: Builder â†’ 4-Lens Review â†’ Devil's Advocate â†’ Differential Comparison     â•‘
â•‘      For pipelines: Include Data-Quality Lens as 5th lens                     â•‘
â•‘                                                                               â•‘
â•‘  ðŸ”´ CRITICAL RULES                                                            â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  ðŸ”´ NEVER use Reviewer/Verification in same chat as Builder                   â•‘
â•‘  ðŸ”´ ALWAYS start Reviewer/Verification in FRESH CHAT                          â•‘
â•‘  ðŸ”´ NEVER show Builder's explanation to Reviewer                              â•‘
â•‘  ðŸ”´ ALWAYS paste actual TOOL RESULTS (not claims)                             â•‘
â•‘  ðŸ”´ ALWAYS include METADATA block in outputs                                  â•‘
â•‘  ðŸ”´ Risk > 2 = MUST address issues before merge                               â•‘
â•‘  ðŸ”´ T3: Use DIFFERENT model families for each lens where possible             â•‘
â•‘  ðŸ”´ Agreement > 80% = Trigger Devil's Advocate                                â•‘
â•‘                                                                               â•‘
â•‘  PROMPT SHORTCUTS                                                             â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  "Build this"        â†’ #1 Builder Mode                                        â•‘
â•‘  "Review this"       â†’ #2 Reviewer Mode (new chat!)                           â•‘
â•‘  "Verify T2/T3"      â†’ #3 Verification Mode (new chat!)                       â•‘
â•‘  "Security check"    â†’ #4 Security-First Lens                                 â•‘
â•‘  "Perf check"        â†’ #5 Performance-First Lens                              â•‘
â•‘  "Maintain check"    â†’ #6 Maintainability-First Lens                          â•‘
â•‘  "Resilience check"  â†’ #7 Resilience-First Lens                               â•‘
â•‘  "Challenge this"    â†’ #8 Devil's Advocate                                    â•‘
â•‘  "Compare models"    â†’ #9 Differential Comparison                             â•‘
â•‘  "Data quality"      â†’ #10 Data-Quality-First Lens (pipelines)                â•‘
â•‘                                                                               â•‘
â•‘  OUTPUT DESTINATIONS                                                          â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  Builder output      â†’ PROGRESS_*.md                                          â•‘
â•‘  Reviewer output     â†’ (inline in PR or saved separately)                     â•‘
â•‘  Verification output â†’ reports/VERIFICATION_*.md                              â•‘
â•‘  Lens outputs        â†’ reports/PERSPECTIVE_[lens]_*.md                        â•‘
â•‘  Devil's Advocate    â†’ reports/DEVILS_ADVOCATE_*.md                           â•‘
â•‘  Comparison output   â†’ reports/COMPARISON_*.md                                â•‘
â•‘  Decision record     â†’ logs/ai-decisions/DECISION_*.json                      â•‘
â•‘                                                                               â•‘
â•‘  AGREEMENT MEASUREMENT (for Devil's Advocate trigger)                         â•‘
â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•‘
â•‘  >80% overlap = "All reviewers raise essentially the same 2-3 issues"         â•‘
â•‘  Risk spread <1 = "All risk scores within 1 point of each other"              â•‘
â•‘  Either condition â†’ Trigger Devil's Advocate review                           â•‘
â•‘                                                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  ðŸš«  Free-Form Prompts Policy
For T1-T3 changes: Free-form prompts are NOT allowed.
All IDE-based AI usage for code must use ONLY the prompts in this document. This ensures:
	â€¢	Consistent quality controls
	â€¢	Audit trail compatibility
	â€¢	Compliance with AICPA/EU AI Act/PCAOB requirements
For T0 changes: Free-form prompts are acceptable but discouraged.

  ðŸ”®  FUTURE AUTOMATION NOTE
The agreement measurement for Devil's Advocate trigger (>80% overlap AND risk spread <1) is currently manual. Future roadmap includes:
	â€¢	Automated PERSPECTIVE_*.md parsing to extract issues and risk scores
	â€¢	Similarity computation (Jaccard on issue tuples, cosine on embeddings)
	â€¢	Auto-trigger script that spawns Devil's Advocate when threshold met
For now, manually compare reviewer outputs and invoke Devil's Advocate (#8) when agreement seems suspiciously high.

END OF IDE QUICK PROMPTS v2.2
