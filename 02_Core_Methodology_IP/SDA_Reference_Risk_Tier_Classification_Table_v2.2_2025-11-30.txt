SparkData Analytics
Risk Tiering & Required Controls
SOP Section 1.2 (Drop-in Ready)
Version: 2.2  |  Last Updated: November 30, 2025
Compliance Alignment: AICPA QM  ‚Ä¢  EU AI Act  ‚Ä¢  PCAOB  ‚Ä¢  Agentic AI Best Practices

Purpose
This tiering and control matrix implements SparkData's alignment with:
	‚Ä¢	AICPA QM standards (risk-based quality controls)
	‚Ä¢	EU AI Act requirements (logging and human oversight)
	‚Ä¢	PCAOB guidance (independent verification and professional skepticism)
	‚Ä¢	Production agentic AI patterns (multi-agent, multi-model verification)
1.2.1 Risk Tier Definitions
Tier
Name
Typical Examples
Default?
T0
Low / Trivial
Docs-only edits, comments, non-prod scripts, cosmetic refactors, internal notes
No
T1
Medium
Non-core features, UI changes, minor pipeline steps, small schema tweaks
YES (Default)
T2
High
Core business logic, key pipeline stages, data model changes, auth-adjacent code
No
T3
Critical
Security-sensitive logic, billing/financial flows, regulatory workflows, PII handling, MDRS/RAI core
No

1.2.2 Required Controls Matrix
TIER
AI AGENTS REQUIRED
EVIDENCE REQUIRED
HUMAN OVERSIGHT
T0
LOW
‚Ä¢ Builder Agent (optional)
‚Ä¢ No formal review required
‚Ä¢ None formal
‚Ä¢ Quick lint if code touched
‚Ä¢ None, or spot-check at reviewer's discretion
T1
MEDIUM
DEFAULT
‚Ä¢ Planner (optional for small items)
‚Ä¢ Builder Agent
‚Ä¢ Adversarial Reviewer (blind, fresh chat)
‚Ä¢ test_evidence.txt
‚Ä¢ lint_evidence.txt
‚Ä¢ types_evidence.txt
‚Ä¢ Human merges PR
‚Ä¢ Human reads review before approving
T2
HIGH
‚Ä¢ Planner Agent REQUIRED
‚Ä¢ Builder Agent
‚Ä¢ Independent Verification Agent
‚ö†Ô∏è MUST be separate model invocation in fresh chat with NO access to Builder or Reviewer narratives/rationales
‚Ä¢ At least 1 specialized lens (Security OR Performance)
‚Ä¢ For pipelines: Data-Quality Lens required
‚Ä¢ All T1 evidence
‚Ä¢ DECISION_*.json
‚Ä¢ VERIFICATION_*.md
‚Ä¢ Human lead must approve
‚Ä¢ Cannot auto-merge
‚Ä¢ Review documented in PR
T3
CRITICAL
‚Ä¢ Planner Agent REQUIRED
‚Ä¢ Builder Agent
‚Ä¢ Four-Lens Review (all 4 lenses)
‚ö†Ô∏è Each lens SHOULD use different model family (Claude/GPT/Gemini) where available
- Security-First
- Performance-First
- Maintainability-First
- Resilience-First
- Data-Quality-First (for pipelines)
‚Ä¢ Devil's Advocate Re-Audit (triggered if agreement > 80%)
‚Ä¢ Differential Model Comparison Gate (‚â•2 different model families)
‚Ä¢ All T2 evidence
‚Ä¢ 4x PERSPECTIVE_*.md (or 5x for pipelines)
‚Ä¢ COMPARISON_*.md
‚Ä¢ CONSENSUS_*.md
‚Ä¢ AI_RUN_*.json
‚Ä¢ Senior human owner must approve
‚Ä¢ For PII/billing/regulated: Compliance/Business owner sign-off REQUIRED before merge
‚Ä¢ Breaking changes require rollback plan + explicit sign-off

1.2.3 Evidence File Schemas
File Type
Description
Required Contents
test_evidence.txt
Test execution results
Command(s) run, pass/fail count, coverage %, failures detail
lint_evidence.txt
Linter output
Tool name, command, pass/fail, issue count, issue details
types_evidence.txt
Type checker output
Tool (mypy/pyright/etc), command, error count, errors detail
DECISION_*.json
AI decision record
{ tier, models[], risk_score, verdict, timestamp, issues[], human_override }
VERIFICATION_*.md
Verification report
Gate criteria evaluation, risk score, verdict, evidence bullets
PERSPECTIVE_*.md
Lens-specific review
Lens name, issues found, strengths, risk score, gate-stoppers
COMPARISON_*.md
Differential analysis
Candidates compared, differences by category, recommendation
CONSENSUS_*.md
Multi-model consensus
Agreement level, divergence areas, Devil's Advocate trigger status
AI_RUN_*.json
Run metadata
Models used, timestamps, prompt hashes, input/output hashes
Minimal DECISION_*.json Schema
{
  "decision_id": "DECISION_2025-11-30T10-30-00Z_abc123",
  "tier": "T2",
  "timestamp": "2025-11-30T10:30:00Z",
  "models": [
    {"name": "claude-opus-4", "role": "builder"},
    {"name": "gpt-5-codex", "role": "verification"}
  ],
  "risk_score": 3,
  "verdict": "APPROVE_WITH_CONDITIONS",
  "issues": [
    {"severity": "medium", "category": "performance", "description": "N+1 query in loop"}
  ],
  "conditions": ["Add caching for user lookup"],
  "human_override": {"status": "none", "approver": null, "notes": null}
}
1.2.4 Agreement Measurement (Devil's Advocate Trigger)
Devil's Advocate Re-Audit is triggered when reviewer agreement is suspiciously high.
Measurement Methods (choose one):
Method
Trigger Threshold
How to Calculate
Issue Overlap
> 80%
(issues in common) / (total unique issues)
Risk Score Spread
< 1.0
max(scores) - min(scores)
Jaccard Similarity
> 0.8
On (severity, category, location) tuples
Embedding Cosine
> 0.9
Cosine similarity of issue text embeddings
Simple Rule: If >80% of issues overlap AND risk scores differ by <1, trigger Devil's Advocate.
Operational Definition of "Agreement > 80%"
In practice, trigger Devil's Advocate when:
	‚Ä¢	All reviewers raise essentially the same 2‚Äì3 issues and nothing else
	‚Ä¢	No reviewer identifies a unique concern the others missed
	‚Ä¢	Risk scores are within 1 point of each other across all reviewers
	‚Ä¢	Review outputs read like slight paraphrases of each other (indicates possible cross-contamination)
This suspiciously high agreement often indicates reviewers are summarizing each other rather than performing independent analysis. Devil's Advocate forces a genuinely adversarial perspective.
üîÆ Future Automation Note
The agreement measurement is currently manual. Future roadmap includes:
	‚Ä¢	Automated PERSPECTIVE_*.md parsing to extract issues and risk scores
	‚Ä¢	Similarity computation script (Jaccard on issue tuples, cosine on embeddings)
	‚Ä¢	CI hook that auto-triggers Devil's Advocate prompt when threshold met
For now, manually compare reviewer outputs and invoke Devil's Advocate when agreement seems suspiciously high.
1.2.5 Tier Assignment Rules
	‚Ä¢	Default to T1 unless you have a clear reason to go lower or higher
	‚Ä¢	Elevate to T2 whenever a change touches: core logic, data models, auth, or key pipelines
	‚Ä¢	Elevate to T3 whenever a change touches: security, PII, billing, regulatory logic, or SparkData core analytics
	‚Ä¢	Lowering a tier (e.g., T2 ‚Üí T1) must be justified in the decision log with explicit rationale
	‚Ä¢	When in doubt, go higher - the extra review is cheap insurance
	‚Ä¢	‚ö†Ô∏è ESCALATION RULE: If time pressure conflicts with required tier controls, the tier cannot be lowered. Escalation to Engineering Lead / CTO is required. Compliance cannot be traded for speed.
üî¥ TIER DISPUTE RESOLUTION
If model or developer and reviewer disagree on tier assignment:
	‚Ä¢	The human lead makes the final call
	‚Ä¢	Decision MUST be logged in DECISION_*.json with human_override.notes explaining rationale
	‚Ä¢	If no human lead available, default to higher tier until resolution

1.2.6 Quick Reference: When to Elevate Tier
If the change touches...
Minimum Tier
Rationale
Documentation only
T0
No code risk
Non-production code (scripts, tools)
T0
Limited blast radius
UI/cosmetic changes
T1
User-facing but low logic risk
Non-core business logic
T1
Standard development
Core business logic
T2
High correctness requirements
Data models / schemas
T2
Data integrity risk
Key pipeline stages
T2
Data flow risk
Auth-adjacent code
T2
Security-adjacent
Security-sensitive logic
T3
Direct security impact
Billing / financial flows
T3
Financial/legal risk
Regulatory / compliance code
T3
Compliance risk
PII handling
T3
Privacy/legal risk
MDRS / RAI core logic
T3
SparkData core IP
1.2.7 Data Pipeline Special Rules
SparkData-Specific: Data pipelines (dbt, Airflow, Spark, etc.) have elevated requirements.
Pipeline Change Type
Minimum Tier
Additional Requirements
Schema changes
T2
Data-Quality Lens required
New pipeline creation
T2
Full test plan required
Core transformation logic
T2
Resilience Lens required
Pipeline affecting billing/compliance data
T3
All 5 lenses + Compliance sign-off
Evidence Requirements for Pipelines:
	‚Ä¢	Log Great Expectations / dbt test summaries into test_evidence.txt
	‚Ä¢	Include row count assertions in metrics_snapshot
	‚Ä¢	Document schema contracts in DECISION_*.json
1.2.8 Tier Declaration Template
Required: Add this to the top of every AI session AND PR description.
## Risk Tier Declaration
 
**Assigned Tier:** [T0 | T1 | T2 | T3]
**Justification:** [Why this tier was selected - reference ¬ß1.2.6]
**Elevating factors:** [List any factors that pushed tier up, or "None"]
**Required controls per ¬ß1.2.2:**
- [ ] [List each required control]
- [ ] [Check off as completed]
 
**Escalation (if applicable):** [If tier was contested, document decision]

1.2.9 Inline AI (Autocomplete) Policy
For T2/T3 work: Any AI-generated code‚Äîincluding inline suggestions from Copilot, Cursor autocomplete, tab-completion, etc.‚Äîmust still be put through:
	‚Ä¢	A Builder Mode pass over the final diff, AND
	‚Ä¢	The appropriate Reviewer / Verification / Lens prompts before merge
Inline assistance is allowed, but it never bypasses the multi-agent quality gate. The tier controls apply to the final code regardless of how it was generated.
1.2.10 Compliance Alignment Summary
Tier
AICPA QM
EU AI Act
PCAOB
Agentic AI
T0
Minimal controls
N/A
N/A
N/A
T1
Standard QC
Basic logging
Basic skepticism
Single-agent
T2
Enhanced QC + monitoring
Full logging + documentation
Independent verification
Multi-agent
T3
Full QM system
Human oversight mandatory
Multi-perspective + skepticism
Multi-model ensemble
1.2.11 CI Enforcement Hooks (Recommended)
For T2/T3 PRs, CI pipeline should verify:
# .github/workflows/tier-compliance.yml
- name: Check T2/T3 Compliance
  if: contains(github.event.pull_request.labels.*.name, 'T2') || contains(github.event.pull_request.labels.*.name, 'T3')
  run: |
    # Verify DECISION_*.json exists
    if [ ! -f logs/ai-decisions/DECISION_*.json ]; then
      echo "ERROR: T2/T3 requires DECISION_*.json"
      exit 1
    fi
    
    # Verify VERIFICATION_*.md exists for T2+
    if [ ! -f reports/VERIFICATION_*.md ]; then
      echo "ERROR: T2/T3 requires VERIFICATION_*.md"
      exit 1
    fi
    
    # For T3, verify PERSPECTIVE_*.md files exist (at least 4)
    if [[ "${{ contains(github.event.pull_request.labels.*.name, 'T3') }}" == "true" ]]; then
      PERSPECTIVE_COUNT=$(ls reports/PERSPECTIVE_*.md 2>/dev/null | wc -l)
      if [ "$PERSPECTIVE_COUNT" -lt 4 ]; then
        echo "ERROR: T3 requires 4 PERSPECTIVE_*.md files (found $PERSPECTIVE_COUNT)"
        exit 1
      fi
    fi
    
    # Verify tier in decision log matches PR label
    DECISION_TIER=$(jq -r '.tier' logs/ai-decisions/DECISION_*.json | head -1)
    if [[ ! "${{ github.event.pull_request.labels.*.name }}" == *"$DECISION_TIER"* ]]; then
      echo "ERROR: Decision log tier ($DECISION_TIER) doesn't match PR label"
      exit 1
    fi
This moves from "SOP says we should" to "the pipeline refuses to merge unless we did."

1.2.12 Approved Model Families for T3
For T3 (Critical) work, use at least 2 distinct model families across your agents:
Family
Models
Recommended For
Anthropic
Claude Opus 4, Claude Sonnet 4, Claude Haiku 4
Planning, Code Review, Security Review
OpenAI
GPT-5 Codex, GPT-4o, o1-pro
Implementation, Performance Review
Google
Gemini 2.0 Pro, Gemini 2.0 Ultra
Alternative perspective, Resilience Review
Meta
Llama 3.1 405B (if self-hosted)
Secondary verification
T3 Minimum Requirement: Builder and at least one Verification/Lens agent MUST use different model families.
1.2.13 Pipeline & Workflow Examples by Tier
Concrete examples to help engineers assign the correct tier for common SparkData scenarios:
Tier
Pipeline / Workflow Example
Code Example
T0
‚Ä¢ Add comments to dbt model
‚Ä¢ Update Airflow DAG description
‚Ä¢ Fix typo in pipeline README
‚Ä¢ Rename internal variable
‚Ä¢ Add logging statement
‚Ä¢ Update docstrings
T1
‚Ä¢ Add new non-critical dbt model
‚Ä¢ Minor Airflow task optimization
‚Ä¢ New dashboard data source
‚Ä¢ New utility function
‚Ä¢ UI component styling
‚Ä¢ Non-core API endpoint
T2
‚Ä¢ New customer matching pipeline
‚Ä¢ Schema changes to staging tables
‚Ä¢ Core transformation logic update
‚Ä¢ Core business logic changes
‚Ä¢ Database model modifications
‚Ä¢ Auth-adjacent code changes
T3
‚Ä¢ Changes to MDRS scoring rules
‚Ä¢ Billing data pipeline changes
‚Ä¢ PII handling in data flows
‚Ä¢ RAI core algorithm changes
‚Ä¢ Authentication/authorization
‚Ä¢ Financial calculation logic
‚ö†Ô∏è WHEN IN DOUBT, CHOOSE THE HIGHER TIER
The extra review is cheap insurance. It's always safer to over-review than under-review.

1.2.14 Implementation Checklist
Use this checklist to operationalize the Risk Tier controls across your development infrastructure:
Infrastructure Setup
‚òê
Pre-commit hook to block merges without required evidence files for the declared tier
‚òê
CI job to verify DECISION_*.json exists for all AI-assisted PRs (see ¬ß1.2.11)
‚òê
IDE snippets/macros for each Quick Prompt shortcut (VS Code, Cursor, JetBrains)
‚òê
Weekly scorecard script that generates model performance metrics from decision logs
‚òê
Evidence directory structure created: logs/ai-decisions/ and reports/
Team Enablement
‚òê
All engineers trained on Risk Tier definitions and when to escalate
‚òê
IDE Quick Prompts document distributed and bookmarked by team
‚òê
PR template updated with Risk Tier Declaration block (¬ß1.2.8)
‚òê
Human leads identified for T2/T3 approval authority
Compliance & Audit Readiness
‚òê
Decision log retention policy defined (recommend 3+ years for regulated work)
‚òê
Model scorecard baseline established for quarterly performance reviews
‚òê
Audit trail verification tested: can reconstruct any AI-assisted decision from logs

END OF RISK TIER TABLE v2.1 (Enhanced)

